oDCM - APIs 101 (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(reticulate)
py_install("requests")
```

Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("API 101")__.

- If you haven't done so, open slides at https://odcm.hannesdatta.com/docs/modules/week4/ (I have just uploaded them)
- Today's (live) code is available at [https://tiu.nu/livecoding](https://tiu.nu/livecoding) (refresh may be needed)
- You can work in Google Colab throughout this class


Before we start
========================================================

- Tech development & extra material
  - Selenium?
  - Note there are "advanced" tutorials for extra code snippets
- Team projects
  - enthusiastic about many of your ideas
  - get started building your prototypes
  - still revising template for writing it up
<br>
- Any issues so far?

<!--
- API authentication will be pain point - highly idiosyncratic!
-->

Agenda
=======
- In-class: https://api.music-to-scrape.org/docs
- Coaching session (challenges #2.1-#2.4)
  - which information to retrieve?
  - for which seeds?
  - at what frequency?
  - how to process the data during the collection
- After class
  - complete tutorial yourselves

What are APIs?
============

- APIs are connectors within and between platforms and databases, using the web to talk to each other
  - backend to frontend: e.g., database <-> Reddit API <-> website
  - backend to backend: e.g., Twitter <-> Twitter API <-> some trading software
- Initially made available for developers, later for researchers (e.g., OpenAI's API)
- Available usually only after registration and approval

Technical differences between APIs and web scraping
============

- Web scraping
  - not always 100% clear whether allowed
  - have to structure (unstructured) HTML code
<br>
- API
  - good if you have permission (but, be realistic about getting access!)
  - *structured* JSON data

Discovering an API documentation
=========

- Probably the most difficult part w/ APIs is to get authenticated
  - recall: need platform's permission (but this API doesn't require authentication)
- Find an endpoint (aka page)
- Let's explore the documentation of... [OpenAI](https://platform.openai.com/docs/introduction)!
  - which endpoints are available?
  - what could you do with them?

Do: Zooming in on Music-to-scrape's API
=====

__Browse__ the [documentation](https://api.music-to-scrape.org/docs) of music-to-scrape's API.

1) Describe which endpoints are available. What do they allow you to do?

2) Can you come up with reasons why these endpoints are available? "Why has the API been created" (other than for this class...)

Do: Making a connection with an API
=================
class: small-code
  
Run the snippet below.

```{python, eval=FALSE}
import requests
url = "https://api.music-to-scrape.org/users"
response = requests.get(url)
request = response.json()
print(request)
```

1. Can you explain what happens when you run this cell?
2. Try adding the limit parameter (`?limit=20`) and run again. What happens?

How can we make sense of JSON data?
========

- JSON data is structured in a hierarchy
  - use [JSONviewer.stack.hu](http://jsonviewer.stack.hu) to make sense of it
  - alternatively, write to `.json` file and view in VS Code
  - many other possibilities exist (e.g., in-browser) - check what you like best!
  
__DO:__ Run this snippet to write JSON output to a file! Inspect it using [JSONviewer](http://jsonviewer.stack.hu) or VS Code!

```{python, eval=FALSE}
import json
f=open('users.json', 'w', encoding='utf-8')
f.write(json.dumps(request))
f.close()
```

Getting data from a JSON dictionary
===========
class: small-code

- Let's try to only extract a list of user names, rather than the entire dictionary.
- For this, we need to know how to access nodes of a dictionary.
  - `['name_of_attribute']`
  - `.get('name_of_attribute')`
- Do: Let's try it out

```{python, eval=FALSE}
request['limit']

# or: 

request.get('limit')
```

Do: Getting user names
===========

1. How would you have to modify the snippet below to access the list of user names?
2. Can you write a loop that extracts user names and puts them to a new list?

```{python, eval=FALSE}
request['limit'] # this one worked for extracting the value for limit...
```

Do: Anonymizing data
====

Suppose we do not want to extract the ages and country names for the users...

__Can you come up with a way to anonymize them (e.g., overwrite them with NA), while keeping the rest of the dictionary intact?__

Difference between `.append()` and `.extend()`
========
class: small-code

- `.append()` "adds" one item at a time to an existing list
```{python, eval=FALSE}
users = [] # empty list
users.append('another user')
users.append('yet another user')
users
```

- `extend()` adds multiple items to an existing list
```{python, eval=FALSE}
users = []
new_users = ['another user','yet another user']
users.extend(new_users)
users
```


Understanding parameters in API calls
===============

- APIs are useful because you can *modify* its search results
  - say, you want to see the next batch of users (when listing users), or 
  - search for a particular thing (when searching via the API)
- Many ways to "submit" parameters to an API
  - directly in URL, e.g., `myapi.com/search/cats_and_dogs` [not the case for music to scrape]
  - w/ parameters in URL, e.g., `myapi.com/search/?query=cats_and_dogs` 
  - in __header__ of request (think of it like the address [=header] on an envelop [=data])

<br><br>

__The API documentation will tell you what is required!__

Example: submitting parameters in the header via the `params` argument
======

- `params` requires to be a dictionary with the parameter names and corresponding values

```{python, eval=FALSE}
import requests
url = "https://api.music-to-scrape.org/users"
response = requests.get(url, params = {'limit': 15})
request = response.json()
print(request)
```

Can you __speculate about the benefits__ of submitting parameters in the header (`params`) rather than in the URL?

Do: Iterating through API endpoints
=========
class: small-code

- Remember iterating through pages on a website to "view" data? APIs know the same concept!

1. Try to add the `offset` parameter to the snippet below. What happens when you set it to `1`? What happens when you set it to `5`?
2. To which value would you have to set it to see the next batch of users?

```{python, eval=FALSE}
# start with this code
import requests
url = "https://api.music-to-scrape.org/users?limit=10"
response = requests.get(url)
request = response.json()
print(request)
```

Do: Let's wrap your code in a function
======

1. Construct a function, called `get_users()`, with parameters `limit` and `offset`, returning the dictionary of users from the API endpoint `/users`.

Looping through all result pages of an endpoint
=====

- Let's try to construct a loop that repeatedly fetches user names.
- Let us first use a `for` loop.

```{python, eval=FALSE}
for x in range(6):
  print(x)
```

__Do:__ Modify the snippet below so that it calls `get_users()` 10 times, incrementing the offset by 10 at each iteration.

For loop vs. while loop
========
class: small-code

- Difference between for `for` loops (you usually know beforehand when to stop), and `while` loops (the ending point can change, say when "there is no new data coming in")

```{python, eval=FALSE}
# for loop
for x in range(6):
  print(x)
```

```{python, eval=FALSE}
cntr = 0
while cntr < 6:
  print(cntr)
  cntr = cntr+1

```

Tying everything together
=============

We can now combine our learnings to build a function that extracts all user names to new-line separated JSON files.

- Implement it step-by-step
- Also load packages!
- Save as `.py` file and test whether it runs from command prompt/terminal


Solution: 
=============
class: small-code

```{python, eval=FALSE}
# will develop in class
```

Do: Onwards to more endpoints
=======================
class: small-code

The tutorial proceeds by introducing a series of additional endpoints.

- [`user/plays`](https://api.music-to-scrape.org/docs#operation/get_total_plays_for_username_user_plays_get) - get a user's total number of plays
- [`charts/top-artists`](https://api.music-to-scrape.org/docs#operation/chart_get_top_artists_charts_top_artists_get) - see a list of top-performing artist for this week (and previous weeks)

1. Try fetching data from `user/plays`, following the guidelines in the documentation. Do you succeed?
2. Now do the same for `charts/top-artists`. Do you get some output?

Let's extract several of these weekly charts
=======

```{python, eval=FALSE}
# code in class
```


Reflection
===========

- wow - no "structuring" of HTML "soup" (THAT's the key difference compared to web scraping)
- Spend time trying to get your first data from an API -- authentication __will__ be an issue!
- Time left? We could also go over some [legal issues](https://htmlpreview.github.io/?https://github.com/hannesdatta/legal-situation-of-web-scraping/blob/main/legal-status-of-web-scraping.html#/title-slide)
- After this class: coaching session (focus on [activity #3](https://odcm.hannesdatta.com/docs/project/workplan/activity3/))
- Next week
  - presentations about your projects
  - afterwards: coaching (get your prototype up and running!)

Coaching session
=========

- Coaching session (challenges #2.1-#2.4)
  - which information to retrieve?
  - for which seeds?
  - at what frequency?
  - how to process the data during the collection
- Work on your prototypes
  - Selenium vs. `requests`?
  - Getting authenticated with the API?
