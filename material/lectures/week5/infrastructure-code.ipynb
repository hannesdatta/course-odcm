{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa84ba1f",
   "metadata": {},
   "source": [
    "# Infrastructural Considerations\n",
    "\n",
    "This notebook demonstrates **key infrastructure patterns** for web scraping/API projects, including:\n",
    "\n",
    "- renting a cloud machine on Azure (run long scrapers without keeping your laptop on)  \n",
    "- scheduling scrapers with `cron` (automate daily/hourly data collection)  \n",
    "- saving full HTML responses (debug broken parsers and verify past results)  \n",
    "- writing cleaned data to CSV / JSONL (easy to inspect, share, and load)  \n",
    "- using SQLite for structured storage (query whether data was already collected)  \n",
    "- pushing files to Azure Blob Storage (centralized, durable storage for large outputs)  \n",
    "- tracking metadata (detect file growth, duplicate records, failed requests)  \n",
    "- sending notifications (get alerts when scrapes succeed or fail)  \n",
    "- managing environments with `virtualenv` / Docker (ensure reproducible code execution)  \n",
    "- keeping secrets in `.env` files (avoid leaking API keys in code or repos)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb337696",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "We will use a few common Python libraries:\n",
    "\n",
    "- `requests` for HTTP requests  \n",
    "- `pandas` for tabular data  \n",
    "- `sqlite3` (built-in) for SQLite databases  \n",
    "- `python-dotenv` to load secrets from a `.env` file  \n",
    "- `azure-storage-blob` for Azure Blob Storage\n",
    "\n",
    "You **don't** have to run this cell in class, but it shows students how they would set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29518617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests pandas python-dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ac104",
   "metadata": {},
   "source": [
    "## 1. Renting a Virtual Machine on Azure\n",
    "\n",
    "Curious to run a scraping project on a virtual machine in the cloud? Try out any of the cloud providers (e.g., AWS, GCP, Azure). Below, we illustrate a workflow on Microsoft Azure (may require you to link your credit card)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba19cc1",
   "metadata": {},
   "source": [
    "### How to Rent a Computer on Azure (Virtual Machine)\n",
    "\n",
    "1. Log in to the [Azure Portal](https://portal.azure.com).  \n",
    "2. Click **\"Create a resource\" → \"Virtual Machine\"**.  \n",
    "3. Choose:\n",
    "   - Subscription + Resource Group  \n",
    "   - Region (e.g., West Europe)  \n",
    "   - VM image (e.g., Ubuntu 22.04 LTS if you're comfortable with the command line; if you're more used to Windows, choose Ubuntu Desktop)\n",
    "   - Size (number of vCPUs / RAM)  \n",
    "4. Set authentication:\n",
    "   - SSH key (recommended) or password.  \n",
    "5. Configure:\n",
    "   - Disk size  \n",
    "   - Networking (public IP, security group allowing SSH)  \n",
    "6. Review + create.  \n",
    "7. Connect via SSH:\n",
    "\n",
    "```bash\n",
    "ssh username@YOUR_VM_IP_ADDRESS\n",
    "```\n",
    "\n",
    "From there, you can:\n",
    "\n",
    "- install Python / R  \n",
    "- clone your scraping repository\n",
    "- run your scraper, or use `cron` to schedule scraping jobs (see next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae06b7b",
   "metadata": {},
   "source": [
    "## 2. Scheduling with `cron`\n",
    "\n",
    "We show:\n",
    "- a simple Python script that does something (e.g., prints timestamp)  \n",
    "- an example `crontab` entry to run it periodically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c55428",
   "metadata": {},
   "source": [
    "### Example Python script\n",
    "\n",
    "Please save the snippet below as `run_scraper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def main():\n",
    "    now = datetime.datetime.now().isoformat()\n",
    "    print(f\"[{now}] Running scraper... (placeholder)\")\n",
    "    # Here you would call your actual scraping functions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1b9d1",
   "metadata": {},
   "source": [
    "### Example `crontab` entry\n",
    "\n",
    "On your computer (e.g., VM, Mac/Linux; Windows users cannot use this implementation of Cron), run: \n",
    "\n",
    "```bash\n",
    "crontab -e\n",
    "```\n",
    "\n",
    "Add a line like:\n",
    "\n",
    "```bash\n",
    "# Run scraper every day at 02:00\n",
    "0 2 * * * /usr/bin/python3 /home/username/run_scraper.py >> /home/username/scraper.log 2>&1\n",
    "```\n",
    "\n",
    "This:\n",
    "- runs the script daily at 02:00  \n",
    "- appends output + errors to `scraper.log`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a32c78-2c54-47f4-bc0d-a16b5810b1e3",
   "metadata": {},
   "source": [
    "### Overview about Cron syntax\n",
    "\n",
    "```\n",
    "* * * * * command_to_run\n",
    "│ │ │ │ │\n",
    "│ │ │ │ └── Day of week (0–7)\n",
    "│ │ │ └──── Month (1–12)\n",
    "│ │ └────── Day of month (1–31)\n",
    "│ └──────── Hour (0–23)\n",
    "└────────── Minute (0–59)\n",
    "```\n",
    "\n",
    "__Examples__\n",
    "\n",
    "* Run a script every day at 08:30\n",
    "  `30 8 * * * /usr/bin/python3 /path/script.py`\n",
    "* Run every 5 minutes\n",
    "  `*/5 * * * * /path/job.sh`\n",
    "* Run every Monday at midnight\n",
    "  `0 0 * * 1 /path/backup.sh`\n",
    "* `>> /home/username/scraper.log 2>&1` ensures that any (printed) output of your scripts is written to a file called `scraper.log`\n",
    "\n",
    "### View your scheduled jobs\n",
    "\n",
    "```bash\n",
    "crontab -l\n",
    "```\n",
    "\n",
    "**Tip:** Always use **absolute paths** to files, commands, and environments when scheduling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039f520",
   "metadata": {},
   "source": [
    "## 3. Storing as CSV or JSON\n",
    "\n",
    "Assume we parsed some data (e.g., a list of products).  \n",
    "Here we simulate that with a small dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8af9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [\n",
    "    {\"product_id\": 1, \"name\": \"Widget\", \"price\": 9.99},\n",
    "    {\"product_id\": 2, \"name\": \"Gadget\", \"price\": 14.99},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "csv_path = \"data/processed/products.csv\"\n",
    "json_path = \"data/processed/products.json\"\n",
    "\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_json(json_path, orient=\"records\", lines=True)\n",
    "\n",
    "print(f\"Saved CSV to {csv_path}\")\n",
    "print(f\"Saved JSON to {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a057667",
   "metadata": {},
   "source": [
    "## 4. Storing Full HTML\n",
    "\n",
    "We send a request to a page and store the **raw HTML** to disk.\n",
    "In practice, you’d loop over many URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d62840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs(\"data/html\", exist_ok=True)\n",
    "\n",
    "url = \"https://music-to-scrape.org\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # raise error if not 200\n",
    "\n",
    "html_path = \"data/html/example_com.html\"\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(f\"Saved HTML from {url} to {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0119d0e",
   "metadata": {},
   "source": [
    "## 5. SQL with SQLite\n",
    "\n",
    "SQLite is:\n",
    "- file-based  \n",
    "- ships with Python (module `sqlite3`)  \n",
    "- good for small projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287f85a3-e289-4c5f-a9a4-6a5d4acf781e",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Starting to use any database for a scraping project typically requires you to first define the type of database you would like to use. In essence, this boils down to\n",
    "\n",
    "- choosing a path where the SQLite database is to be located (here: `scraping.db`.\n",
    "- defining the tables that the data base should have (think of them like different Excel \"sheets\" in a workbook of multiple Excel sheets). Each table has\n",
    "    - a name (e.g., products)\n",
    "    - variables (e.g., `product_id`, `name`, `price`) and associated data types (`integer` for product IDs, `text` for product names, and floats (i.e., numbers with decimals) for prices, here encoded as `real`). SQL also supports \"JSON\" data that can be inserted in a column - here called `attributes`, and set to type `JSON`. The timestamp is encoding in `unixempoch`.\n",
    "    - primary keys (an \"index\" to each table, which will allow you rapidly search and combine different tables) - also \"defines a unique row in this table\" (therefore, it is mandatory). Here, the primary key is a so-called composite key, consisting of the product ID and the timestamp of when the data was collected (obviously, information for one product could be collected multiple times).\n",
    "    - (optional) foreign keys (similar to primary keys, to be used for optional merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539fc59a-a64b-4750-9c27-bf98c47acabf",
   "metadata": {},
   "source": [
    "__Creating the database__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c2e1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "db_path = \"scraping.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a simple table (if it doesn't yet exist)\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS products (\n",
    "        product_id INTEGER,\n",
    "        timestamp unixepoch,\n",
    "        name TEXT,\n",
    "        price REAL,\n",
    "        attributes JSON,\n",
    "        PRIMARY KEY (product_id, timestamp)\n",
    "    )\n",
    "    \"\"\"\n",
    ")\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6f591-02c5-4e66-b154-c5cd11ffa2b0",
   "metadata": {},
   "source": [
    "__Using the database (store data)__\n",
    "\n",
    "During a scraping project, you then use the created database to insert data. The code below has a \"dummy\" function (`insert_one_product()`) that simulates \"scraping\" product information and inserting it into your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf596f-656d-401f-bf7e-1ddf20c2e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Deterministic product catalog\n",
    "PRODUCT_CATALOG = {\n",
    "    1: \"Apple AirPods\",\n",
    "    2: \"Sony WH-1000XM4\",\n",
    "    3: \"Samsung Galaxy Buds\",\n",
    "    4: \"Bose QuietComfort\",\n",
    "    5: \"JBL Live Pro\"\n",
    "}\n",
    "\n",
    "def insert_one_product(db_path=\"scraping.db\"):\n",
    "    \"\"\"Insert ONE deterministic product into the database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Pick a product deterministically\n",
    "    product_id = random.choice(list(PRODUCT_CATALOG.keys()))\n",
    "    name = PRODUCT_CATALOG[product_id]\n",
    "\n",
    "    # Dummy price + attributes\n",
    "    price = round(random.uniform(50, 300), 2)\n",
    "    attributes = {\n",
    "        \"battery_life\": random.choice([10, 20, 30]),\n",
    "        \"color\": random.choice([\"black\", \"white\", \"silver\"]),\n",
    "        \"wireless\": True\n",
    "    }\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO products (product_id, timestamp, name, price, attributes)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (product_id, timestamp, name, price, json.dumps(attributes))\n",
    "    )\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Inserted product {product_id} – {name} at {timestamp}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578837e-24ee-4cce-8ca8-50274c3f467f",
   "metadata": {},
   "source": [
    "Now let's run your function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317b6f9-e852-4f11-877e-4ba304018d99",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "duration = 10 # seconds\n",
    "interval = 1 # pause between requests\n",
    "db_path=\"scraping.db\"\n",
    "\n",
    "start = time.time()\n",
    "while time.time() - start < duration:\n",
    "    insert_one_product(db_path)\n",
    "    time.sleep(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd2b5e9-bbfd-455d-81e7-986ecbabaae0",
   "metadata": {},
   "source": [
    "__Fetching the data / exporting__\n",
    "\n",
    "After the job is \"done\", we can also export the data from SQLite back to JSON or CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be6628-98df-468b-868a-f68629818c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "db_path = \"scraping.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Fetch back\n",
    "cursor.execute(\"SELECT * FROM products\")\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get column names (useful for JSON + CSV)\n",
    "colnames = [desc[0] for desc in cursor.description]\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Database path: {db_path}\")\n",
    "print(\"Rows in 'products' table:\", rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87ea53-8c52-4935-9d95-1b26dc5a6dbe",
   "metadata": {},
   "source": [
    "__Write to JSON__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a778d-7e62-43e9-8b0f-2e573f4eda67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "jsonl_path = \"products.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in rows:\n",
    "        row_dict = dict(zip(colnames, row))\n",
    "        f.write(json.dumps(row_dict) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote JSONL to: {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27efea-6c4e-42f3-b3cd-4c24870b67b3",
   "metadata": {},
   "source": [
    "__Write to CSV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51a4d2-9556-4ca0-9b57-58d88a82cd2b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "csv_path = \"products.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(colnames)   # header\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"Wrote CSV to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bc31b-49b6-4306-907a-78391968195d",
   "metadata": {},
   "source": [
    "### Checking Whether Data Has Already Been Collected\n",
    "\n",
    "Before scraping or inserting new records, it’s useful to check whether a \n",
    "product (or specific timestamp) is *already* stored in the database.  \n",
    "This prevents duplicate work and keeps the dataset clean.\n",
    "\n",
    "In the example below, we query the database to see whether a given \n",
    "`product_id` already exists. If it does, we skip inserting it; if not, \n",
    "we can safely proceed. This pattern is useful for incremental scraping, \n",
    "daily updates, and idempotent data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3999a9c-9241-4796-973f-d307e7077708",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def product_exists(product_id, db_path=\"scraping.db\"):\n",
    "    \"\"\"Return True if the product_id already exists in the DB.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT 1 FROM products WHERE product_id = ? LIMIT 1\",\n",
    "        (product_id,)\n",
    "    )\n",
    "    exists = cursor.fetchone() is not None\n",
    "\n",
    "    conn.close()\n",
    "    return exists\n",
    "\n",
    "\n",
    "# --- Example usage 1 ---\n",
    "test_id = 3  # try any ID from your catalog\n",
    "\n",
    "if product_exists(test_id): \n",
    "    print(f\"Product {test_id} already exists → skip inserting.\")\n",
    "else:\n",
    "    print(f\"Product {test_id} not in DB → safe to insert.\")\n",
    "\n",
    "\n",
    "# --- Example usage 2 ---\n",
    "test_id = 99  # try any ID not part of your catalog\n",
    "\n",
    "if product_exists(test_id): \n",
    "    print(f\"Product {test_id} already exists → skip inserting.\")\n",
    "else:\n",
    "    print(f\"Product {test_id} not in DB → safe to insert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbcb73",
   "metadata": {},
   "source": [
    "## 6. Using Azure Blob Storage\n",
    "\n",
    "To use Azure Blob Storage:\n",
    "- create a Storage Account + Blob Container  \n",
    "- get the **connection string** or **SAS token**  \n",
    "- store credentials in `.env` (see secrets section)  \n",
    "- use `azure-storage-blob` in Python.\n",
    "\n",
    "First, make sure you have the package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b6ffb-6357-44cd-9492-9eb0aa782128",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f81c7",
   "metadata": {},
   "source": [
    "### Example: Upload a File to Azure Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a32aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Load secrets from .env\n",
    "load_dotenv()  # looks for a `.env` file in current directory\n",
    "\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"AZURE_BLOB_CONTAINER\", \"scraping-data\")\n",
    "\n",
    "if connection_string:\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # Create container if it does not exist\n",
    "    try:\n",
    "        container_client.create_container()\n",
    "    except Exception:\n",
    "        pass  # likely already exists\n",
    "\n",
    "    # Upload our HTML file as an example\n",
    "    blob_name = \"html/example_com.html\"\n",
    "    with open(html_path, \"rb\") as data_stream:\n",
    "        container_client.upload_blob(name=blob_name, data=data_stream, overwrite=True)\n",
    "\n",
    "    print(f\"Uploaded {html_path} to Azure Blob as {blob_name}\")\n",
    "else:\n",
    "    print(\"AZURE_STORAGE_CONNECTION_STRING not set. Skipping Azure demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d2fbd",
   "metadata": {},
   "source": [
    "## 7. Metadata Tracking\n",
    "\n",
    "We track:\n",
    "- number of records  \n",
    "- file size  \n",
    "- HTTP status codes  \n",
    "- changes in volume over time  \n",
    "\n",
    "This is useful for **monitoring** scrapers and detecting breakage.\n",
    "\n",
    "Before you can run the examples, let's just create a sample CSV file on our disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad41165e-5bed-4546-ab53-35d7332e21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "csv_path = \"random.csv\"\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"value\"])                # header\n",
    "    for _ in range(0,random.randint(1, 50)):\n",
    "        writer.writerow([random.randint(1, 1000),       # one random row\n",
    "                         random.choice([\"foo\", \"bar\", \"baz\"])])\n",
    "        \n",
    "print('Done writing!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a55c15d",
   "metadata": {},
   "source": [
    "### Example: Simple metadata for a scraped CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_stats = os.stat(csv_path)\n",
    "file_size_bytes = file_stats.st_size\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "record_count = len(df)\n",
    "\n",
    "metadata = {\n",
    "    \"filename\": csv_path,\n",
    "    \"record_count\": record_count,\n",
    "    \"file_size_bytes\": file_size_bytes,\n",
    "}\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8cc87",
   "metadata": {},
   "source": [
    "In a real project you would:\n",
    "\n",
    "- append one such metadata record per scraping run to a log file or database  \n",
    "- plot `record_count` over time to detect sudden drops/spikes  \n",
    "- track average price, number of missing values, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2d644",
   "metadata": {},
   "source": [
    "### Example: Tracking HTTP Status Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://music-to-scrape.org/\",\n",
    "    \"https://music-to-scrape.org/non-existent-page\",\n",
    "]\n",
    "\n",
    "status_log = []\n",
    "for u in urls:\n",
    "    r = requests.get(u)\n",
    "    status_log.append({\"url\": u, \"status_code\": r.status_code})\n",
    "\n",
    "status_log_df = pd.DataFrame(status_log)\n",
    "status_log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc8e2c",
   "metadata": {},
   "source": [
    "## 8. Sending Notifications\n",
    "\n",
    "There are many options (email, Slack, Pushover, MS Teams, etc.).  \n",
    "Here we show a simple example using **Pushover**, see https://pushover.com. There are also open source variants, such as ntfy.sh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f749183-2f3e-460e-916f-263904c10c68",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Code for illustration only\n",
    "\n",
    "import requests\n",
    "\n",
    "def notify(msg):\n",
    "    requests.post(\n",
    "        \"https://api.pushover.net/1/messages.json\",\n",
    "        data={\n",
    "            \"token\": \"YOUR_APP_TOKEN\",\n",
    "            \"user\": \"YOUR_USER_KEY\",\n",
    "            \"message\": msg\n",
    "        }\n",
    "    )\n",
    "\n",
    "notify(\"Scraper finished successfully!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f09072",
   "metadata": {},
   "source": [
    "In practice, you might:\n",
    "\n",
    "- send a message when the scraper fails  \n",
    "- send a message when record counts drop to zero  \n",
    "- send a message when a new dataset is ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df300a1",
   "metadata": {},
   "source": [
    "## 9. Using `virtualenv` and Docker\n",
    "\n",
    "These tools help ensure **reproducible environments**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d93072",
   "metadata": {},
   "source": [
    "### Python: `virtualenv` (or `venv`)\n",
    "\n",
    "- Creates an isolated **Python package environment**\n",
    "- Ensures the same Python libraries are installed (e.g., `requests`, `pandas`)\n",
    "- Great for local development, teaching, and small scraping projects\n",
    "- But:\n",
    "  - Depends on your host system (OS, Python version)\n",
    "  - Can still break if someone has different system libraries\n",
    "\n",
    "**Good for:** simple, local reproducible scraping pipelines.\n",
    "\n",
    "\n",
    "__Example scripts to be executed on your command line/terminal__\n",
    "\n",
    "```bash\n",
    "# create virtual environment\n",
    "python3 -m venv .venv\n",
    "\n",
    "# activate (macOS/Linux)\n",
    "source .venv/bin/activate\n",
    "\n",
    "# activate (Windows PowerShell)\n",
    ".venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# install dependencies\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# freeze current environment\n",
    "pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "→ same `requirements.txt` = same environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aeb0fb-5c40-49bd-8bc9-2e2c615e1ce4",
   "metadata": {},
   "source": [
    "__Once your virtual environment is activated__ (`source .venv/bin/activate`), you can run your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fedab6-54c5-4f46-8f9a-13b669b6c135",
   "metadata": {},
   "source": [
    "__Other users__ can \"install\" the same environment on their machines:\n",
    "\n",
    "```bash\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700d613",
   "metadata": {},
   "source": [
    "### Docker \n",
    "\n",
    "- Packages *everything* your scraper needs (inside a `Dockerfile`):\n",
    "  - Python version  \n",
    "  - System libraries (e.g., `libxml2`, SSL)  \n",
    "  - Python dependencies  \n",
    "  - Your scraping code  \n",
    "- Runs identically everywhere:\n",
    "  - macOS, Windows, Linux, servers, GitHub Actions, cloud\n",
    "- Eliminates “works on my machine”\n",
    "- Perfect for automation, cron jobs, and long-term reproducibility\n",
    "\n",
    "**Good for:** deployment, production, shared pipelines, research workflows that must be stable over time.\n",
    "\n",
    "#### Try it out\n",
    "\n",
    "1. Install Docker  \n",
    "2. Save the scraper below as `run_scraper.py`  \n",
    "3. Use the Dockerfile to build a container  \n",
    "4. Mount a host folder so scraped data appears **outside** the container\n",
    "\n",
    "__Example scraper (save as `run_scraper.py`)__\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Make sure output folder exists (inside the container)\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "html = requests.get(\"https://music-to-scrape.org\").text\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "title = soup.title.string\n",
    "\n",
    "# Write results to a file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "filename = f\"output/scrape_{timestamp}.txt\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"Title: {title}\\n\")\n",
    "\n",
    "print(\"Wrote:\", filename)\n",
    "```\n",
    "\n",
    "__Dockerfile (save as `dockerfile`, without any extension)__\n",
    "\n",
    "```bas\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "CMD [\"python\", \"run_scraper.py\"]\n",
    "\n",
    "```\n",
    "\n",
    "__Requirements with instructions which packages need to be installed (save as `requirements.txt`)__\n",
    "\n",
    "```bash\n",
    "requests\n",
    "beautifulsoup4\n",
    "lxml\n",
    "```\n",
    "\n",
    "__When done, you need to have the following project structure implemented__\n",
    "\n",
    "```bash\n",
    "my-scraper/\n",
    "├── Dockerfile\n",
    "├── requirements.txt\n",
    "└── run_scraper.py\n",
    "```\n",
    "\n",
    "You can then \"compile\" your Docker package and run it:\n",
    "\n",
    "```bash\n",
    "docker build -t my-scraper .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f67cc-dd71-4650-b9dc-d1512c817639",
   "metadata": {},
   "source": [
    "#### Running the Scraper in Docker (with Mounted Output Folder)\n",
    "\n",
    "To run the scraper so that **all output files appear on your computer**, follow these steps:\n",
    "\n",
    "1. Make sure your project contains:\n",
    "```\n",
    "Dockerfile\n",
    "requirements.txt\n",
    "run_scraper.py\n",
    "```\n",
    "\n",
    "2. Build the Docker image:\n",
    "```bash\n",
    "docker build -t my-scraper .\n",
    "````\n",
    "\n",
    "3. Create an output folder on your machine:\n",
    "   ```bash\n",
    "   mkdir -p scraper_output\n",
    "   ```\n",
    "   \n",
    "4. Run the container and **mount** the folder:\n",
    "   ```bash\n",
    "   docker run --rm \\\n",
    "     -v $(pwd)/scraper_output:/app/output \\\n",
    "     my-scraper\n",
    "   ```\n",
    "\n",
    "   *(Windows PowerShell)*\n",
    "\n",
    "   ```powershell\n",
    "   docker run --rm `\n",
    "     -v ${PWD}/scraper_output:/app/output `\n",
    "     my-scraper\n",
    "   ```\n",
    "   \n",
    "5. Result:\n",
    "   Scraped files appear on your machine in:\n",
    "\n",
    "   ```\n",
    "   scraper_output/\n",
    "   ```\n",
    "\n",
    "Mounting lets the scraper write results to your computer while still running inside a reproducible Docker container.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b70a8f",
   "metadata": {},
   "source": [
    "## 10. Managing Secrets with `.env`\n",
    "\n",
    "We **never** want secrets (API keys, passwords) hard-coded in code or committed to Git.\n",
    "Instead, we use a `.env` file and load it at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d146db9",
   "metadata": {},
   "source": [
    "### Example `.env` file\n",
    "\n",
    "```text\n",
    "# .env (do NOT commit this to Git)\n",
    "AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=...\n",
    "AZURE_BLOB_CONTAINER=scraping-data\n",
    "SMTP_HOST=smtp.example.com\n",
    "SMTP_PORT=587\n",
    "SMTP_USER=your_email@example.com\n",
    "SMTP_PASS=super-secret-password\n",
    "NOTIFY_EMAIL=team@example.com\n",
    "```\n",
    "\n",
    "Add `.env` to your `.gitignore` to avoid any of your secrets are committed to Git/GitHub:\n",
    "\n",
    "```text\n",
    "# .gitignore\n",
    ".env\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3a5fd-40f1-474c-aa48-ee2e83098421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy .env file\n",
    "f = open('.env', 'w')\n",
    "f.write('TEST_KEY=123456')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f186d6f",
   "metadata": {},
   "source": [
    "### Loading secrets with `python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")  # returns dict of variables\n",
    "print(\"Loaded keys from .env:\", list(config.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7d887",
   "metadata": {},
   "source": [
    "You can use `os.getenv(\"VAR_NAME\")` after calling `load_dotenv()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a79266-a62f-4401-b207-8aa81195ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.getenv(\"TEST_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3636043-a586-4bbb-9cc3-abdf7da31dfe",
   "metadata": {},
   "source": [
    "\n",
    "This keeps:\n",
    "\n",
    "- configuration separate from code  \n",
    "- secrets out of your repository  \n",
    "- different settings for development vs production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766afc9c-b15c-485a-afdd-3a92996df47d",
   "metadata": {},
   "source": [
    "__How to use secrets with Docker?__\n",
    "\n",
    "Simply pass the `.env` file to docker when running a project. Your container then reads the secret with `os.getenv()` in Python, while the value itself never gets stored in the Docker image. Never hard-code any of your secret keys in your code.\n",
    "\n",
    "```bash\n",
    "docker run --rm --env-file .env my-scraper\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6299813b-c9e8-4cae-9ecd-41ca1f61d3c5",
   "metadata": {},
   "source": [
    "## 11. Hashing User Identifiers Before Saving (Privacy-by-Design)\n",
    "\n",
    "When scraping or collecting logs, you should **never store clear-text usernames or IDs**.  \n",
    "Instead, transform them into **salted hashes** so they cannot be traced back to individuals, even if the database is leaked.\n",
    "\n",
    "A *salt* is a secret random string added before hashing.  \n",
    "This makes the hash irreversible and prevents “dictionary attacks” or guessing by brute-force.\n",
    "\n",
    "The example below shows how to hash a username securely before inserting it into a database or CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf410e-3398-459e-8b06-b720cb7df5b4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "# Generate or load your secret salt (store securely, not in Git!)\n",
    "# Here we generate it once for demonstration.\n",
    "salt = os.environ.get(\"SCRAPER_SALT\") or \"my-demo-salt-123\"\n",
    "\n",
    "def hash_identifier(username: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a salted SHA-256 hash of a sensitive identifier.\n",
    "    Safe for logging or database storage.\n",
    "    \"\"\"\n",
    "    value = (salt + username).encode(\"utf-8\")\n",
    "    return hashlib.sha256(value).hexdigest()\n",
    "\n",
    "# Example usage\n",
    "raw_username = \"john_doe_1984\"\n",
    "hashed_username = hash_identifier(raw_username)\n",
    "\n",
    "print(\"Raw username:   \", raw_username)\n",
    "print(\"Hashed version: \", hashed_username)\n",
    "\n",
    "# Save hashed_user to the database instead of the raw identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246fec8",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we illustrated:\n",
    "\n",
    "- Renting a VM on Azure\n",
    "- Scheduling scraping jobs with `cron`  \n",
    "- Storing full HTML for reproducibility  \n",
    "- Saving data as CSV and JSON  \n",
    "- Using SQLite for structured storage  \n",
    "- Uploading files to Azure Blob Storage  \n",
    "- Tracking metadata (record count, file size, HTTP status codes)  \n",
    "- Sending notifications on completion/failure  \n",
    "- Using `virtualenv` / Docker for reproducible environments  \n",
    "- Managing secrets safely with `.env` files  \n",
    "\n",
    "You can use the code snippets in this notebook for your own scraping projects."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
