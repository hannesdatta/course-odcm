{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0184989",
   "metadata": {},
   "source": [
    "# 1. Getting Started with Python and Web Scraping/APIs\n",
    "\n",
    "Welcome to your first Python session for Web Data!  \n",
    "\n",
    "- In section 1, we’ll make sure you can run Python both locally and in the cloud, understand how Jupyter works, and get a first feel for the environment you’ll use throughout this course. \n",
    "- In section 2, we go over basic programming concepts in Python. If you’ve used R or RStudio before, many things will feel familiar — just with slightly different syntax and a few new tricks. \n",
    "- Finally, in section 3, we introduce you to web scraping and APIs, and applying some of the concepts covered in earlier sections.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04337f6",
   "metadata": {},
   "source": [
    "## 1.1 Why Python (and How to Run It)\n",
    "\n",
    "Python is one of the most widely used programming languages in data science.  \n",
    "We use it for everything from **web scraping** and **automation** to **machine learning** and **data visualization**.\n",
    "\n",
    "A few reasons why Python is such a good choice for us:\n",
    "- It’s open-source and completely free.\n",
    "- It’s used both in academia and industry.\n",
    "- It has fantastic documentation and community support.\n",
    "- It runs on Windows, Mac, and Linux — so everyone’s included.\n",
    "\n",
    "---\n",
    "\n",
    "### Working locally\n",
    "\n",
    "If you like having everything on your computer, the easiest way to install Python is through **Anaconda**.  \n",
    "It comes with both **Jupyter Notebook** and **Visual Studio Code (VS Code)**, which are the two main tools we’ll use.\n",
    "\n",
    "- Download Anaconda here: [anaconda.com/products/distribution](https://www.anaconda.com/products/distribution)  \n",
    "- Install it using the default settings.  \n",
    "- After installation, open either:\n",
    "  - **Jupyter Notebook** → great for step-by-step exploration.\n",
    "  - **VS Code** → great for bigger projects.\n",
    "\n",
    "---\n",
    "\n",
    "### Working in the cloud\n",
    "\n",
    "If you don’t want to install anything yet, that’s totally fine!  \n",
    "Just head over to [**Google Colab**](https://colab.research.google.com).  \n",
    "It runs notebooks directly in your browser and automatically saves them in your Google Drive.  \n",
    "You can follow all exercises there without setting up anything locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352ebb4",
   "metadata": {},
   "source": [
    "## 1.2 Navigating the Command Line\n",
    "\n",
    "Before you open Jupyter, let’s quickly talk about the terminal (also called **command prompt**).  \n",
    "Think of it as a “text-based door” to your computer.  \n",
    "You’ll occasionally use it — for example, when installing packages or launching Jupyter.\n",
    "\n",
    "Here are a few basic commands:\n",
    "\n",
    "| Action | Windows | Mac/Linux |\n",
    "|:--|:--|:--|\n",
    "| Show current folder | `cd` | `pwd` |\n",
    "| Change directory | `cd folder_name` | `cd folder_name` |\n",
    "| Move up one level | `cd ..` | `cd ..` |\n",
    "| List files | `dir` | `ls` |\n",
    "| Clear the screen | `cls` | `clear` |\n",
    "\n",
    "These will come in handy later when we save our scraped data or navigate between project folders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef736b",
   "metadata": {},
   "source": [
    "## 1.3 Launching Jupyter Notebook or VS Code\n",
    "\n",
    "Once Anaconda is installed, you can start **Jupyter Notebook** in one of two ways:\n",
    "\n",
    "- Through the **Anaconda Navigator** app — click “Launch” next to Jupyter Notebook.  \n",
    "- Or by typing `jupyter notebook` in your terminal and pressing Enter.  \n",
    "\n",
    "Jupyter will open a new browser window showing your folders. From there, you can open existing notebooks or create new ones.  \n",
    "Each notebook is made up of “cells,” which can contain either **code** or **text**.\n",
    "\n",
    "---\n",
    "\n",
    "Alternatively, if you’re using **VS Code**, simply open a folder and click **New File → Jupyter Notebook**.  \n",
    "It behaves the same way, but it’s a bit more powerful for larger projects.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Let’s make sure everything runs properly.  \n",
    "Run the cell below — it should print a short message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ff33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "print(\"Hello, Python!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a95fd7c",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e368888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, Python!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a33ff27",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "If you see “Hello, Python!” appear below the cell, everything works.  \n",
    "Each time you press **Shift + Enter**, Jupyter runs the active cell and shows the result below.  \n",
    "We’ll build on this in the next exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d1fdc",
   "metadata": {},
   "source": [
    "## 1.4 Installing and Importing Packages\n",
    "\n",
    "Python becomes truly powerful once you start using **packages** — small add-ons that provide extra functionality.\n",
    "\n",
    "For example:\n",
    "- `json` helps us work with web data and APIs.  \n",
    "- `csv` lets us read and write spreadsheets.  \n",
    "- `requests` lets us download web pages.\n",
    "\n",
    "You only need to *install* a package once (with `pip install ...`),  \n",
    "but you have to *import* it every time you start a new session.\n",
    "\n",
    "---\n",
    "\n",
    "::: {.callout-note}\n",
    "__For R users:__  \n",
    "Think of `install.packages()` as Python’s `>pip install`,\n",
    "and `library()` as Python’s `import`.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "Let’s try it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fea27",
   "metadata": {},
   "source": [
    "### Exercise 2 – Importing Packages\n",
    "\n",
    "The cell below already imports one package called `json`.  \n",
    "Your task:\n",
    "1. Add another relevant package — for example `csv`.  \n",
    "2. Use the `help()` function to inspect what each package can do.\n",
    "\n",
    "Don’t worry, this is just to show that your imports actually work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "import json\n",
    "help(json)  # shows documentation\n",
    "\n",
    "# TODO: also import csv and call help(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8e77c",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f19483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "help(json)\n",
    "help(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf545ba6",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "- `import json` loads tools for working with structured data — the same format used by most web APIs.  \n",
    "- `import csv` loads tools for handling comma-separated files, which we’ll use later to store scraped data.  \n",
    "- `help()` gives you quick documentation right inside Jupyter.  \n",
    "You don’t need to read it all now — just know that you can always look things up later.\n",
    "\n",
    "---\n",
    "\n",
    "::: {.callout-note}\n",
    "__Need help?__\n",
    "\n",
    "- You can always ask our __Tilly Chatbot__ on Canvas for course-specific questions.  \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8544ad1",
   "metadata": {},
   "source": [
    "## 1.5 Summary\n",
    "\n",
    "You’ve now:\n",
    "- Launched Python successfully.  \n",
    "- Learned how to open Jupyter Notebook or VS Code.  \n",
    "- Imported your first packages (`json` and `csv`).  \n",
    "\n",
    "We’ll use these same tools throughout the course to work with real web data.  \n",
    "In the next session, we’ll move into **Python Basics**, where we’ll start writing small bits of code to process data ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1c0a3",
   "metadata": {},
   "source": [
    "# 2. Python Basics – from R to Python\n",
    "\n",
    "In this part, we’ll connect what you already know from R or RStudio to Python.  \n",
    "You’ll learn how to work with variables, loops, lists, and dictionaries — all the small building blocks we’ll later need to collect and store web data.\n",
    "\n",
    "Don’t worry if it feels like a lot at once — we’ll move slowly, and every step connects to something we’ll actually do later when scraping websites or using APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee5897",
   "metadata": {},
   "source": [
    "## 2.1 Variables and f-strings\n",
    "\n",
    "In Python, we use **variables** to store information, just like you do in R.  \n",
    "You assign them using an equals sign `=` instead of the arrow `<-`.\n",
    "\n",
    "Let’s look at a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c178708",
   "metadata": {},
   "source": [
    "### Exercise 3 – Creating Variables\n",
    "\n",
    "The cell below already defines a variable called `name`.  \n",
    "Run it once, and then extend it by:\n",
    "1. Adding another variable `age`, and  \n",
    "2. Extending the *f-string* so it prints both your name and your age.\n",
    "\n",
    "*(If you’ve never seen an f-string before, it’s a way to mix text and variables in one sentence.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (works)\n",
    "name = \"Ada\"\n",
    "print(f\"My name is {name}.\")  # TODO: also show your age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85339347",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86aca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Ada\"\n",
    "age = 30\n",
    "print(f\"My name is {name} and I am {age} years old.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f10ca0",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "- The `f` before the string lets you place variables inside `{}`.  \n",
    "- Try changing `name` or `age` and run the cell again.  \n",
    "- Later, we’ll use f-strings to build URLs dynamically — for instance:  \n",
    "  ```python\n",
    "  url = f\"https://api.example.com/users/{user_id}\"\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc41c451",
   "metadata": {},
   "source": [
    "## 2.2 Lists and Dictionaries – Flexible Data Structures\n",
    "\n",
    "Web data rarely comes in neat tables.  \n",
    "Often, each record (a product, a tweet, a post) contains multiple pieces of information.  \n",
    "In Python, we can store such data using **lists** and **dictionaries**.\n",
    "\n",
    "- A **list** is like an ordered collection — similar to a vector in R.  \n",
    "- A **dictionary** is like a named list or a tiny spreadsheet row: it stores *key-value pairs*.\n",
    "\n",
    "Let’s see what that looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b5f08",
   "metadata": {},
   "source": [
    "### Exercise 4a – Exploring a List of Dictionaries\n",
    "\n",
    "Here’s an example list of artists.  \n",
    "Run the cell below and look carefully at what prints out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = [\n",
    "    {\"name\": \"Adele\", \"genre\": \"Pop\"},\n",
    "    {\"name\": \"Kendrick Lamar\", \"genre\": \"Hip-Hop\"}\n",
    "]\n",
    "print(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967986a",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "The square brackets `[]` mean “this is a list”,  \n",
    "and the curly brackets `{}` show “this is one dictionary (record)”.  \n",
    "Each dictionary has two keys: `name` and `genre`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09a0cc",
   "metadata": {},
   "source": [
    "### Exercise 4b – Adding Another Item\n",
    "\n",
    "Let’s extend the list.  \n",
    "Add a third artist to the list (remember commas between entries) and then re-run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (works)\n",
    "artists = [\n",
    "    {\"name\": \"Adele\", \"genre\": \"Pop\"},\n",
    "    {\"name\": \"Kendrick Lamar\", \"genre\": \"Hip-Hop\"}\n",
    "    # TODO: add another artist here\n",
    "]\n",
    "print(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2e7aa",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = [\n",
    "    {\"name\": \"Adele\", \"genre\": \"Pop\"},\n",
    "    {\"name\": \"Kendrick Lamar\", \"genre\": \"Hip-Hop\"},\n",
    "    {\"name\": \"Taylor Swift\", \"genre\": \"Pop\"}\n",
    "]\n",
    "print(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df0ee33",
   "metadata": {},
   "source": [
    "**Why this matters for web data**  \n",
    "When we later scrape sites like Spotify or IMDb, each API response is basically a big dictionary or a list of dictionaries.  \n",
    "For example:  \n",
    "```python\n",
    "track = {\"title\": \"Hello\", \"artist\": \"Adele\", \"streams\": 520000000}\n",
    "```\n",
    "Tables can’t easily handle this nested structure — dictionaries can.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c43df",
   "metadata": {},
   "source": [
    "## 2.3 Control Flow – Repeating Web Actions\n",
    "\n",
    "When you collect data, you’ll often repeat the same action for many pages or items.  \n",
    "That’s where **loops** come in.\n",
    "\n",
    "- **for loops** repeat actions for a known list of items.  \n",
    "- **while loops** repeat until a condition is no longer true — handy when you don’t know how many pages there are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40188434",
   "metadata": {},
   "source": [
    "### Exercise 5 – for loops\n",
    "\n",
    "The code below prints each artist’s name.  \n",
    "Modify it so it also prints the artist’s genre next to their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d471081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "for artist in artists:\n",
    "    print(artist[\"name\"])  # TODO: include genre too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e2f18",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4884b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist in artists:\n",
    "    print(f\"{artist['name']} – {artist['genre']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1566e",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "- `for artist in artists:` goes through the list one record at a time.  \n",
    "- Inside the loop, we use dictionary keys (`['name']`, `['genre']`) to access each field.  \n",
    "- In real scraping, each iteration could download a different webpage or API response.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102cabbc",
   "metadata": {},
   "source": [
    "### Exercise 6 – while loops\n",
    "\n",
    "Sometimes we don’t know how many pages exist — imagine a “next page” button at the bottom of a site.  \n",
    "The `while` loop keeps running as long as a certain condition is true.\n",
    "\n",
    "Try running this example.  \n",
    "It simulates visiting pages 1 to 3 and then stopping automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec01e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "page = 1\n",
    "has_more_pages = True\n",
    "\n",
    "while has_more_pages:\n",
    "    print(f\"Scraping page {page} …\")\n",
    "    page += 1\n",
    "    if page > 3:   # simulate last page\n",
    "        has_more_pages = False\n",
    "\n",
    "print(\"No more pages left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40384c20",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "The loop keeps going until the condition `has_more_pages` turns False.  \n",
    "This is a common pattern when scraping — you can keep clicking “next” until there is none.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e79e5a",
   "metadata": {},
   "source": [
    "## 2.4 Functions – Reusing Code\n",
    "\n",
    "As you start writing longer scripts, you’ll notice you often copy the same few lines of code.  \n",
    "Instead of repeating them, we can bundle them into a **function**.\n",
    "\n",
    "Functions help you stay organized and make your code easier to read and debug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4ddff",
   "metadata": {},
   "source": [
    "### Exercise 7 – Writing a Simple Function\n",
    "\n",
    "Write a function called `fetch_data(url)` that prints  \n",
    "“Fetching data from &lt;url&gt;…”.  \n",
    "Then call it for each URL in the list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3bfa0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "def fetch_data(url):\n",
    "    # TODO: print message that includes the URL\n",
    "    pass\n",
    "\n",
    "urls = [\"page1.html\", \"page2.html\", \"page3.html\"]\n",
    "for u in urls:\n",
    "    fetch_data(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01edcce",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2504814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url):\n",
    "    print(f\"Fetching data from {url} …\")\n",
    "\n",
    "urls = [\"page1.html\", \"page2.html\", \"page3.html\"]\n",
    "for u in urls:\n",
    "    fetch_data(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe7587",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "- We define functions with `def function_name(parameters):`.  \n",
    "- Indentation matters — everything inside the function is indented.  \n",
    "- Functions let us reuse logic (like downloading data from a URL) without copy-pasting it each time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ad1c1",
   "metadata": {},
   "source": [
    "## 2.5 Saving Data – CSV and JSON\n",
    "\n",
    "Once you’ve collected data, you’ll want to save it — either as a flat CSV file or as a structured JSON file.  \n",
    "Both formats are human-readable and easy to share."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cb09a7",
   "metadata": {},
   "source": [
    "### Exercise 8 – Writing a CSV File\n",
    "\n",
    "Let’s save our artists to a file called `artists.csv`.  \n",
    "Add a header row and a couple of artist rows.  \n",
    "After running it, open the file in Excel or a text editor to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "import csv\n",
    "with open(\"artists.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"artist\", \"genre\"])\n",
    "    writer.writerow([\"Adele\", \"Pop\"])  # TODO: add another row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070525b6",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"artists.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"artist\", \"genre\"])\n",
    "    writer.writerow([\"Adele\", \"Pop\"])\n",
    "    writer.writerow([\"Kendrick Lamar\", \"Hip-Hop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6923481",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "Each `writerow()` adds a line to the CSV file.  \n",
    "This is exactly how you’ll store scraped data later — one row per record.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ac5f7a",
   "metadata": {},
   "source": [
    "### Exercise 9 – Writing a JSON File\n",
    "\n",
    "Now let’s do the same thing in JSON format — the standard for APIs and web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starter code (runs)\n",
    "import json\n",
    "artists_data = [\n",
    "    {\"artist\": \"Adele\", \"genre\": \"Pop\"},\n",
    "    {\"artist\": \"Kendrick Lamar\", \"genre\": \"Hip-Hop\"}\n",
    "]\n",
    "with open(\"artists.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(artists_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1757421c",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "artists_data = [\n",
    "    {\"artist\": \"Adele\", \"genre\": \"Pop\"},\n",
    "    {\"artist\": \"Kendrick Lamar\", \"genre\": \"Hip-Hop\"},\n",
    "    {\"artist\": \"Taylor Swift\", \"genre\": \"Pop\", \"country\": \"US\"}\n",
    "]\n",
    "with open(\"artists.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(artists_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86eaf7f",
   "metadata": {},
   "source": [
    "**Explanation**  \n",
    "- `json.dump()` writes structured data in the same format most web APIs use.  \n",
    "- Notice how we could add an extra field (`country`) without breaking anything — that’s why JSON is so flexible.\n",
    "\n",
    "---\n",
    "\n",
    "| Feature | CSV | JSON |\n",
    "|:--|:--|:--|\n",
    "| Structure | Flat (rows + columns) | Nested (key-value) |\n",
    "| Good for Excel | ✅ Yes | ⚠️ Limited |\n",
    "| Good for APIs | ❌ No | ✅ Yes |\n",
    "| Human Readable | ✅ | ✅ |\n",
    "\n",
    "---\n",
    "\n",
    "**Tip:** You’ll often use both — JSON to store raw API responses and CSV to summarize cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e70cf9",
   "metadata": {},
   "source": [
    "## 2.6 Summary\n",
    "\n",
    "Great work — you’ve covered a lot!  \n",
    "By now you can:\n",
    "- Create variables and combine them with text using f-strings.  \n",
    "- Store information in lists and dictionaries.  \n",
    "- Repeat actions with loops and organize code with functions.  \n",
    "- Save data in both CSV and JSON formats.  \n",
    "\n",
    "These are the core skills you’ll need for our next topic: **Understanding Web Data** — where we’ll start talking about what HTML and APIs actually look like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf8141c",
   "metadata": {},
   "source": [
    "# 3  Understanding Web Data\n",
    "\n",
    "You now know how to code, loop, and store data – time to apply these skills to the *web*!  \n",
    "In this part we’ll learn how to:\n",
    "- Fetch information from websites and APIs  \n",
    "- Extract what we need  \n",
    "- Add timestamps  \n",
    "- Store everything in files  \n",
    "- Repeat the process automatically using loops\n",
    "\n",
    "You’ll see that the ideas from Section 2 (variables, loops, files) appear again here – just applied to *web data*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25f0de8",
   "metadata": {},
   "source": [
    "## 3.1  Web Scraping vs APIs – How Data Travels on the Web\n",
    "\n",
    "So far, we’ve worked with data that already lived on our computer — numbers, strings, CSV files.  \n",
    "But the real fun begins when we get data **from the web**.\n",
    "\n",
    "Every time you open a website, two things happen:\n",
    "\n",
    "1. Your **browser** sends a *request* to a **server**.\n",
    "2. The **server** replies with a *response* — often a big chunk of text, called **HTML**.\n",
    "\n",
    "HTML describes what the browser should display — the layout, the colors, the content, everything.  \n",
    "But we can also read that text with Python and extract specific information from it, like song titles or artist names.\n",
    "\n",
    "There’s another way websites provide data: through **APIs** (Application Programming Interfaces).  \n",
    "Instead of HTML, an API sends back structured data in a format called **JSON**.  \n",
    "JSON is like a digital spreadsheet — organized, consistent, and easy for Python to read.\n",
    "\n",
    "You can think of the difference like this:\n",
    "\n",
    "| For humans | For machines |\n",
    "|-------------|--------------|\n",
    "| 🧍 Websites → HTML pages | 🤖 APIs → JSON data |\n",
    "\n",
    "Both contain information — but one is formatted for your *eyes*, the other for your *code*.\n",
    "\n",
    "![The Web Data Workflow](images/web_data_workflow.png)\n",
    "> *Figure 1.* The Web Data Workflow – four layers that structure how we collect web data.  \n",
    "> In this tutorial, we’ll focus on the first two layers: **Extraction** and **Looping**.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- **Web scraping:** collecting hotel prices, comparing booking sites, reading product reviews.  \n",
    "- **APIs:** accessing Spotify song data, querying OpenAI for text, or using the Google Maps API for locations.\n",
    "\n",
    "The good news? You already know most of the programming tools we’ll need:  \n",
    "loops to repeat actions, variables to store results, and files to save them.\n",
    "\n",
    "Now let’s see what these two worlds — HTML and APIs — look like in practice.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 10 – Spot the Difference\n",
    "\n",
    "1. Open [`https://music-to-scrape.org`](https://music-to-scrape.org) in your browser.  \n",
    "   Right-click and choose **Inspect Element** — this shows you the HTML that the server sent.\n",
    "2. Now open [`https://music-to-scrape.org/api/songs`](https://music-to-scrape.org/api/songs).  \n",
    "   You’ll see raw JSON — not pretty, but very structured.\n",
    "3. Compare:  \n",
    "   - Which version looks nicer for humans?  \n",
    "   - Which one is easier for a computer to understand?\n",
    "4. That’s the difference between **web scraping** and **APIs**.  \n",
    "   Both give you access to the same kind of information — but in different formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9ec73",
   "metadata": {},
   "source": [
    "## 3.2  Web Scraping in Action (HTML Extraction + Storage + Timestamps)\n",
    "\n",
    "Let’s put our Python skills to work and scrape real data from the web!  \n",
    "We’ll use a demo site, [music-to-scrape.org](https://music-to-scrape.org), which lists songs and artists.\n",
    "\n",
    "Our goal:  \n",
    "- Extract the list of songs  \n",
    "- Add the time of collection  \n",
    "- Store the results in a CSV file  \n",
    "\n",
    "---\n",
    "### Concept: How Web Scraping Works\n",
    "\n",
    "When your browser opens a website, it receives **HTML** — text that describes what’s on the page.  \n",
    "We can ask Python to fetch that same HTML, then use a helper library called **BeautifulSoup** to “read” it.\n",
    "\n",
    "BeautifulSoup turns HTML into something we can search through, like:  \n",
    "“Find every `<div>` with the class name `song`.”\n",
    "\n",
    "Let’s try it together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227562f",
   "metadata": {},
   "source": [
    "### Step 1 – Fetch and Parse the Page\n",
    "\n",
    "First, we’ll fetch the web page and inspect the first song block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266de880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://music-to-scrape.org\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find one song element\n",
    "first_song = soup.find(\"div\", class_=\"song\")\n",
    "print(\"Example song element ⬇️\")\n",
    "print(first_song.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d09777",
   "metadata": {},
   "source": [
    "You’ve just fetched a web page and extracted the first song title. 🎉  \n",
    "The next step is to get *all* songs on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07669b",
   "metadata": {},
   "source": [
    "### Exercise 11 – Collect All Songs\n",
    "\n",
    "Your turn!  \n",
    "Modify the code above so it collects **all** songs.\n",
    "\n",
    "**Hints**\n",
    "- Use `.find_all()` instead of `.find()`.  \n",
    "- Loop through the results and append each song’s text to a list called `songs`.  \n",
    "- Print how many songs you found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e588fe74",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3f71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = []\n",
    "for item in soup.find_all(\"div\", class_=\"song\"):\n",
    "    songs.append(item.get_text(strip=True))\n",
    "\n",
    "print(f\"Found {len(songs)} songs.\")\n",
    "print(\"First few songs:\", songs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec205b2",
   "metadata": {},
   "source": [
    "Well done — you’ve just turned a web page into a Python list!  \n",
    "That’s the core idea of **web scraping**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3f2cc",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2 – Add a Timestamp\n",
    "\n",
    "In data projects, it’s important to record *when* you collected the data.  \n",
    "That helps others (and future you!) reproduce or compare results later.\n",
    "\n",
    "Python’s `time` module can generate timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94749183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = time.time()\n",
    "print(\"Current Unix time:\", timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72aa110e",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "**What’s Unix time?**  \n",
    "It counts the seconds since 1 January 1970 — a universal format computers use to record moments in time.  \n",
    "It’s perfect for logging when your data was collected.\n",
    ":::\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 12 – Add Timestamps and Save to CSV\n",
    "\n",
    "Let’s save our scraped songs to a CSV file — but before writing,  \n",
    "**add a timestamp** to each record.\n",
    "\n",
    "**Hints**\n",
    "- You can create a list like `rows = [[song, timestamp] for song in songs]`.  \n",
    "- Use the `csv` library to write a header (`title`, `timestamp`) and the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d007fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### ✅ Solution – Exercise 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Add timestamp to each song\n",
    "timestamp = time.time()\n",
    "rows = [[song, timestamp] for song in songs]\n",
    "\n",
    "# Write to CSV\n",
    "with open(\"songs.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"title\", \"timestamp\"])\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(\"✅  Saved songs.csv with titles and timestamps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dae46a",
   "metadata": {},
   "source": [
    "Excellent!  \n",
    "You’ve now completed the **Extraction** part of the Web Data Workflow:\n",
    "\n",
    "1️⃣ Fetched a page using `requests`  \n",
    "2️⃣ Parsed the HTML with BeautifulSoup  \n",
    "3️⃣ Added timestamps to record collection time  \n",
    "4️⃣ Stored the data in a CSV file  \n",
    "\n",
    "Next, we’ll see how to collect structured data more directly — through **APIs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17f079",
   "metadata": {},
   "source": [
    "## 3.3  APIs in Action (Fetching Structured Web Data)\n",
    "\n",
    "In the previous section, we scraped a web page — we asked Python to read and interpret the HTML of a site.  \n",
    "Now we’ll explore an alternative (and often better) way to get web data: **APIs**.\n",
    "\n",
    "---\n",
    "### Concept: What is an API?\n",
    "\n",
    "An **API** (Application Programming Interface) is like a “data menu” for a website.  \n",
    "Instead of us digging through HTML, the website provides structured data — usually in **JSON** format — directly ready for analysis.\n",
    "\n",
    "Think of it this way:\n",
    "- A **website** is meant for *humans* (pretty, clickable, visual).  \n",
    "- An **API** is meant for *computers* (clean, predictable, machine-readable).\n",
    "\n",
    "Many companies provide APIs:  \n",
    "- OpenAI (for language models)  \n",
    "- Spotify (for song metadata)  \n",
    "- Twitter/X, Instagram, or Reddit (for social data)  \n",
    "\n",
    "We’ll again use the `music-to-scrape.org` demo API, which returns information about artists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a83bc2",
   "metadata": {},
   "source": [
    "### Step 1 – Fetch Data from an API\n",
    "\n",
    "The endpoint below returns JSON data describing featured artists on the site.  \n",
    "Let’s make a request and print the first few items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.music-to-scrape.org/artists/featured\"\n",
    "response = requests.get(url)\n",
    "data = response.json()   # Convert JSON text into a Python dictionary\n",
    "\n",
    "# Show part of the structure\n",
    "print(\"Top-level keys:\", data.keys())\n",
    "print(\"First artist entry:\")\n",
    "print(data[\"artists\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45a7cc",
   "metadata": {},
   "source": [
    "See the difference?  \n",
    "With web scraping, we had to search through messy HTML.  \n",
    "With an API, the data comes pre-structured in dictionaries and lists — ready for analysis.\n",
    "\n",
    "---\n",
    "### Exercise 13 – Extract Artist Names\n",
    "\n",
    "Look at the variable `data`. It contains a list under `data[\"artists\"]`.  \n",
    "Each artist is a dictionary with several attributes (e.g., `artist_name`, `artist_id`).\n",
    "\n",
    "Your task:  \n",
    "- Loop through all artists in `data[\"artists\"]`.  \n",
    "- Collect the names into a list called `artist_names`.  \n",
    "- Print the number of artists and the first few names.\n",
    "\n",
    "*(You can reuse ideas from Section 3.2 where you looped through songs.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e827468c",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_names = []\n",
    "for artist in data[\"artists\"]:\n",
    "    artist_names.append(artist[\"artist_name\"])\n",
    "\n",
    "print(f\"Found {len(artist_names)} artists.\")\n",
    "print(\"First few:\", artist_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb2aa4",
   "metadata": {},
   "source": [
    "Great!  \n",
    "You’ve just extracted structured information from an API.\n",
    "\n",
    "---\n",
    "### Step 2 – Combine with a Loop and Delay\n",
    "\n",
    "Sometimes APIs allow you to request different resources by ID.  \n",
    "For example, each artist has a unique `artist_id`.  \n",
    "We can use these IDs to fetch more detailed information — but we should be polite and wait between requests.\n",
    "\n",
    "That’s where a small **delay** (`time.sleep()`) comes in.  \n",
    "It prevents us from overwhelming the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b051f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Preview one artist ID\n",
    "print(\"Example ID:\", data[\"artists\"][0][\"artist_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b390d2ce",
   "metadata": {},
   "source": [
    "Let’s loop through three artists and fetch their info pages.\n",
    "\n",
    "This code runs a few requests and prints the artist IDs found in each response.\n",
    "The delay of one second keeps things polite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f657b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {'User-Agent': 'Python Bootcamp student'}\n",
    "\n",
    "artists = ['ARICCN811C8A41750F', 'AR1GW0U1187B9B29FD', 'ARZ3U0K1187B999BF4']\n",
    "\n",
    "for artist in artists:\n",
    "    print(f\"Fetching artist {artist} ...\")\n",
    "    url = f\"https://api.music-to-scrape.org/artist/info?artistid={artist}\"\n",
    "    response = requests.get(url, headers=header)\n",
    "    info = response.json()\n",
    "    print(info)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca0c07",
   "metadata": {},
   "source": [
    "Notice how similar this looks to our scraping code — but here, we don’t search the page with BeautifulSoup.  \n",
    "The API directly gives us the data we need.\n",
    "\n",
    "---\n",
    "### Exercise 14 – Add a Timestamp and Store API Results\n",
    "\n",
    "Your turn!  \n",
    "Extend the loop above so that:\n",
    "\n",
    "- Each artist dictionary also stores a new key `\"timestamp\"` using `time.time()`.  \n",
    "- You collect all enriched artist dictionaries in a list called `records`.  \n",
    "- You then write `records` to a JSON file called `artists.json`.\n",
    "\n",
    "**Hint:** Use `json.dump(records, file)` from the `json` module to store the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0731aea",
   "metadata": {},
   "source": [
    "### ✅ Solution – Exercise 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29136548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "records = []\n",
    "\n",
    "for artist in artists:\n",
    "    url = f\"https://api.music-to-scrape.org/artist/info?artistid={artist}\"\n",
    "    response = requests.get(url, headers=header)\n",
    "    info = response.json()\n",
    "    info[\"timestamp\"] = time.time()\n",
    "    records.append(info)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"artists.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Saved artists.json with timestamps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696af94",
   "metadata": {},
   "source": [
    "---\n",
    "### What You’ve Learned\n",
    "\n",
    "In this section, you’ve seen how APIs complement web scraping:\n",
    "\n",
    "| Task | Web Scraping | API |\n",
    "|------|---------------|-----|\n",
    "| Data format | HTML | JSON |\n",
    "| For humans or machines? | Humans | Machines |\n",
    "| Libraries used | `requests`, `BeautifulSoup` | `requests`, `json` |\n",
    "| Common in | Small websites | Big data platforms (e.g., OpenAI, Spotify) |\n",
    "\n",
    "---\n",
    "**Recap**\n",
    "\n",
    "1️⃣ You fetched data directly from an API endpoint.  \n",
    "2️⃣ You looped through results and added timestamps.  \n",
    "3️⃣ You saved your results as a JSON file.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83435c0c",
   "metadata": {},
   "source": [
    "## 4.  Wrap-Up and Reflection\n",
    "\n",
    "Congratulations — you’ve completed your first full journey from **starting a Python environment** to **collecting and storing real-world data**.  \n",
    "Across the three parts of this tutorial, you learned to:\n",
    "\n",
    "- **Set up and navigate** Python using Jupyter / VS Code / Google Colab.  \n",
    "- **Transfer your R knowledge** to Python — understanding variables, loops, and data structures.  \n",
    "- **Fetch and parse** information from the web using `requests` and `BeautifulSoup`.  \n",
    "- **Retrieve structured data** from APIs and store it as CSV or JSON.  \n",
    "- **Enrich results** with timestamps and write clean, reproducible code.\n",
    "\n",
    "Each section built on the previous one: first you learned *how to run Python*, then *how to structure code*, and finally *how to apply it to real web data*.\n",
    "\n",
    "---\n",
    "### Summary of What You Can Now Do\n",
    "\n",
    "| Step | Skill | Key Tools | What You Achieved |\n",
    "|------|--------|------------|-------------------|\n",
    "| **1 – Getting Started** | Launch and manage Python environments | Anaconda / VS Code / Colab | Run and edit notebooks locally or in the cloud |\n",
    "| **2 – Python Basics** | Variables, loops, file I/O | `requests`, `csv`, `json`, `time` | Reused R-style logic in Python, saved small datasets |\n",
    "| **3 – Web Data Collection** | Web scraping & APIs | `BeautifulSoup`, `requests`, `json` | Extracted songs & artists, added timestamps, stored results |\n",
    "\n",
    "---\n",
    "### Looking Ahead\n",
    "\n",
    "In the next tutorials, you’ll **go deeper**:\n",
    "- Automating larger scraping tasks and multi-page loops  \n",
    "- Handling structured APIs with authentication  \n",
    "- Cleaning, merging, and visualizing your collected data  \n",
    "\n",
    "Before that, take a moment to review your own notebook:\n",
    "- Are all cells executed top-to-bottom without errors?  \n",
    "- Did you comment your code clearly enough that future-you understands it?  \n",
    "- Can you explain, in your own words, the difference between scraping and an API?\n",
    "\n",
    "If yes — you’re ready for the next level.\n",
    "\n",
    "::: {.callout-note}\n",
    "__Takeaway__\n",
    "\n",
    " Every dataset on the web — whether hidden in HTML or exposed through an API — can be reached, understood, and stored with a few lines of clean, reproducible Python code.\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
