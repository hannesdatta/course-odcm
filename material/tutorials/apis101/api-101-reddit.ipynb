{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIs 101 (oDCM)\n",
    "\n",
    "*The focus in this tutorial lies on pagination (i.e., looping through multiple \"pages\"), and parameters (i.e., telling the API what you really want).*\n",
    "\n",
    "*This tutorial uses two data sources: a somewhat \"simple\" one that serves you dad jokes, and - directly from the real world - the [Reddit.com]() API.*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Send HTTP requests to a web API, and retrieve JSON responses\n",
    "* Use parameters to modify the results of an API call\n",
    "* Iterate over multiple pages of JSON responses \n",
    "* Extract and store results of an API request in lists and files\n",
    "\n",
    "--- \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Icanhazdadjoke\n",
    "\n",
    "### 1.1 Make an API request\n",
    "\n",
    "[Icanhazdadjoke.com](https://icanhazdadjoke.com) is a simple web site that allows users of their API to receive (randomized) *dad jokes*. Yes, we know that sounds stupid, but we like that API for its simplicity, which is ideal when explaining to you more about APIs.\n",
    "\n",
    "So, the code cell below calls the joke API, and the result of the API request displays a joke. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Run the cell a few times to notice that with each call, you see a new joke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request JSON output from icanhazdadjoke API\n",
    "import requests\n",
    "url = \"https://icanhazdadjoke.com\"\n",
    "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "joke_request = response.json() \n",
    "print(joke_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Use parameters to modify the API results   \n",
    "\n",
    "__Importance__\n",
    "\n",
    "Probably you agree that dad jokes aren't that exciting. Wouldn't it be amazing to search for __particular jokes instead__?\n",
    "\n",
    "APIs usually provide you with an option to *customize* requests. That's where APIs make most of a difference! You have probably already modified the results of an API call a dozen times without even knowing it. For example, if you Google the word `cat`, the results page may look something like this:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/apis101/images/google.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you note that the URL in the browser starts with [`google.com/search?q=cat...`](https://www.google.com/search?q=cat)? \n",
    "\n",
    "What happened here is that your search query was passed to the Google Search API, and hence returned the results of the search query `cat`. That search query is even already embedded in the link itself. Cool, right?\n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "So, rather than filling out the search box on the website of Icanhazdadjoke.com itself, you can also tweak it in the URL directly. Open your browser now at [https://icanhazdadjoke.com/search?term=cat](https://icanhazdadjoke.com/search?term=cat), and modify the `term` parameter to try a search for different jokes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/apis101/images/cat_jokes.gif\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the idea of passing parameters to a website, we can update the `search_url` and include the `params` attribute, which contains a dictionary with parameters that further specify our request. Run the cell below to see cat jokes here in Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "response = requests.get(search_url, headers={\"Accept\": \"application/json\"}, \n",
    "                        params={\"term\": \"cat\"})\n",
    "joke_request = response.json()\n",
    "print(joke_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `joke_request` object now contains a list with all cat-related jokes (`joke_request['results']`), the search term (`cat`), and the total number of jokes (`10`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "1. Change the search term parameter to `dog` and revisit `joke_request['results']`. How many dog jokes are there? \n",
    "2. Write a function `find_joke()` that takes a query as an input parameter and returns the number of jokes from the `icanhazdadjoke` search API (tip: use your answer to question 1 as a starting point!). \n",
    "\n",
    "__Tips:__\n",
    "\n",
    "- Rather than writing `joke_request['results']`, you can also use the function `get()` to retrieve the particular attribute: `joke_request.get('results')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 \n",
    "search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "response = requests.get(search_url, headers={\"Accept\": \"application/json\"}, \n",
    "                        params={\"term\": \"dog\"})\n",
    "joke_request = response.json()\n",
    "print(f\"The number of dog jokes is: {joke_request.get('total_jokes')}\")\n",
    "# or alternatively: print(f\"The number of dog jokes is: {joke_request['total_jokes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "def find_jokes(term):\n",
    "    search_url = \"https://icanhazdadjoke.com/search\"\n",
    "\n",
    "    response = requests.get(search_url, headers={\"Accept\": \"application/json\"}, \n",
    "                            params={\"term\": term})\n",
    "    joke_request = response.json()\n",
    "    return joke_request.get('total_jokes')\n",
    "\n",
    "find_jokes(\"some-searchterm-you-would-like-to-try-out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "Transferring data is costly - not strictly in a monetary sense, but in *time*. So - APIs are typically very greedy in returning data. Ideally, they only produce a very targeted data point that t to see. On icanhazdadjoke.com, for example, that would be a few jokes at maximum. It saves the website owner paying for bandwidth and guarantees that the site responds fast to user input (such as navigating the site or searching for jokes).\n",
    "\n",
    "However, when using APIs for research purposes, we are frequently interested in obtaining *everything*. What's the use, for example, to get a book's most recent ten reviews, if there are hundreds of reviews written?\n",
    "\n",
    "We think you see where we're going with this... \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "So, let's grab all 700+ jokes currently available at Icanhazdadjoke.com. The API output, unfortunately, only shows the *first 20 jokes*. To retrieve the remaining jokes, you need *pagination*. The API divides the data into smaller subsets that can be accessed on various pages, rather than returning all output at once. \n",
    "\n",
    "Let's retrieve the first batch of dad jokes (note, here we're searching for the `term` `\"\"` - an empty string - which brings us to the entire set of jokes available via the API. In practice, searching for `\"\"` is often blocked by APIs - simply because the site doesn't *want* you to extract a complete copy of their data. In that case, you'd have to become creative to obtain your seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://icanhazdadjoke.com/search\", headers={\"Accept\": \"application/json\"}, \n",
    "                        params={\"term\": \"\"})\n",
    "joke_request = response.json()\n",
    "joke_request['results'] = '' # let's remove all jokes, and only look at the other attributes in the JSON response\n",
    "joke_request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that by default, each page contains 20 jokes (see `limit` in the JSON response above), where page 1 shows jokes 1 to 20, page 2 jokes 21 to 40, and so forth.\n",
    "\n",
    "You can adjust the number of results on each page (max. 30) with the `limit` parameter (e.g., `params={\"limit\": 10}`). In practice, almost every API on the web limits the results of an API call (`100` is also a common cap).\n",
    "\n",
    "In the example below, we set `limit` equal to `10`, `20`, and `30`, and see how it affects the number of total pages (`total_pages`) on which jokes are listed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for limit in range(10, 31, 10):  # note that range(a, b) runs from a to b-1; so the last value is exclusive (so from 10 to 30 with steps of 10)\n",
    "    response = requests.get(search_url, \n",
    "                            headers={\"Accept\": \"application/json\"}, \n",
    "                            params={\"term\": \"\", \n",
    "                                   \"limit\": limit})\n",
    "    joke_request = response.json()\n",
    "    print(f\"Limit {limit} gives {joke_request['total_pages']} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we find that the higher the limit, the more results fit on a single page, and thus the *lower the number of pages* to loop through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Exercise 2\n",
    "\n",
    "In addition to the `limit` parameter, you can specify the current page number with the `page` parameter (e.g., `params={\"term\": \"\", \"page\": 2}`. See the example in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(search_url, \n",
    "                            headers={\"Accept\": \"application/json\"}, \n",
    "                            params={\"term\": \"\", \n",
    "                                   \"limit\": 5,\n",
    "                                   \"page\": 2})\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the function `find_joke()` (see question 2 of exercise 1), such that it loops over *all available pages*, and stores the ids and jokes in a list. You can leave the `limit` parameter at its default value (20). Make sure that your function also works when you pass it a search `term`. \n",
    "\n",
    "Tip: To determine how many pages you need to loop through, you can use the `total_pages` field (e.g., there are only ten cat jokes, so in that case, 1 page would suffice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_jokes(term):\n",
    "    search_url = \"https://icanhazdadjoke.com/search\"\n",
    "    page = 1\n",
    "    jokes = []\n",
    "\n",
    "    while True:  # alternatively you can also use a for-loop that goes from page 1 to total_results / 20 (rounded up)\n",
    "        response = requests.get(search_url, \n",
    "                                headers={\"Accept\": \"application/json\"}, \n",
    "                                params={\"term\": term,  # optionally you can add \"limit\": 20 but that's already the default so it doesn't change anything\n",
    "                                        \"page\": page})\n",
    "        joke_request = response.json()\n",
    "        jokes.extend(joke_request['results']) # the .extend() function extends the list of jokes (jokes = []) with the result. Unlike append, it adds the entire list of results (rather than just one item).\n",
    "        if joke_request['current_page'] <= joke_request['total_pages']:\n",
    "            page += 1\n",
    "        else: \n",
    "            return jokes\n",
    "\n",
    "output = find_jokes(\"cat\") # try running it with \"\", too! (and wait for some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"You've collected {len(output)} jokes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Wrap-up\n",
    "\n",
    "To sum up, we have seen how *parameters* can be a powerful tool when working with APIs. They allow you to tailor your request to be more specific or loop through multiple pages. \n",
    "\n",
    "In the API documentation, you typically find more information about the available parameters and the values they can take on. For example, the `icanhazdadjoke` [documentation](https://icanhazdadjoke.com/api) includes a section on the `/search` endpoint and the accepted parameters (`page`, `limit`, `term`). These parameters, however, differ from one API to another. So it's crucial to study each web service's API documentation carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 2. Reddit\n",
    "\n",
    "### 2.1 Subreddits\n",
    "\n",
    "[Reddit](https://reddit.com) is a widespread social news aggregation and discussion site. The service uses an API to generate the website's content and grants public access to the API.\n",
    "\n",
    "In this tutorial, we zoom in on \"subreddits\", which are niche communities centered around a particular topic. Users can nearly post anything in these subreddits, and you'd be surprised to find out what people are talking about. For example, see below for a screenshot of the [subreddit on Science](https://www.reddit.com/r/Science).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/apis101/images/reddit_science.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Subreddits all start with `reddit.com/r/...`. Here are a few examples:\n",
    "\n",
    "- [askreddit](https://www.reddit.com/r/AskReddit), \n",
    "- [aww](https://www.reddit.com/r/aww/), \n",
    "- [gifs](https://www.reddit.com/r/gifs/), \n",
    "- [showerthoughts](https://www.reddit.com/r/Showerthoughts), \n",
    "- [lifehacks](https://www.reddit.com/r/lifehacks), \n",
    "- [getmotivated](https://www.reddit.com/r/GetMotivated), \n",
    "- [moviedetails](https://www.reddit.com/r/MovieDetails), \n",
    "- [todayilearned](https://www.reddit.com/r/todayilearned/), \n",
    "- [foodporn](https://www.reddit.com/r/FoodPorn/). \n",
    "\n",
    "Take your time to browse through some of the subreddits, and get familiar with the structure of the pages.\n",
    "\n",
    "After a while, you'd probably notice that subreddits are hosted by moderators, who monitor whether the posts adhere to a set of (informal) rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/apis101/images/reddit_moderators.png\" width=60% align=\"left\"  style=\"border: 1px solid black\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, links to papers you share in [`r/science`](https://www.reddit.com/r/science/) must be less than 6 months old. \n",
    "\n",
    "Other users can join a subreddit so that they receive updates about new posts and comments.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Consult the [`marketing`](https://www.reddit.com/r/marketing/hot/) subreddit and answer the following questions: \n",
    "1. For your thesis, you need to collect survey responses. Are you allowed to share a link to your survey in this subreddit? Please elaborate on how you came to this conclusion. \n",
    "2. You post a link (and wonder how many users will potentially be able to see your post). How many users are subscribed to the subreddit? How many users are currently online?\n",
    "3. Like other social media platforms, you can navigate towards Reddit's user-profiles and learn more about these persons. Inspect the profile of one of the users who has posted on the Reddit (actually, it is one of the moderators), [`sixwaystop313`](https://www.reddit.com/user/sixwaystop313). Describe in your own words what types of information you can gather from this user. How is the feed organized?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. No, the subreddit rules prescribe users not to post surveys and homework assignments (right sidebar).\n",
    "2. [`r/marketing`](https://reddit.com/r/marketing) is moderated has 460k+ members, and (at the time of writing this tutorial), about 244 of them were online.\n",
    "3. On a user page, you find the bio, trophies, communities the user moderates, connected accounts, and most importantly: all user's posts and comments.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 API headers  \n",
    "\n",
    "**Importance**  \n",
    "\n",
    "Let's now obtain some of the data we have seen on the \"About\" page of the `marketing` thread, using the  Reddit API. \n",
    "\n",
    "To request data from the Reddit API, we need to include `headers` in our request. HTTP headers are a vital part of any API request, containing *meta data associated with the request* (e.g., type of browser, language, expected data format, etc.). \n",
    "\n",
    "**Let's try it out**  \n",
    "\n",
    "Below we request the about page of the [`marketing`]() subreddit that includes such a header. We make our first request to the Reddit API and parse the output in the upcoming exercise!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.reddit.com/r/marketing/about/.json'\n",
    "\n",
    "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
    "response = requests.get(url, headers=headers)\n",
    "json_response = response.json()\n",
    "json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "1. First, take a look at the `json_response` object. Then, leave out the `headers` parameter in your request (so it becomes `requests.get(url)` instead), rerun the cell, and inspect the `json_response` another time. Are there any differences? \n",
    "2. Write a while-loop that prints the count of the number of currently active users of the `marketing` subreddit. Have your code pause every 5 seconds before refreshing. Stop the loop after 3 iterations. For pausing, use the function `time.sleep(5)`. Import the time package using `import time`.\n",
    "3. Convert your code from the previous exercise into a function `get_usercount()` that takes a `subreddit` as input and returns the total number of users, and the number of currently active users as a dictionary. Test your function for the `science`, `skateboarding`, and `marketing` subreddits. How many total and currently active users do these communities have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. Without the `headers` parameter, the API returns an error code (429). Headers are frequently used to track who is using the API. The user of the \"anonymous header\" has pushed the boundaries too much!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2 \n",
    "import time\n",
    "\n",
    "i = 1\n",
    "while i <= 3:\n",
    "    url = 'https://www.reddit.com/r/marketing/about/.json'\n",
    "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    json_response = response.json()\n",
    "    \n",
    "    print(json_response['data']['active_user_count'])\n",
    "    i += 1\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "def get_usercount(subreddit):\n",
    "    response = response = requests.get(f'https://www.reddit.com/r/{subreddit}/about/.json', headers=headers)\n",
    "    json_response = response.json()\n",
    "    out = {}\n",
    "    out['subreddit'] = subreddit\n",
    "    out['total_users'] = json_response['data']['subscribers']\n",
    "    out['active_users'] = json_response['data']['active_user_count']\n",
    "    return out\n",
    "    \n",
    "get_usercount('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_usercount('skateboarding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_usercount('marketing')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3 Profile pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to subreddits (`r/...`) and about pages (`.../about/`), Reddit users have their own profile page. Let's have another look at the marketing moderator [profile](https://www.reddit.com/user/sixwaystop313) we saw before. Each of the `children` in the `data` is characterized by a type (e.g., `t1` = comment, `t3` = post; for details see [API documentation](https://redditclient.readthedocs.io/en/latest/reference/)), subreddit, timestamp, number of comments, upvotes, downvotes, and many others. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Run the API call below, and browse the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = \"sixwaystop313\"\n",
    "response = requests.get(f'https://www.reddit.com/user/{mod}.json', headers=headers)\n",
    "json_response = response.json()\n",
    "json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a whole lot of output, which is difficult to go through. So let's copy it to a [JSON viewer](https://jsonviewer.stack.hu). Before we can do that, we have to replace the Pythonic `None`, `True` and `False` by strings (JSON viewer throws an error otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_response = json.loads(response.text.replace('null', '\"None\"').replace('True','\"True\"').replace('False','\"False\"'))\n",
    "json_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "1. The `json_response` object contains both comments and posts ordered chronologically (exactly as they appear on the profile page). Run the next cell to save the first occuring comment from the site (`kind`: `'t1'`) in a variable called `comment_post`. \n",
    "\n",
    "   Now, write some code to store the author name and text of the comment in a JSON dictionary (attributes: `author` and `comment_text`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_post = next(x for x in json_response.get('data').get('children') if x.get('kind')=='t1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How many objects are stored in `json_response['data']['children']`? What does that mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions\n",
    "1. At the moment of creating this solutions file, the 1st item in the list is a comment which we extract as follows:\n",
    "`comment_text = json_response['data']['children'][0]['data']`. In your case, it may be 2nd (or 3rd, 4th, ... item), however, provided that all other items in the lists are posts. For that reason, the counter after `[0]` may deviate from time to time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this solution throws a \"KeyError: body\" error it means the most recent JSON object is not a comment of kind t1 (so change the 0 for 1, 2, ... until it runs) - see question 2\n",
    "{'author': comment_post.get('data').get('author'),\n",
    " 'comment_text': comment_post.get('data').get('body')}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The object comprises 25 items, which implies that only the 25 most recent comments and posts are shown. Thus, we need to apply pagination to obtain historical records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_response['data']['children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Pagination\n",
    "\n",
    "__Importance__\n",
    "\n",
    "As you've just noticed, the API only returns a subset of all records (every time you scroll to the bottom of the page, it pulls in new data - ordered chronologically). After all, it would take ages to show all data for a user that has been active on Reddit since 2009! \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Similar to `icanhazdadjoke`, we apply pagination to tell the API which part of the data to return. The difference, however, is that it's not a number (like `\"page\": 2`) but a string of characters that can only be obtained from the previous request (i.e., we cannot derive what the next key will be from a pattern, like page 2, 3, ..., etc.). The request we already made contains this \"secret\" key in the attribute `after`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response['data']['after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we attach this key to our request with the `after` parameter to obtain the next subset of items and assign the responses to a variable called `json_response_after`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = json_response['data']['after']\n",
    "url = f'https://www.reddit.com/user/{mod}.json'\n",
    "response = requests.get(url, \n",
    "                        headers=headers, \n",
    "                        params={\"after\": after})\n",
    "json_response_after = response.json()\n",
    "json_response_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point of writing this tutorial (when you're doing this tutorial it's likely different!), the last item in `json_respose` is the following post (`Detroit's Brewing Heritage' on tap at Historical Museum`): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/tutorials/apis101/images/json_response.png\" width=60% align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and second items in `json_response_after` are the two comments below that (\"Shame on ... us back.\" and \"Are you ... comment /u/ehchip\"). In other words, where one object ends, another begins. We apply this concept to loop over the first ten pages. At each iteration, we store the `after` attribute, which we use as a parameter in the follow-up request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after = None\n",
    "posts = []\n",
    "\n",
    "for counter in range(10): \n",
    "    url = f'https://www.reddit.com/user/{mod}.json'\n",
    "    print('processing ' + url + ' with \"after\" parameter: ' + str(after))\n",
    "    response = requests.get(url, \n",
    "                            headers=headers, \n",
    "                            params={\"after\": after})\n",
    "    json_response = response.json()\n",
    "    after = json_response['data']['after'] \n",
    "\n",
    "    # store all posts in the posts list\n",
    "    posts.extend(json_response['data']['children'])\n",
    "\n",
    "# let's view the number of posts\n",
    "len(posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "1. Why do we define `after = None` at the top of the file? Can we leave it out? \n",
    "2. Convert the code snippet above into a function `reddit_activity()` that takes a `username` and `num_pages` as inputs and returns all posts by the user as a list of dictionaries. For example, `reddit_activity(\"sixwaystop313\", 5)` should return a list of the subreddits in which the user has posted or commented across the 125 (25 x 5) most recent items. \n",
    "3. Write a function that extracts a user's written comments from your result obtained in 2 to a new JSON dictionary, stored in a file called `comments.json`. Tip: Use some code that you've written to answer exercise 5 (part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions \n",
    "1. In our first request we don't know the value of `after` yet. It is important, however, to include this line because otherwise the `after` value in `params={}` is undefined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "def reddit_activity(username, num_pages):\n",
    "    after = None\n",
    "    posts = []\n",
    "\n",
    "    for counter in range(num_pages): \n",
    "        url = f'https://www.reddit.com/user/{username}.json'\n",
    "        print('processing ' + url + ' with after parameter: ' + str(after))\n",
    "        response = requests.get(url, headers=headers, params={\"after\": after})\n",
    "        json_response = response.json()\n",
    "        after = json_response['data']['after']\n",
    "\n",
    "        # collect all posts/comments\n",
    "        posts.extend(json_response['data']['children'])\n",
    "    return posts\n",
    "\n",
    "reddit_data = reddit_activity(\"sixwaystop313\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3:\n",
    "import json\n",
    "\n",
    "# We can first build a loop through the result of the previous question.\n",
    "result = []\n",
    "\n",
    "for item in reddit_data:\n",
    "    if item.get('data').get('body') is not None:\n",
    "        result.append({'comment_text': item.get('data').get('body')})\n",
    "\n",
    "with open('comments.json', 'w') as writer: writer.write(json.dumps(result)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.5 Time Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "When retrieving data from an API - in particular *user-level data*, not only the content of a post or comment matters, but also when the comment or post was written. In other words, we seek to extract the date and time (timestamps) from a users' comments and posts on Reddit.\n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "Run the cell below, to extract the timestamp from `sixwaystop313`'s most recent activity on Reddit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.reddit.com/user/sixwaystop313.json'\n",
    "response = requests.get(url, headers=headers)\n",
    "response.json()['data']['children'][0]['data']['created_utc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm... is *that* really a timestamp?\n",
    "\n",
    "Well... computers handle time differently than humans. And what programmers somewhat converged to is that timestamps are best measured in *the number of seconds passed since 1 January, 1970*. With the use of the `time` library, we can easily convert it into a readable date and time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "time_example = response.json()['data']['children'][0]['data']['created_utc']\n",
    "time_converted = time.gmtime(time_example)\n",
    "print(time_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `time_converted` you can extract the day, month, and year separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"The day is: {time_converted.tm_mday}\")\n",
    "print(f\"The month is: {time_converted.tm_mon}\")\n",
    "print(f\"The year is: {time_converted.tm_year}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or together, like this (characters that start with `%` have a special meaning, the `-` in  between these characters are literally the dashes you see in the output): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time.strftime(\"%d-%m-%Y\", time_converted))  \n",
    "# %d = day\n",
    "# %m = month\n",
    "# %Y = year (4 digits) and %y = year (2 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### Exercise 7 \n",
    "1. In a similar way, you can convert the UTC time into an hour (`%H`) and minute (`%M`). Transform `time_example` into a readable time. The output should be `06:17`. \n",
    "2. Suppose we want to capture the Reddit use of `sixwaystop313` throughout the day. More specifically, we want to know during what hours the user is most actively commenting on the platform.\n",
    "  * Use the solution to exercise 6 (part 3) you wrote earlier to pull out the UTC timestamps.\n",
    "  * Extract the hour from these timestamps. \n",
    "  * Save the data as a CSV file (with columns: comments, timestamp and hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 \n",
    "print(time.strftime(\"%H:%M\", time_converted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "reddit_data = reddit_activity(\"sixwaystop313\", 5)\n",
    "\n",
    "# We can first build a loop through the result of the previous question.\n",
    "result = []\n",
    "\n",
    "for item in reddit_data:\n",
    "    if item.get('data').get('body') is not None:\n",
    "        timestamp = item.get('data').get('created_utc')\n",
    "        time_converted = time.gmtime(timestamp)\n",
    "    \n",
    "        result.append({'comment_text': item.get('data').get('body'),\n",
    "                       'timestamp': timestamp,\n",
    "                       'hour': time_converted.tm_hour})\n",
    "\n",
    "# convert dictionary to pandas data frame\n",
    "df = pd.DataFrame.from_dict(result)\n",
    "\n",
    "# save pandas data frame to CSV\n",
    "df.to_csv('comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.6 Building an API data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "Up to now, we've written functions that in itself carry out separate tasks: one for obtaining data from the subreddit's about page, and one to obtain particular attributes of users of the subreddit.\n",
    "\n",
    "However, when we use APIs for research, we are not so much interested in the results of \"single-shot\" API requests, but we would like to obtain a *copy* of the entire data, so that we can analyze it later.\n",
    "\n",
    "So, the purpose of this section is to \"stitch\" together individual API requests. For now, we assume that we are interested in studying how the posting behavior of users currently active on the channel influences the total number of active users of the community.\n",
    "\n",
    "In other words, we need to \n",
    "- obtain a list of all users who have currently posted on the subreddit (the first 25), and\n",
    "- store all of their posts and comments in a JSON (raw data) file.\n",
    "- extract some attributes to a CSV file (which can be used for analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Let's first make use of a function `get_users()` that returns the currently active users from the `marketing` subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users(subreddit):\n",
    "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
    "    response = requests.get(url, \n",
    "                            headers=headers)\n",
    "    json_response = response.json()\n",
    "    users = []\n",
    "    # loop over all items in a request\n",
    "    for item in json_response['data']['children']:\n",
    "        users.append(item['data']['author'])\n",
    "    return users\n",
    "\n",
    "users = get_users('marketing')\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have written our `reddit_activity` function, which we could call now using the (for prototypical purposes) first moderator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_activity(users[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above uses the Reddit API to download all kind of posts and comments by the particular user. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 8__\n",
    "\n",
    "1. Write a loop that calls `reddit_activity()` on all 25 most recent users of the subreddit `marketing`, and stores the result in a (long) list of dictionaries. Limit yourself to the first 3 pages per user (to save time doing this exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__\n",
    "\n",
    "*Question 1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_content = []\n",
    "for user in users:\n",
    "    content = reddit_activity(user, 3)\n",
    "    all_content.extend(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's preview the first few results\n",
    "all_content[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.7 Exporting data to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "Alright, you've almost made it. We've accomplished a whole lot by now: we just wrote a tool that extracts user-level data, and appends all that data in a list of dictionaries stored in Python. But, how can we port the data to another software program (e.g., R, Excel)?\n",
    "\n",
    "We need to convert the data to a Comma Separated Values (CSV) file, and extract only some particular attributes/variables that we're interested in.\n",
    "\n",
    "More specifically, we'd like to have a file with five columns, containing:\n",
    "- the username (`author`),\n",
    "- the type of content (post vs. comment) (`kind`),\n",
    "- the timestamp (`created_utc`),\n",
    "- the subreddit name (`subreddit`), and\n",
    "- the body (`body`), __if present__.\n",
    "\n",
    "To faciliate writing to a CSV file, we'll make use of the `pandas` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Here, we'll start with a code snippet to extract the `author`, content type (`kind`) and timestamp (`created_utc`) to a new list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_content = []\n",
    "\n",
    "for post in all_content:\n",
    "    username = post.get('data').get('author')\n",
    "    post_type = post.get('kind') # explore the JSON object to see why we don't have to do .get('data') here!\n",
    "    timestamp = post.get('data').get('created_utc')\n",
    "    \n",
    "    filtered_content.append({'username': username,\n",
    "                            'post_type': post_type,\n",
    "                            'timestamp': timestamp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the snippet and the next cell to see the result (it should create a new file `reddit_posts.csv` in your current working directory)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(filtered_content).to_csv('reddit_posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preview the content of that file directly in Jupyter, using the `pandas` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_posts.csv\")\n",
    "# shows top 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job, so far. But the file is not ready yet, so let's work on the exercises by extending the code snippet above.\n",
    "\n",
    "---\n",
    "\n",
    "__Exercise 9__  \n",
    "The `reddit_posts.csv` file now only includes 3 columns (`userame`, `post_type`, `timestamp`). Please add the following columns as well: the subreddit name, the text of users' comments, and the timestamp converted to YYYY-MM-DD (date, e.g., 2021-01-15) and HH:MM (e.g., 08:00). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solutions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "filtered_content = []\n",
    "\n",
    "for post in all_content:\n",
    "    username = post.get('data').get('author')\n",
    "    post_type = post.get('kind') # explore the JSON object to see why we don't have to do .get('data') here!\n",
    "    subreddit = post.get('data').get('subreddit')\n",
    "    timestamp = post.get('data').get('created_utc')\n",
    "\n",
    "    time_converted = time.gmtime(timestamp)\n",
    "    datestamp = time.strftime(\"%Y-%m-%d\", time_converted)\n",
    "    timestamp = time.strftime(\"%H:%M\", time_converted)\n",
    "    try:\n",
    "        bodytext = post.get('data').get('body')\n",
    "    except:\n",
    "        bodytext = ''\n",
    "        \n",
    "    filtered_content.append({'username': username,\n",
    "                            'post_type': post_type,\n",
    "                            'subreddit': subreddit,\n",
    "                            'timestamp': timestamp,\n",
    "                            'date':  datestamp,\n",
    "                             'time': timestamp,\n",
    "                             'body': bodytext\n",
    "                            })\n",
    "    \n",
    "# store as CSV file\n",
    "pd.DataFrame.from_dict(filtered_content).to_csv('reddit_posts.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preview\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_posts.csv\")\n",
    "# shows top 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job - you've made it!\n",
    "\n",
    "After working on this set of exercises, you should be able to further explore the Reddit API on your own. \n",
    "\n",
    "In particular, we use our new skills to extract data, store them in JSON or CSV files, and then analyze them (but, recall, analyzing web scraped/API data is not part of this course).\n",
    "\n",
    "So - does `sixwaystop313` spend the most time in subreddits in which he gets the most upvotes? Did his posting behavior change over time? Are users that have posted recently more likely to be a premium Reddit user? Think what data is required to obtain such data, and then try to extract such data.\n",
    "\n",
    "At the same time, realize that we have only scratched the surface of what's possible with APIs. Headers and pagination played a vital role in requests and were sufficient thus far. Yet, the majority of [API endpoints](https://www.reddit.com/dev/api/) require authentication (OAuth), which is a whole topic on its own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
