oDCM - Web Scraping 101 (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(reticulate)
py_install("requests")
py_install("bs4")
```

Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("web scraping 101")__.

- If you haven't done so, open slides at https://odcm.hannesdatta.com/docs/modules/week3/
- Today's (live) code is available at [tiu.nu/livecoding]() (refresh may be needed)
- You can use Google Colab at the beginning of this tutorial, BUT... you will have to use __Jupyter Notebook on your laptops__ towards the end


Before we start
========================================================

- Recap coaching session for team activity #1
  - explore broadly! ("universe discovery!") --> check challenge #1.1
  - compare sources thoroughly --> e.g., comparison table + challenge #1.2
  - Consider example projects (e.g., overview tables, illustrations)
- Previous today's coaching session: __decide__ which website or API to use (challenges #1.1-#1.3)
- Any questions so far?

A note on my (live) tutorials
===============

- Core material is the (Jupyter Notebook) tutorial of the week
- It's *written* and illustrated with screenshots -- it gives you all of the necessary information and you can do it at your own time after class.
- IN CLASS, my ambition is to *preview selected* issues 
  - do you feel comfortable w/ code: then write code with me and try it out!
  - if you struggle to code: follow the lecture and *see me code*. Understand concepts, take notes. Then, spend more time on tutorial after class
  - if you get lost throughout: stop coding, understand concepts.
  
- Recordings of these sessions posted on Canvas (non-public)

__I hope this helps.__


Framework
=======

![](https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/jmxa/2022/jmxa_86_5/00222429221100750/20220801/images/large/10.1177_00222429221100750-fig2.jpeg)

Today: zooming in more on __collection design__

Recap from last week's tutorial
==========

- We focused on which __information to extract__ (Challenge #2.1)
  - e.g., explore a website, navigate broadly to find information that's interesting, etc.
- We focused on __how to extract__ that information
  - BeautifulSoup: `.find()` and `.find_all()` functions
  - Use (a combination of) tags, classes, attributes, attribute-value pairs

DO: Recap from last week's tutorial
==========
class: small-code

__Extend the code snippet below to extract the release year of the song below.__

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup
url = 'https://music-to-scrape.org/song?song-id=SOJZTXJ12AB01845FB'
request = requests.get(url, headers =  {'User-agent': 'Mozilla/5.0'})
song_page = BeautifulSoup(request.text)
about = song_page.find(class_='about_artist')
title = about.find('p')
```

DO: Solution
==========
class: small-code

- Use the browser's __inspect__ mode to find the relevant attributes
- Consecutively develop the code step-by-step, "onion"-like

```{python, eval=TRUE}
about.find_all('p')
year=about.find_all('p')[3].get_text()
```

__Recommendations:__
- Practice even more, e.g., with __new information__ that you haven't extracted before from this and other sections of the website.
- Always build code __gradually__, __never__ from scratch

DO: After-class exercise from last week
==========
class: small-code

Any issues you've run into?

__Bonus point__ for a student that
 - adds solution to Jupyter Notebook from last week using GitHub
 - records a video tutorial with the solution

Let me know if you're interested.


Today's focus
================

__Remaining challenges for the "design" phase of collecting data.__

- How to sample from a website? (Challenge #2.2)
  - Recall that we do not have access to a firm's database (so we can't sample from a population of, say, users)
  - With web scraping, we always need a "starting point" for a data collection
- Examples for seeds/sample
  - list of recently active users (music-to-scrape.org)
  - home page of Twitch (for currently active live streams)
<br>
 
DO: Starting up a data collection for your own project
============

__Using your own project ideas__, tell us how you could __sample__ from the site?

<br>
<br>

Tips:
- Make use of the best practices for Challenge #2.2 in [Table 3](https://journals.sagepub.com/doi/full/10.1177/00222429221100750?journalCode=jmxa#table3-00222429221100750)


Sampling users
===========
class: small-code

For today's tutorial, we will be sampling users from the main website of music-to-scrape.org

__Can you propose a strategy to capture them?__ Any attributes/classes you see? 


DO: Extract all links
=======
class: small-code

__Please use the code snippet below to extract the 'href' attribute of all links on the website.__

- Links are identifiable by the "a" tag. 
- The relevant attribute is called "href"
- Build your code consecutively!

```{python, eval = TRUE}
url = 'https://music-to-scrape.org'

res = requests.get(url)
res.encoding = res.apparent_encoding

homepage = BeautifulSoup(res.text)

# continue here
```

Solution: Extract all links
=====

```{python, eval=TRUE}
for link in homepage.find_all("a"):
    if 'href' in link.attrs: 
        print(link.attrs["href"])
```

Solution: Extract all links
=====

- But... are all of these links really relevant?
- Recall: what "seeds" do we need to gather?

Narrowing down our extraction
============

- Let us explore the site structure a bit more
- Particularly, can we identify other ways to navigate on the site?
- Remember, your strategy can be a "multi-step" strategy (first to A, then within A to B)!
- __Let's open the inspect mode of our browser and come up with an updated strategy.__

DO: Narrowing down our extraction
============

- The relevant links all reside in the `recent_users` section, starting with `<section`>

```{python, eval=TRUE}
relevant_section = homepage.find('section',attrs={'name':'recent_users'})
```

__DO:__ Can you come up with a way to loop through all of the links WITHIN `relevant_section` and store them?

Solution: Narrowing down our extraction
============
class: small-code

```{python, eval=TRUE}
users = []
for link in relevant_section.find_all("a"):
  if ('href' in link.attrs):
      users.append(link.attrs['href'])
users
```

- Let's now take a look at these links more closely. 

Modifying the links
========
class: small-code

- Notice that the links were *relative* links and we won't be able to use them for looping (e.g., try pasting them in your browser - they won't work!)
- So, let's turn them into absolute links, simply by concatenating (= combining) strings.

```{python, eval=TRUE}
urls = []
for link in relevant_section.find_all("a"):
    if 'href' in link.attrs: 
        extracted_link = link.attrs['href']
        urls.append(f'https://music-to-scrape.org/{extracted_link}')
urls
```

DO: Remember functions?
======

Write a function that...

- makes a web request to https://music-to-scrape.org
- extract all links to the user pages and returns them back (`return()`)

__Do you remember why we like functions so much?__

Solution
======
class: small-code

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup

def get_users():
    url = 'https://music-to-scrape.org/'
  
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    
    soup = BeautifulSoup(res.text)
    
    relevant_section = soup.find('section',attrs={'name':'recent_users'})

    links = []
    for link in relevant_section.find_all("a"):
        if 'href' in link.attrs: 
            extracted_link = link.attrs['href']
            links.append(f'https://music-to-scrape.org/{extracted_link}')
    return(links) # to return all links

# let's try it out
# users = get_users() 
```

- Functions allow us to __re-use code__; it prevents errors, and helps us structure our code.

JSON data
=====

- For now, the data is just in memory.
- But, we can also save it!
- For this, we *store* the data as JSON objects/dictionaries
- Use `json.dumps` to convert JSON to text (and then save it)
- Use `json.loads` to convert text to JSON (and then use it)

Example: JSON data
=====
class: small-code

```{python}
import json

users = get_users()
# build JSON dictionary
f = open('users.json','w')
for user in users:
  obj = {'url': user,
         'username': user.replace('https://music-to-scrape.org/user?username=','')}
  f.write(json.dumps(obj))
  f.write('\n')
f.close()
```

Preventing array misalignment
==============

- When extracting web data, it's important to extract information in its "original" (nested) structure
- For example, hover around the user names and observe the song names that pop up.
- If you were to extract this information __separately__, you wouldn't be able to know __which one relates to which user__.

Demonstration
=======
class: small-code

```{python}
links = []
for link in relevant_section.find_all("a"):
    if 'href' in link.attrs: 
        extracted_link = link.attrs['href']
        links.append(f'https://music-to-scrape.org/{extracted_link}')

# getting songs
songs = []
for song in relevant_section.find_all("span"):
    songs.append(song.get_text())

len(links)
len(songs)
```

DO: Preventing array misalignment
====
class: small-code

Which solutions do you see -- by iterating differently through the source code -- to prevent array misalignment?

Tip: Observe how we can first iterate through users, THEN extract links (similar to how we have done it with `relevant_section`).

```{python}
users = relevant_section.find_all(class_='mobile-user-margin')
data = []
for user in users: # iterate through each user
  obj = {'link':user.find('a').attrs['href']  }
  data.append(obj)
data

```

Solution
====
class: small-code

```{python}
users = relevant_section.find_all(class_='mobile-user-margin')
data = []
for user in users:
  if user.find('span') is not None:
    song_name=user.find('span').get_text()
  else:
    song_name='NA'
  obj = {'link':user.find('a').attrs['href'],
         'song_name': song_name}
  
  data.append(obj)
data

```



Let's take a step back...
=======

- What we've learnt so far...
  - Extract individual information from a website, e.g., the homepage of music-to-scrape (ch. #2.1)
  - Collect all links to user profiles from the homepage (ch. #2.2)
  
- What's missing
  - VISIT each fo the profile pages
  - Loop through them
  
- We can then tie things together in a scraper
  - "collect" all user seeds (scraper 1)
  - then collect all consumption data from individual user pages (scraper 2)
  
Navigating on a website
=========
class: small-code

- Two strategies
  - Understand how links are built (prebuilding them; strategy 1)
  - Understand how to "click" to the next/previous page (consecutively building them; strategy 2)

```{python, eval=TRUE}
urls = []
counter = 37
while counter >= 0:
  urls.append(f'https://music-to-scrape.org/user?username=StarCoder49&week={counter}')
  counter = counter - 1
urls
```

DO: Navigating on a Website (strategy 2)
=========
class: small-code

__Run the code below. How to extend the code to extract the LINK of the previous button?__

```{python, eval=TRUE}
url = 'https://music-to-scrape.org/user?username=StarCoder49&week=36'
header = {'User-agent': 'Mozilla/5.0'}
res = requests.get(url, headers = header)
res.encoding = res.apparent_encoding
userpage = BeautifulSoup(res.text)
button=userpage.find(class_='page-link', attrs={'type':'previous_page'})

```

Solution
======

```{python}
button.attrs['href']
```

__DISCUSS:__ When do we use strategy 1 (prebuilt) vs. strategy 2 (do it on the fly)?

Let's tie things together
=========
class: small-code

- We now have a function `get_users()` to retrieve user names
- We also know how to "find" the next link to visit (a user's previous page)
- How can we now visit ALL pages, for ALL users?
- Let's take a look at some pseudo code!

```
users = get_users()

consumption_data = []

for user in users:
  url = user['url']
  
  while url is not None:
    # scrape information from URL # challenge #2.1, #2.4
    # determine "previous page"
    time.sleep(1) # challenge #2.3
  
    # if previous page exists: rerun loop on next URL
    # if previous page does not exist: stop while loop, go to next 
```

Challenge #2.3: At which frequency to extract the data?
=======

- We can decide how often to capture information from a site
  - e.g., once, every 5 minutes, every day
- Potential gains in extracting data multiple times
- Extraction limits & __technically feasible sample size!__
- More issues explained in table 3, challenge #2.3

 
Challenge #2.4: How to process data during collection
==========

- value of retaining raw data

```
# pseudo code!
product_data = []
for s in seeds:
  # store raw html code here
  store_to_file(s, 'website.html')
  # continue w/ parsing some information
  product = get_product_info(s)
  product_data.append(product)

```

Challenge #2.4: How to process data during collection
==========

- parsing data on the fly (vs. after the collection)

```
# pseudo code!

product_data = []
for s in seeds:
  product = get_product_info(s) 
  product_updated = processing(product)
  time.sleep(1) 
  product_data.append(product_updated) 
  # write data to file here (parsing on the fly!!!)
  
# write data to file here (after the collection - avoid!)

```

Extract consumption data
====

We still need to extract the consumption data from a user's profile page.

Let's start with the snippet below.

```{python}
url = 'https://music-to-scrape.org/user?username=StarCoder49&week=10'
soup = BeautifulSoup(requests.get(url).text)
```

Can you propose:
- how to find the table?
- how to iterate through each row?
- how to save the information as a JSON dictionary



Scraping more advanced websites
=========
incremental: true
class: small-code
 
- So far, we have done this
  - `requests` (to get) --> `beautifulsoup` (to extract information, "parse")
  
- __DO:__ Run the snippet below and open `amazon.html`  
```{python, eval=FALSE}
import requests
header = 
f = open('amazon.html', 'w', encoding = 'utf-8')
f.write(requests.get('https://amazon.com', headers =  {'User-agent': 'Mozilla/5.0'}).text)
f.close()
```

__Can you explain what happened?__

Alternative ways to make connections
========
class: small-code

- Many dynamic sites require what I call "simulated browsing"

- Try this:

```{python, eval=FALSE}
!pip install webdriver_manager
!pip install selenium

# Using selenium 4 - ensure you have Chrome installed!
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

url = "https://amazon.com/" # remember to solve captchas
driver.get(url)
```

- __See your browser opening up?__
- Beware of rerunning code - a new instance will run each time!

Continuing with beautifulsoup
=========

- We can convert the site's source code (`page_source`) to `BeautifulSoup`, and proceed as always

```{python, eval=FALSE}
from bs4 import BeautifulSoup
soup = BeautifulSoup(driver.page_source)

for el in soup.find_all(class_ = 'a-section'):
    title = el.find('h2')
    if title is not None: print(title.text)
```

Different ways to open a site
=========

- Static/easy site: use `requests`
  - `requests` --> `BeautifulSoup` --> `.find()`
- Dynamic/difficult site: also use `selenium`
  - `selenium` --> wait --> `BeautifulSoup` --> `.find()`
  
Next steps
===========

- Focus on self-study material
- Today's coaching session...!
  - settle on extraction design (challenges #1.1-1.3) 
   - get started w/ __prototyping data extraction__
   - consider `selenium` and APIs (see next tutorials)

Thanks!
=======

- Any questions?
- Be in touch for feedback via WhatsApp.


Coaching session
=====

- Decide which website or API to use (challenges #1.1-#1.3)
- Instructions, see: https://odcm.hannesdatta.com/docs/project/workplan/activity2/

<br>
- Please actively solicit my feedback!
