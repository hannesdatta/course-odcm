oDCM - Web Scraping 101 (Tutorial)
========================================================
author: Hannes Datta
date:
autosize: true

<style>
.small-code pre code {
  font-size: 1em;
}
</style>

<!--#

https://support.rstudio.com/hc/en-us/articles/200486468
-->

```{r setup, echo = FALSE, message=FALSE, warning=FALSE, include=FALSE}
suppressPackageStartupMessages(library(reticulate))
py_install("requests")
py_install("bs4")
```


Welcome to oDCM!
========================================================

We're about to start with __today's tutorial ("web scraping 101")__.

- If you haven't done so, open slides at https://odcm.hannesdatta.com/docs/modules/week3/
- Today's (live) code is available at [tiu.nu/livecoding]() (refresh may be needed)
- You can use Google Colab at the beginning of this tutorial, BUT... you will have to use __Jupyter Notebook on your laptops__ towards the end

Before we start (I): live coding
===============
  
- feel comfortable w/ code?
  - then write code with me and try it out!
- struggle? 
    - just *see me code*.
    - focus on understanding concepts & take notes
    - spend more time on tutorial after class
    - keep asking questions!
- "bored"/too advanced?
  - flip through slides and work on advanced exercises
  - ask advanced questions when I walk around (1:1)


Before we start (II)
========================================================

- Recap from last week (`.find()`, `.find_all()`)
- Part 1: `beautifulSoup` (static websites; works in Colab)
- Part 2: `selenium` (dynamic websites; only works on your laptops)

<br>
- Today's coaching session: __decide__ which website or API to use (challenges #1.1-#1.3) & start coding your collections

Recap 
===

- We focused on which __information to extract__ (Challenge #2.1)
  - e.g., explore a website, navigate broadly to find information that's interesting, etc.
- We focused on __how to extract__ that information
  - BeautifulSoup: `.find()` and `.find_all()` functions
  - Use (a combination of) tags or attributes (e.g., classes; particularly attribute-value pairs)


Recap (Framework)
==========

![](https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/jmxa/2022/jmxa_86_5/00222429221100750/20220801/images/large/10.1177_00222429221100750-fig2.jpeg)


DO: Recap from last week's tutorial
==========
class: small-code

__Extend the code snippet below to extract the release year of the song below.__

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup
url = 'https://music-to-scrape.org/song?song-id=SOJZTXJ12AB01845FB'
request = requests.get(url, headers =  {'User-agent': 'Mozilla/5.0'})
song_page = BeautifulSoup(request.text)
about = song_page.find(class_='about_artist')
title = about.find('p')
```

DO: Solution
==========
class: small-code

- Use the browser's __inspect__ mode to find the relevant attributes
- Consecutively develop the code step-by-step, "onion"-like

```{python, eval=TRUE}
about.find_all('p')
plays=about.find_all('p')[3].get_text()
```

__Recommendations:__
- Practice even more, e.g., with __new information__ that you haven't extracted before from this and other sections of the website.
- Always build code __gradually__, __never__ from scratch


Part 1: `beautifulSoup` (static websites; works in Colab)
=======

__We use `beautifulSoup` for addressing the remaining challenges for the "design" phase of collecting data.__

- How to sample from a website? (Challenge #2.2)
  - Recall that we do not have access to a firm's database (so we can't sample from a population of, say, users)
  - With web scraping, we always need a "starting point" for a data collection
- Examples for seeds/sample
  - list of recently active users (music-to-scrape.org)
  - homepage of Twitch (for current live streams)
<br>

__Using your own project ideas__, tell us how you could __sample__ from the site? (--> see [table 3 in "Fields of Gold"](https://journals.sagepub.com/doi/full/10.1177/00222429221100750?journalCode=jmxa#table3-00222429221100750))


Sampling users (I)
===========
class: small-code

Let's sample users from the main website of music-to-scrape.org

__Can you propose a strategy to capture them?__ Any attributes/classes you see? 


Sampling users (II)
=======
class: small-code

__DO__: Please use the code snippet below to extract the 'href' attribute of all links on the website.

- Links are identifiable by the "a" tag. 
- The relevant attribute is called "href"
- Build your code consecutively!

```{python, eval = TRUE}
url = 'https://music-to-scrape.org'

res = requests.get(url)
res.encoding = res.apparent_encoding

homepage = BeautifulSoup(res.text)

# continue here
```

Sampling users (III): Solution
=====

```{python, eval=TRUE}
for link in homepage.find_all("a"):
    if 'href' in link.attrs: 
        print(link.attrs["href"])
```

Narrowing down (I)
=====

- But... are all of these links really relevant?
- Recall: what "seeds" do we need to gather?

Narrowing down (II)
============

- Let us explore the site structure a bit more
- Particularly, can we identify other ways to navigate on the site?
- Remember, your strategy can be a "multi-step" strategy (first to A, then within A to B)!
- __Let's open the inspect mode of our browser and come up with an updated strategy.__

Narrowing down (III)
============

- The relevant links all reside in the `recent_users` section, starting with `<section`>

```{python, eval=TRUE}
relevant_section = homepage.find('section',attrs={'name':'recent_users'})
```

__DO:__ Can you come up with a way to loop through all of the links WITHIN `relevant_section` and store them?

Narrowing down (IV): Solution
============
class: small-code

```{python, eval=TRUE}
users = []
for link in relevant_section.find_all("a"):
  if ('href' in link.attrs):
      users.append(link.attrs['href'])
users
```

- Let's now take a look at these links more closely. 

Narrowing down (V): More extensions
========
class: small-code

- Notice that the links were *relative* links and we won't be able to use them for looping (e.g., try pasting them in your browser - they won't work!)
- So, let's turn them into *absolute* links, simply by concatenating (= combining) strings.

```{python, eval=TRUE}
urls = []
for link in relevant_section.find_all("a"):
    if 'href' in link.attrs: 
        extracted_link = link.attrs['href']
        urls.append(f'https://music-to-scrape.org/{extracted_link}')
urls
```

Narrowing down (VI): More extensions: Functions
====

__DO:__ Write a function that...

- makes a web request to https://music-to-scrape.org
- extract all links to the user pages and returns them back (`return()`)

__Do you remember why we like functions so much?__

Narrowing down (VII): Solution

======
class: small-code

```{python, eval=TRUE}
import requests
from bs4 import BeautifulSoup

def get_users():
    url = 'https://music-to-scrape.org/'
  
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    
    soup = BeautifulSoup(res.text)
    
    relevant_section = soup.find('section',attrs={'name':'recent_users'})

    links = []
    for link in relevant_section.find_all("a"):
        if 'href' in link.attrs: 
            extracted_link = link.attrs['href']
            links.append(f'https://music-to-scrape.org/{extracted_link}')
    return(links) # to return all links

# let's try it out
# users = get_users() 
```

- Functions allow us to __re-use code__; it prevents errors, and helps us structure our code.

JSON data (I)
=====

- For now, the data is just in memory.
- But, we can also save it!
- For this, we *store* the data as JSON objects/dictionaries
- Use `json.dumps` to convert JSON to text (and then save it)
- Use `json.loads` to convert text to JSON (and then use it)

JSON data (II): Example
=====
class: small-code

```{python}
import json

users = get_users()
# build JSON dictionary
f = open('users.json','w')
for user in users:
  obj = {'url': user,
         'username': user.replace('https://music-to-scrape.org/user?username=','')}
  f.write(json.dumps(obj))
  f.write('\n')
f.close()
```

Preventing array misalignment (I)
==============

- When extracting web data, it's important to extract information in its "original" (nested) structure
- For example, hover around the user names and observe the song names that pop up.
- If you were to extract this information __separately__, you wouldn't be able to know __which one relates to which user__.

Preventing array misalignment (II): Demo
=======
class: small-code

```{python}
links = []
for link in relevant_section.find_all("a"):
    if 'href' in link.attrs: 
        extracted_link = link.attrs['href']
        links.append(f'https://music-to-scrape.org/{extracted_link}')

# getting songs
songs = []
for song in relevant_section.find_all("span"):
    songs.append(song.get_text())

len(links)
len(songs)
```

Preventing array misalignment (III): DO
====
class: small-code

__DO:__ How can you capture the song data, such that we can always link the particular song to particular users? (i.e., to prevent array misalignment)?

Tip: Observe how we can first iterate through users, THEN extract links (similar to how we have done it with `relevant_section`).

```{python}
users = relevant_section.find_all(class_='mobile-user-margin')
data = []
for user in users: # iterate through each user
  obj = {'link':user.find('a').attrs['href']  }
  data.append(obj)
data

```

Preventing array misalignment (IV): Solution
====
class: small-code

```{python}
users = relevant_section.find_all(class_='mobile-user-margin')
data = []
for user in users:
  if user.find('span') is not None:
    song_name=user.find('span').get_text()
  else:
    song_name='NA'
  obj = {'link':user.find('a').attrs['href'],
         'song_name': song_name}
  
  data.append(obj)
data

```


Let's take a step back...
=======

- What we've learnt so far...
  - Extract individual information from a website, e.g., the homepage of music-to-scrape (ch. #2.1)
  - Collect all links to user profiles from the homepage (ch. #2.2)
  
- What's missing
  - VISIT each of the profile pages
  - Loop through them
  
- We can then tie things together in a scraper
  - "collect" all user seeds (scraper 1)
  - then collect all consumption data from individual user pages (scraper 2)
  
Navigating on a website (I)
=========
class: small-code

- Two strategies
  - Understand how links are built (pre-building them; strategy 1)
  - Understand how to "click" to the next/previous page (consecutively building them; strategy 2)

```{python, eval=TRUE}
urls = []
counter = 37
while counter >= 0:
  urls.append(f'https://music-to-scrape.org/user?username=StarCoder49&week={counter}')
  counter = counter - 1
urls
```

Navigating on a website (II): DO (strategy 2)
=========
class: small-code

__DO__: Run the code below. How to extend the code to extract the LINK of the previous button?

```{python, eval=TRUE}
url = 'https://music-to-scrape.org/user?username=StarCoder49&week=36'
header = {'User-agent': 'Mozilla/5.0'}
res = requests.get(url, headers = header)
res.encoding = res.apparent_encoding
userpage = BeautifulSoup(res.text)
button=userpage.find(class_='page-link', attrs={'type':'previous_page'})

```

Navigating on a website (III): Solution
======

```{python}
button.attrs['href']
```

__DISCUSS:__ When do we use strategy 1 (pre-built) vs. strategy 2 (do it on the fly)?

Tying things together
=========
class: small-code

- We now have a function `get_users()` to retrieve user names
- We also know how to "find" the next link to visit (a user's previous page)
- How can we now visit ALL pages, for ALL users?
- Let's take a look at some pseudo code!

```
users = get_users()

consumption_data = []

for user in users:
  url = user['url']
  
  while url is not None:
    # scrape information from URL # challenge #2.1, #2.4
    # determine "previous page"
    time.sleep(1) # challenge #2.3
  
    # if previous page exists: rerun loop on next URL
    # if previous page does not exist: stop while loop, go to next 
```

Challenge #2.3: At which frequency to extract the data?
=======

- We can decide how often to capture information from a site
  - e.g., once, every 5 minutes, every day
- Potential gains in extracting data multiple times
- Extraction limits & __technically feasible sample size!__
- More issues explained in table 3, challenge #2.3

 
Challenge #2.4: How to process data during collection
==========

- value of retaining raw data

```
# pseudo code!
product_data = []
for s in seeds:
  # store raw html code here
  store_to_file(s, 'website.html')
  # continue w/ parsing some information
  product = get_product_info(s)
  product_data.append(product)

```

Challenge #2.4: How to process data during collection
==========

- parsing data on the fly (vs. after the collection)

```
# pseudo code!

product_data = []
for s in seeds:
  product = get_product_info(s) 
  product_updated = processing(product)
  time.sleep(1) 
  product_data.append(product_updated) 
  # write data to file here (parsing on the fly!!!)
  
# write data to file here (after the collection - avoid!)

```

Part 2: Scraping more advanced websites
=========
incremental: true
class: small-code
 
- So far, we have done this
  - `requests` (to get) --> `beautifulsoup` (to extract information, "parse")
  
- __DO:__ Run the snippet below and open `amazon.html`  
```{python, eval=FALSE}
import requests
header = 
f = open('amazon.html', 'w', encoding = 'utf-8')
f.write(requests.get('https://amazon.com', headers =  {'User-agent': 'Mozilla/5.0'}).text)
f.close()
```

__Can you explain what happened?__

Alternative ways to make connections
========
class: small-code

- Many dynamic sites require what I call "simulated browsing"

- Try this:

```{python, eval=FALSE}
!pip install webdriver_manager --upgrade
!pip install selenium --upgrade

# Using selenium 4 - ensure you have Chrome installed!
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager

driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))

url = "https://music-to-scrape.org/"
driver.get(url)
```

- __See your browser opening up?__
- Beware of rerunning code - a new instance will run each time!

Continuing with beautifulsoup
=========

- We can convert the site's source code (`page_source`) to `BeautifulSoup`, and proceed as always

```{python, eval=FALSE}
from bs4 import BeautifulSoup
soup = BeautifulSoup(driver.page_source)

cards = soup.find_all(class_='card-body')

counter = 0
for card in cards:
    counter = counter + 1
    print('Card ' + str(counter) + ': ' + card.get_text())

```

Clicking with selenium
======

```python
url = "https://music-to-scrape.org/"
driver.get(url)
time.sleep(3) # wait for 3 seconds

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains

try:
    cookie_button = driver.find_element(By.ID, "accept-cookies")
    cookie_button.click()
except:
    print('No cookie button found (anymore)!')
```

Scrolling with selenium
=====

```python
scroll_pause_time = 2
for _ in range(3):  # Scroll down 3 times
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(scroll_pause_time)
```

Different ways to open a site
=========

- Static/easy site: use `requests`
  - `requests` --> `BeautifulSoup` --> `.find()`
- Dynamic/difficult site: also use `selenium`
  - `selenium` --> wait --> `BeautifulSoup` --> `.find()`
  
Next steps
===========

- Focus on self-study material
- Today's coaching session...!
  - sign-off on extraction design (challenges #1.1-1.3) 
  - working on __prototyping the data extraction__
  - consider `selenium` and APIs (see next tutorials)

Thanks!
=======

- Any questions?
- Be in touch for feedback via WhatsApp.

