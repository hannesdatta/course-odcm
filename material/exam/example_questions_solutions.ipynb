{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb6b473",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1 - Python Bootcamp\n",
    "Write a function `url_detector()` that loads a list of URLs from the file `urls.txt` (new-line separated), and filters that list for valid URLs, starting with `https` and containing a link to a product ID. Although you could rely on [regular expressions](https://tilburgsciencehub.com/building-blocks/develop-your-coding-skills/learn-to-code/learn-regular-expressions/) to get the job done, other simpler workarounds exist. How many URLs do you end up with? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c219ed2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "def url_detector(url_list):\n",
    "    return [url for url in url_list if url[0:5] == \"https\" and \"/dp/\" in url]\n",
    "\n",
    "url_list = pd.read_csv(\"urls.txt\", header=None)[0].to_list()\n",
    "print(len(url_detector(url_list))) # 40 urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6ef330",
   "metadata": {},
   "source": [
    "## Question 2 - Web Scraping\n",
    "Scrape the top 1000 lifetime grossing movies (domestic) from [Box Office Mojo](https://www.boxofficemojo.com/chart/top_lifetime_gross/?area=XWW). Filter down on movies released since 2000 and export the rank, title, and lifetime gross of these movies to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82df4418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "\n",
    "def seed_generator(url_base):\n",
    "    return [url_base + str(offset) for offset in range(0, 1000, 200)]\n",
    "\n",
    "def scrape_data(urls):\n",
    "    dfs = pd.DataFrame()\n",
    "\n",
    "    for url in urls: \n",
    "        request_object = requests.get(url)\n",
    "        source_code = request_object.text\n",
    "        soup = BeautifulSoup(source_code, \"html.parser\")\n",
    "\n",
    "        ranks = soup.find_all(class_ = \"mojo-field-type-rank\")[1:]\n",
    "        ranks_cleaned = [int(rank.get_text().replace(\",\", \"\")) for rank in ranks]\n",
    "\n",
    "        titles = soup.find_all(class_ = \"mojo-field-type-title\")[1:]\n",
    "        titles_cleaned = [title.find(\"a\").get_text() for title in titles]\n",
    "\n",
    "        years = soup.find_all(class_ = \"mojo-field-type-year\")[1:]\n",
    "        years_cleaned = [int(year.get_text()) for year in years]\n",
    "\n",
    "        moneys = soup.find_all(class_ = \"mojo-field-type-money\")[1:]\n",
    "        money_cleaned = [int(money.get_text().replace(\",\", \"\").replace(\"$\", \"\")) for money in moneys]\n",
    "\n",
    "        df = pd.DataFrame({\"rank\": ranks_cleaned, \"title\": titles_cleaned, \"years\": years_cleaned, \"gross_dollars\": money_cleaned})\n",
    "        dfs = pd.concat([dfs, df])\n",
    "\n",
    "        sleep(1)\n",
    "    return dfs\n",
    "\n",
    "\n",
    "url_base = \"https://www.boxofficemojo.com/chart/top_lifetime_gross/?area=XWW&offset=\"\n",
    "urls = seed_generator(url_base)\n",
    "\n",
    "data = scrape_data(urls)\n",
    "data_selection = data.loc[data.years >= 2000]\n",
    "data_selection.to_csv(\"box_office_mojo.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90dde0",
   "metadata": {},
   "source": [
    "## 3. APIs\n",
    "As a researcher you're interested in polarity in online communities and therefore collect data on the distribution of up and down votes on Reddit. Extract a random sample of at least 100 Reddit posts from the [`politics`](https://www.reddit.com/r/politics) and [`science`](https://www.reddit.com/r/science) communities and compare the upvote ratio. Comment on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "56afcab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean upvote ratio in the science and politics subreddits is 0.79 and 0.77 respectively\n"
     ]
    }
   ],
   "source": [
    "def reddit_activity(subreddit, attribute):\n",
    "    after = None\n",
    "    headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=0', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "    activity = []\n",
    "\n",
    "    while len(activity) < 100: \n",
    "        # we pick a random sample by selecting posts by recency (rather than by popularity)\n",
    "        url = f'https://www.reddit.com/r/{subreddit}/new.json'  \n",
    "        response = requests.get(url, \n",
    "                                headers=headers, \n",
    "                                params={\"after\": after})\n",
    "        json_response = response.json()\n",
    "        after = json_response['data']['after']\n",
    "\n",
    "        # loop over all items in a request\n",
    "        for item in json_response['data']['children']:\n",
    "            activity.append(item['data'][attribute])\n",
    "    return pd.Series(activity)\n",
    "\n",
    "science = reddit_activity(\"science\", \"upvote_ratio\")\n",
    "politics = reddit_activity(\"politics\", \"upvote_ratio\")\n",
    "\n",
    "print(f\"The mean upvote ratio in the science and politics subreddits is {round(science.mean(),2)} and {round(politics.mean(),2)} respectively\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549e656",
   "metadata": {},
   "source": [
    "## Question 4 - Workflow \n",
    "\n",
    "Review the following text in which a master student describes the institutional background of the data collection. The thesis centers around the effect of hiding like counts on user behavior and thus proposes a methodology for sample construction. Describe how you would define the treatment and control group, and how you would go about collecting data on a user-level. Keep in mind ethical and legal concerns of collecting and storing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357902b1",
   "metadata": {},
   "source": [
    "*Late April 2019 Instagram announced that it would run an experiment among Canadian users in which the like counts were hidden (Constine 2019). Three months later, around mid-July, they expanded the treatment to users in various other countries including Australia, Canada, and Italy. Users located in these countries could not see the number of likes on media posted by others, whereas users living anywhere else could still view like counts (Loren 2020). Thus, treatment groups enter the treated pool of persons sequentially, and assignment to the treatment or control condition was dependent on usersâ€™ geography.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d8ee7",
   "metadata": {},
   "source": [
    "An answer that includes the following elements:\n",
    "* Mechanism to identify country of origin (e.g., manual coding, selective niches, validating country of origin)\n",
    "* Representative sample (across multiple influencer categories, removing business accounts; controlling for algorithmic biases)\n",
    "* Sample criteria (activity prior to and after the intervention)\n",
    "* Privacy concerns (e.g. only public users, data anonymization)\n",
    "* Data cleaning (removing bots, inactive accounts)\n",
    "* Practical (selenium/prebuilt package; Beautifulsoup would not be a good choice; proxies to avoid getting blocked) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c3407",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1 (small coding task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0186916a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-1-14af29fe716c>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-14af29fe716c>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    for post in retrieved_posts:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "headers = {'authority': 'www.reddit.com', 'cache-control': 'max-age=10', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36', 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'sec-fetch-site': 'same-origin', 'sec-fetch-mode': 'navigate', 'sec-fetch-user': '?1', 'sec-fetch-dest': 'document', 'accept-language': 'en-GB,en;q=0.9'}\n",
    "\n",
    "def get_posts(subreddit):\n",
    "    url = f'https://www.reddit.com/r/{subreddit}.json'\n",
    "    response = requests.get(url,\n",
    "                            headers=headers)\n",
    "    json_response = response.json()\n",
    "    posts = []\n",
    "    for item in json_response['data']['children']:\n",
    "        posts.append({'subreddit name': item['data']['subreddit'],\n",
    "                      'title': item['data']['title'],\n",
    "                    'number of comments:': item['data']['num_comments']})\n",
    "    return posts\n",
    "\n",
    "subreddits = ['marketing', 'digitalmarketing', 'socialmedia']\n",
    "\n",
    "all_posts = [] # create empty list to hold final results\n",
    "\n",
    "# loop through subreddits\n",
    "for sub in subreddits:\n",
    "\t# use `get_users()` function to retrieve post for subreddit `sub`\n",
    "    retrieved_posts = get_posts(sub)\n",
    "\t# loop through posts, and add to `posts` list holding all posts as a final result\n",
    "\tfor post in retrieved_posts:\n",
    "\t\tall_posts.append(post)\n",
    "all_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462a8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
