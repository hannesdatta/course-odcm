{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Advanced (oDCM)\n",
    "\n",
    "In this tutorial, we'll expand on tools like BeautifulSoup, which is essential for static websites but has limitations with dynamic content. While BeautifulSoup remains useful, Selenium allows us to handle both static and dynamic sites by simulating user actions like scrolling, clicking, and logging in. It works within a browser window, making it a more intuitive complement to BeautifulSoup, rather than a replacement.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Students will be able to:\n",
    "- Emulate user interactions on a site\n",
    "  - clicking,\n",
    "  - scrolling,\n",
    "  - filling forms.\n",
    "- Save the retrieved data as JSON files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Selenium \n",
    "\n",
    "### 1.1 Let's recap: Why Selenium? \n",
    "\n",
    "In the Web Scraping 101 tutorial, we mainly used BeautifulSoup to turn HTML into a data structure that we could search and access using Python-like syntax. While it is easy to get started with this library, it has limitations when it comes to dynamic websites. That is, websites of which the content changes - even before actively refreshing it. Examples are TikTok (e.g., having an infinite scroll), or Twitch (e.g., the chat window dynamically updating).\n",
    "\n",
    "Selenium can handle both static and dynamic websites and mimic user behavior (e.g., scrolling, clicking, logging in). It launches another web browser window in which all actions are visible which makes it feel more intuitive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Installing Selenium\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Installing Selenium and Chromedriver</b> \n",
    "\n",
    "To install Selenium and Chromedriver locally, please follow the <a href=\"https://tilburgsciencehub.com/topics/collect-store/data-collection/web-scraping/scrape-dynamic-websites/\">Tutorial on Tilburg Science Hub</a>.\n",
    "    \n",
    "You can also use the code snippet below to automate the installation. Running this snippet takes a little longer each time, but the benefit is that it almost always works!\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.34.0)\n",
      "Requirement already satisfied: urllib3~=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: trio~=0.30.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.4.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions~=4.14.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (4.14.0)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: webdriver_manager in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from webdriver_manager) (2.32.4)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from webdriver_manager) (1.1.1)\n",
      "Requirement already satisfied: packaging in /Users/hannesdatta/Library/Python/3.12/lib/python/site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->webdriver_manager) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->webdriver_manager) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->webdriver_manager) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->webdriver_manager) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "# Installing necessary packages\n",
    "!pip3 install selenium\n",
    "!pip3 install webdriver_manager\n",
    "\n",
    "# Importing required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Setting up Chrome WebDriver with WebDriver Manager using Service\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# Opening the 'music to scrape' website\n",
    "url = \"https://music-to-scrape.org/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Optional: Adding some wait time for the page to fully load if needed\n",
    "driver.implicitly_wait(10)  # 10 seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smooth, your computer opened a new Chrome window, and opened `music-to-scrape`. \n",
    "\n",
    "__Importantly, note that you have to run the code cell always when you want to open a new instance of Chrome.__\n",
    "__If you want to close Chrome, you can use `driver.quit()`.__\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Using Google Colab</b> \n",
    "\n",
    "If you're using Google Colab, you don't see your browser open up manually.\n",
    "    \n",
    "Whenever you switch pages, just manually open that page in your browser. Although this feels like a little less interactive, you will still be able to work through this tutorial!\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Getting Access to a Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we're going to tell the browser to visit the \"Music to Scrape\" website. We'll use the `driver` object we created earlier and call the `get` method, passing the URL of the website we'd like to extract data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://music-to-scrape.org/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, we can use BeautifulSoup as we learned previously, though we create the `res` object from the `driver` object this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Selenium in Combination with BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium is powerful for interacting with dynamic websites, but once the webpage has loaded, we can still use BeautifulSoup for parsing the HTML and extracting data efficiently. BeautifulSoup is well-suited for quickly navigating and querying the page's source code, while Selenium handles the initial loading and interaction with dynamic content. This combination gives us flexibility and efficiency in our scraping tasks.\n",
    "\n",
    "Here's an example of how to use Selenium to load a page and then pass the HTML to BeautifulSoup for parsing. We'll extract the title of the page from https://music-to-scrape.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Title: We love being scraped | music-to-scrape\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Selenium WebDriver (make sure you have a WebDriver installed)\n",
    "# (see code snippets above)\n",
    "\n",
    "# Use Selenium to load the page\n",
    "driver.get(\"https://music-to-scrape.org\")\n",
    "\n",
    "# Get the page source and pass it to BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Example: Extracting the page title using BeautifulSoup\n",
    "page_title = soup.find('title').get_text()\n",
    "print(\"Page Title:\", page_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, Selenium loads the webpage, and then we pass the page’s source code to BeautifulSoup to extract the page title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your turn. Use the same combination of Selenium and BeautifulSoup to store the names of the artists in the top 15 weekly tracks in a JSON dictionary.\n",
    "\n",
    "__Hint:__\n",
    "\n",
    "Look at the structure of the page in the browser's developer tools to find where the artist's name is located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Solution - Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Gabriel Yared'},\n",
       " {'name': 'Danny Williams'},\n",
       " {'name': 'Pascal Obispo'},\n",
       " {'name': 'Vangelis'},\n",
       " {'name': 'Solas'},\n",
       " {'name': 'Joi'},\n",
       " {'name': 'Stevie Ray Vaughan And Double Trouble'},\n",
       " {'name': 'Magnatune Compilation'},\n",
       " {'name': 'Mint Condition'},\n",
       " {'name': 'Enslavement Of Beauty'},\n",
       " {'name': 'Gerald Veasley'},\n",
       " {'name': 'Sofi Marinova'},\n",
       " {'name': 'Elmore James'},\n",
       " {'name': 'Snow Patrol'},\n",
       " {'name': 'Converge'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Use Selenium to load the page\n",
    "driver.get(\"https://music-to-scrape.org\")\n",
    "\n",
    "# Wait until the website is loaded\n",
    "time.sleep(2)\n",
    "\n",
    "# Get the page source and pass it to BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Solution: Extracting the first artist's name\n",
    "section = soup.find('section', attrs={'name': 'weekly_15'})\n",
    "\n",
    "tracks = section.find_all(class_='list-group-item')\n",
    "\n",
    "json_dic = []\n",
    "\n",
    "for tr in tracks:\n",
    "    json_dic.append({'name': tr.find('h5').get_text()})\n",
    "json_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we combined Selenium with BeautifulSoup to scrape data from a dynamically loaded website. After using Selenium to load the page and waiting for it to fully render, we passed the HTML to BeautifulSoup for parsing. This allowed us to extract data efficiently, like the weekly top 15 tracks, and store it in a JSON-like format. This method leverages the strengths of both tools: Selenium for dynamic content handling and BeautifulSoup for data extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interacting with Dynamic Websites\n",
    "\n",
    "### 3.1 Clicking on buttons with Selenium\n",
    "In many websites, content is hidden or only appears after certain actions like clicking a button. While BeautifulSoup is excellent for parsing static HTML, it cannot perform actions such as clicking or interacting with dynamic elements. For example, a button might load new content that doesn’t appear in the initial HTML source, or content might change after interacting with certain elements on the page (e.g., loading more items, navigating through a slideshow, etc.). This is where Selenium comes in—it can mimic user interactions like clicking, scrolling, and more, making it a powerful tool for handling dynamic websites.\n",
    "\n",
    "In this section, we’ll show you how to use Selenium to click on a button. For example, let’s try to click the “Previous Page” button on this profile page: https://music-to-scrape.org/user?username=PandaVector67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Section Title: Recently Played between 2025-10-20 and 2025-10-26\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Navigate to the specific user's page\n",
    "driver.get(\"https://music-to-scrape.org/user?username=PandaVector67\")\n",
    "\n",
    "# Wait until the website is fully loaded\n",
    "time.sleep(2)\n",
    "\n",
    "# Locate and click the 'Previous Page' button by its class or identifier (depending on the site structure)\n",
    "previous_button = driver.find_element(By.LINK_TEXT, \"Previous Week\")\n",
    "previous_button.click()\n",
    "\n",
    "# Wait a bit to allow the new page to load\n",
    "time.sleep(2)\n",
    "\n",
    "# Get the new page's source and pass it to BeautifulSoup if needed\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# (Optional) Print the new section title to confirm the action\n",
    "print(\"New Section Title:\", soup.find_all('h2')[1].get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explanation:__\n",
    "1. Selenium Interaction: We used Selenium to load the webpage and then locate the “Previous Page” button.\n",
    "2. Click Action: Using find_element() with the By.LINK_TEXT method, we identified the button by its label text and triggered a click.\n",
    "3.  Post-Click Action: After clicking, we waited for the new content to load and optionally passed it to BeautifulSoup for further scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Primer: Locating Elements in Selenium</b> \n",
    "\n",
    "<p>In Selenium, there are several ways to locate elements on a webpage. While we used <code>LINK_TEXT</code> in the example to click a button, this is just one of many methods available for finding elements. Unlike BeautifulSoup, which relies on methods like <code>find</code> and <code>find_all</code>, Selenium has its own set of strategies for interacting with elements.</p>\n",
    "\n",
    "\n",
    "<p>Here are some common ways to locate elements in Selenium:</p>\n",
    "<ul>\n",
    "  <li><strong>By ID</strong>: <code>driver.find_element(By.ID, \"element_id\")</code></li>\n",
    "  <li><strong>By Name</strong>: <code>driver.find_element(By.NAME, \"element_name\")</code></li>\n",
    "  <li><strong>By Class Name</strong>: <code>driver.find_element(By.CLASS_NAME, \"class_name\")</code></li>\n",
    "  <li><strong>By Tag Name</strong>: <code>driver.find_element(By.TAG_NAME, \"tag_name\")</code></li>\n",
    "  <li><strong>By CSS Selector</strong>: <code>driver.find_element(By.CSS_SELECTOR, \"css_selector\")</code></li>\n",
    "  <li><strong>By XPath</strong>: <code>driver.find_element(By.XPATH, \"xpath_expression\")</code></li>\n",
    "</ul>\n",
    "\n",
    "<p>It is important to note that the syntax and methods in Selenium differ from BeautifulSoup. While BeautifulSoup focuses on parsing HTML and querying the structure, Selenium is designed to interact with the browser, and thus its element locators reflect that.</p>\n",
    "\n",
    "<p>To avoid confusion, keep in mind that the two libraries use different conventions for querying elements, and you will need to adjust your approach accordingly.</p>\n",
    "\n",
    "<p>For more information on the various ways to locate elements in Selenium, check out the official documentation <a href=\"https://selenium-python.readthedocs.io/locating-elements.html\" target=\"_blank\">here</a>.</p>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Scrolling on a site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping websites, you may encounter pages with infinite scrolling or content that only loads when you scroll down, such as social media feeds or product listings. BeautifulSoup cannot handle scrolling since it only interacts with the initial HTML source code. However, Selenium can simulate user interactions like scrolling, making it a critical tool for scraping websites with dynamic loading.\n",
    "\n",
    "In this section, we'll cover how to scroll on a webpage using Selenium, and we'll explain different stopping rules to control the scrolling behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example: Slow Scrolling on music-to-scrape.org__\n",
    "\n",
    "Let's start with a simple example of scrolling through the webpage https://music-to-scrape.org. The following code snippet shows how to scroll slowly down the page using Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling for the 1th time\n",
      "Scrolling for the 2th time\n",
      "Scrolling for the 3th time\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Navigate to the website\n",
    "driver.get(\"https://music-to-scrape.org\")\n",
    "\n",
    "# Sleep to allow the page to load completely\n",
    "time.sleep(2)\n",
    "\n",
    "# Scroll down the page slowly\n",
    "scroll_pause_time = 1\n",
    "for _ in range(3):  # Scroll down 3 times\n",
    "    print(f'Scrolling for the {_+1}th time')\n",
    "    driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(scroll_pause_time)  # Pause to simulate human-like scrolling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example:\n",
    "- We load the website and use Keys.PAGE_DOWN to simulate the down arrow key, which scrolls the page down incrementally.\n",
    "- We include a pause (time.sleep()) between each scroll to mimic a more natural, human-like behavior.\n",
    "\n",
    "There are different stopping rules to control when the scrolling should end:\n",
    "\n",
    "1. Until a certain number of iterations: You can control the scroll by a fixed number of iterations, as shown in the example above, where we scrolled 10 times. This is useful when you want to limit the depth of your scraping (see example above).\n",
    " \n",
    "2. Until an element is located: You might want to stop scrolling once a specific element appears on the page, such as a \"Load More\" button or a particular section.\n",
    "```\n",
    "while not driver.find_elements(BY.CLASS_NAME, 'target-element'):\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.PAGE_DOWN)\n",
    "```\n",
    "\n",
    "3. Until the end of the page is reached: To stop scrolling when the page reaches its bottom, you can compare the current scroll position to the total page height.\n",
    "\n",
    "```\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:  # Stop if the scroll height hasn't changed\n",
    "        break\n",
    "    last_height = new_height\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Exercise 2\n",
    "\n",
    "Now it’s your turn. Use Selenium to scroll through the https://music-to-scrape.org website and extract the name of each track displayed in 'Top 15 Weekly Tracks' after scrolling down several times.\n",
    "1. Scroll down the page five times.\n",
    "2. Store the names of all tracks that appear after scrolling in a JSON dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Solution - Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrolling for the 1th time\n",
      "Scrolling for the 2th time\n",
      "Scrolling for the 3th time\n",
      "Scrolling for the 4th time\n",
      "Scrolling for the 5th time\n",
      "[\n",
      " {\n",
      "  \"name\": \"Gabriel Yared\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Danny Williams\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Pascal Obispo\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Vangelis\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Solas\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Joi\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Stevie Ray Vaughan And Double Trouble\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Magnatune Compilation\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Mint Condition\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Enslavement Of Beauty\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Gerald Veasley\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Sofi Marinova\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Elmore James\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Snow Patrol\",\n",
      "  \"iteration\": 0\n",
      " },\n",
      " {\n",
      "  \"name\": \"Converge\",\n",
      "  \"iteration\": 0\n",
      " }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()  # Ensure you have the correct path to chromedriver\n",
    "driver.get(\"https://music-to-scrape.org\")\n",
    "\n",
    "# Sleep to allow the page to load completely\n",
    "time.sleep(2)\n",
    "\n",
    "json_dic = []\n",
    "\n",
    "# Scroll down the page slowly\n",
    "scroll_pause_time = 1\n",
    "for _ in range(5):  \n",
    "    print(f'Scrolling for the {_+1}th time')\n",
    "    driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(scroll_pause_time)  # Pause to simulate human-like scrolling\n",
    "    \n",
    "    # Get page source and parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # here songs are within elements with class 'center-vertical'\n",
    "    for song in soup.find_all(class_='center-vertical'):\n",
    "        # Extract only the track name (it's in an 'b' tag within 'center-vertical class')\n",
    "        track_title = song.find('b') \n",
    "        if track_title:\n",
    "            song_name = track_title.get_text(strip=True)\n",
    "            \n",
    "            # Check if the extracted name is valid (non-empty) and avoid duplicates\n",
    "            if song_name and song_name not in [item['name'] for item in json_dic]:\n",
    "                json_dic.append({'name': song_name, 'iteration': _})\n",
    "\n",
    "\n",
    "\n",
    "# Print or save the JSON dictionary\n",
    "json_data = json.dumps(json_dic, indent=1)\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this solution:\n",
    "- We scrolled the page five times using Keys.PAGE_DOWN.\n",
    "- After scrolling, we passed the current page source to BeautifulSoup to extract the track names.\n",
    "- We stored the names of all tracks found on the page, along with the iteration number, in a JSON dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Infinite scrolling \n",
    "\n",
    "So far we have specified the number of times we'd like to scroll but in some websites, infinite scrolling is a common feature in modern websites (e.g. Twitter). In such sites, new content is dynamically loaded as the user scrolls down the page. However, it poses a unique challenge for web scraping because the HTML content changes dynamically as new data is loaded. Therefore, to effectively scrape data from such websites, we need to scroll down the page, wait for new content to load, and then capture the updated HTML. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example: Infinite Scrolling on music-to-scrape.org__\n",
    "\n",
    "In this example, we will retrieve all the recently played track titles for the user \"Pixel25\" from the following page: [https://music-to-scrape.org/user?username=Pixel25](https://music-to-scrape.org/user?username=Pixel25).\n",
    "\n",
    "(Note: While music-to-scrape.org loads all its data at once and isn't fully dynamic, this code serves as a demonstration of how to handle infinite scrolling on websites that do load content dynamically.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (127.0.6533.119) detected in PATH at /usr/local/bin/chromedriver might not be compatible with the detected chrome version (128.0.6613.138); currently, chromedriver 128.0.6613.137 is recommended for chrome 128.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Ghost of the Ocean (Live)\",\n",
      "  \"Tables Turn\",\n",
      "  \"Your Heart Can Sing\",\n",
      "  \"Con La Tinta De Mi Sangre\",\n",
      "  \"God Is The Answer (Footprints In The Sand Album Version)\",\n",
      "  \"Lunes Por La Madrugada\",\n",
      "  \"The Imperial March from The Empire Strikes Back\",\n",
      "  \"Sapphire\",\n",
      "  \"From A Jack To A King\",\n",
      "  \"Irish Medley - Jigs\",\n",
      "  \"The World'll Be OK\",\n",
      "  \"That's A Woman\",\n",
      "  \"The drifter\",\n",
      "  \"Mangos\",\n",
      "  \"Don't Owe Me Nothin'\",\n",
      "  \"Anvi Viv?\",\n",
      "  \"Shaggy Dog Songs\",\n",
      "  \"Beauty Supply And Demand\",\n",
      "  \"Pu**y Pop\",\n",
      "  \"Christmas Time\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://music-to-scrape.org/user?username=Pixel25\")\n",
    "\n",
    "# Allow the page to load\n",
    "time.sleep(2)\n",
    "\n",
    "track_titles = []\n",
    "\n",
    "# Infinite scrolling to the bottom\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll to the bottom of the page\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "    # Pause for a moment to allow content to load\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Get the current scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    # Break the loop if the scroll height has not changed\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "# Get the page source and parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the table containing track information\n",
    "table = soup.find('table', class_='table-striped')\n",
    "if table:\n",
    "    # Find all rows in the table body\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "    for row in rows:\n",
    "        # Extract text from the first <td> element for the track title\n",
    "        columns = row.find_all('td')\n",
    "        if columns:\n",
    "            track_title = columns[0].get_text(strip=True)\n",
    "            if track_title and track_title not in track_titles:\n",
    "                track_titles.append(track_title)\n",
    "\n",
    "# Print the titles in JSON format\n",
    "song_data = json.dumps(track_titles, indent=2)\n",
    "print(song_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Solution - Exercise 3\n",
    "\n",
    "- We used a `while` loop to simulate infinite scrolling:\n",
    "  - Scrolled to the bottom of the page using JavaScript (`window.scrollTo`).\n",
    "  - Paused for a moment (`time.sleep(1)`) to allow content to load.\n",
    "  - Checked the page height to determine if new content was loaded; broke out of the loop when no more content was found.\n",
    "- After completing the scrolling, passed the page source to BeautifulSoup for HTML parsing.\n",
    "- Located the table containing track information using its class (`'table-striped'`).\n",
    "- Iterated through each row in the table, extracting the text from the first column (`<td>`) of each row, which contains the track titles.\n",
    "- Added each unique track title to the `track_titles` list.\n",
    "- Converted the list of track titles to a JSON format and printed the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Filling in forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in forms is a common task when interacting with websites, especially when you need to perform actions like logging in, searching, or submitting data. While BeautifulSoup is excellent for parsing static HTML, it cannot interact with forms or simulate user inputs. Selenium, on the other hand, can fill in forms, submit them, and interact with various input fields on a page, making it ideal for tasks like automated searches.\n",
    "\n",
    "In this section, we’ll explore how to use Selenium to fill in a search form, query for artist names, and extract results. We'll query the search bar on https://music-to-scrape.org using artist names found on the page itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Navigate to the website\n",
    "driver.get(\"https://music-to-scrape.org\")\n",
    "time.sleep(2)\n",
    "\n",
    "artists = ['Adrenalin', 'Presets', 'Lupe']\n",
    "\n",
    "# Loop through each artist and perform a search\n",
    "for artist_name in artists:\n",
    "    # Locate the search input field\n",
    "    search_bar = driver.find_element(By.NAME, 'query')\n",
    "\n",
    "    # Clear any previous input, then type the artist's name and submit the form\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(artist_name)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for search results to load\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we combine Selenium and BeautifulSoup to automate searches on the website music-to-scrape.org, search for multiple artist names, and extract results for each query. Here's a breakdown of how the process works:\n",
    "\n",
    "1. __Loading the Website:__     We start by navigating to the website using Selenium's driver.get() method and give the page a moment to load using time.sleep().\n",
    "2. __Looping Through Artists:__    We define a list of artist names (['Adrenalin', 'Presets', 'Lupe']) and loop through each name to perform a search on the site.\n",
    "3. __Interacting with the Search Bar:__  For each artist, we locate the search input field using driver.find_element(By.NAME, 'query'). We clear the field, input the artist's name, and simulate pressing the return key to submit the search.\n",
    "4. __Waiting for Results:__     After submitting the search, we wait for the results to load using another time.sleep(), allowing the page to update with the new search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Exercise 4\n",
    "\n",
    "Extend the code snippet above to store all search results in a JSON dictionary (stored on disk, new-line separated).\n",
    "\n",
    "### ✅ Solution - Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Navigate to the website\n",
    "driver.get(\"https://music-to-scrape.org\")\n",
    "time.sleep(1)\n",
    "\n",
    "artists = ['Adrenalin', 'Presets', 'Lupe']\n",
    "\n",
    "f = open('output.json', 'a')\n",
    "\n",
    "# Loop through each artist and perform a search\n",
    "for artist_name in artists:\n",
    "    # Locate the search input field\n",
    "    search_bar = driver.find_element(By.NAME, 'query')\n",
    "\n",
    "    # Clear any previous input, then type the artist's name and submit the form\n",
    "    search_bar.clear()\n",
    "    search_bar.send_keys(artist_name)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for search results to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    results = soup.find(class_='container').find_all(class_='list-group-item')\n",
    "    counter = 0\n",
    "    for row in results:\n",
    "        counter = counter + 1\n",
    "        res = {} # empty dictionary\n",
    "        res['query'] = artist_name\n",
    "        res['rank'] = counter # rank of search result\n",
    "        res['type'] = row.find(class_='center-text').find('p').get_text()\n",
    "        res['result'] = row.find(class_='center-text').get_text().strip()\n",
    "        f.write(json.dumps(res))\n",
    "        f.write('\\n')\n",
    "f.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we’ve walked through how to automate the process of filling in a search form, extracting the resulting data, and saving it in a structured format using Selenium and BeautifulSoup.\n",
    "\n",
    "Here's a quick summary:\n",
    "1. After submitting each search, we pause briefly using `time.sleep()` to allow the page to load fully before parsing the content. This ensures that we capture the updated results from the search.\n",
    "2. Once the results page has loaded, we use BeautifulSoup to parse the page’s HTML. We specifically look for the `container` section where the search results are displayed, with each result being located within a `list-group-item` class. This allows us to accurately extract the data we’re interested in.\n",
    "3. For each result, we create a JSON-like dictionary to store:\n",
    "- **The artist name** (`query`) that was used in the search.\n",
    "- **The rank** (`rank`) of each result, incremented for each item in the list.\n",
    "- **The type of result** (`type`), such as \"Song\" or \"Album\", which we find in the `<p>` tag within each result.\n",
    "- **The full result text** (`result`), which includes the specific details about the result, extracted from the relevant element.\n",
    "4. We then write each result into an `output.json` file, ensuring each search result is stored as an individual JSON object. This allows us to save our scraped data in a structured and reusable format for future analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Summary\n",
    "\n",
    "In this tutorial, you've explored how to combine Selenium and BeautifulSoup to scrape data from dynamic websites effectively. Here's a quick summary of key takeaways:\n",
    "\n",
    "- Selenium vs. BeautifulSoup:\n",
    "  - Selenium: Ideal for interacting with dynamic websites, such as handling forms, clicking buttons, and simulating user actions like scrolling. It works by controlling a browser and mimicking user behavior.\n",
    "  - BeautifulSoup: Best suited for parsing static HTML content and extracting data from the page source. It cannot interact with dynamic elements but is excellent for navigating and querying structured HTML.\n",
    "-   Using Them Together: By combining Selenium for interaction and BeautifulSoup for parsing, you can scrape dynamic sites, automate form submissions, extract data, and store it efficiently in formats like JSON.\n",
    "\n",
    "This combination allows you to tackle more complex scraping tasks by automating the interaction with dynamic content while still benefiting from BeautifulSoup’s simplicity and efficiency in data extraction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Regular)",
   "language": "python",
   "name": "regularpython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
