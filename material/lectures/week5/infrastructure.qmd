---
title: "Infrastructural Considerations for Web Scraping Projects"
subtitle: "Online Data Collection & Management" # (2025/2026)"
author: "Hannes Datta"
date: "last-modified"
format: 
    beamer:
      theme: metropolis
      colortheme: seahorse
      linkcolor: blue
---

# Overview and Agenda

## Why Infrastructure Matters

- Web data projects (scraping and APIs) are often **long-running** (hours → weeks → months) or require __mitigation of legal and ethical risks__
  - __But:__ Machines crash; hard disks fill up; websites change; networks fail; security could be compromised; data could leak...
- __Good infrastructure__ ensures:  
  - reliability (expectation it will work now and in the future)
  - data quality (alert when data collection stops)
  - security compliance (e.g., hiding API credentials, using right permissions)
  - GDPR compliance (e.g., anonymization on the fly)
  - lower legal/ethical risk (e.g., data minimization, choice of jurisdiction)

## Agenda

1. Computer Infrastructure for Web Scraping
2. Governance, Security & Compliance
3. Summary

# 1. Computer Infrastructure for Web Data Projects

## Choice 1: Compute Environment

__Local Machine__

- Best for prototyping (e.g, easy debugging, fast iteration, admin rights to install new packages)
- Weaknesses:
  - laptop sleeps/shuts down during collection
  - unstable WiFi lowers data quality
  - not suitable for sensitive data (e.g., a laptop could be stolen)
- Use only for early-phase development  

---

## Choice 1: Compute Environment (continued)

__Institutional/organizational Servers__

- Proper "server", remote login 
- Usually stable uptime and automatic backups
- Better for GDPR compliance (e.g., if being in your organization's network)
- Much more reliable for long-running scraping jobs 
- Often free / no marginal cost  

---

## Choice 1: Compute Environment (continued)

__Cloud Virtual Machines__

- “Rent a virtual machine (i.e., computer) in a data center” (e.g., Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP)
- Advantages:
  - always-on, choice of operation system, number of CPUs, RAM, hard disk...
  - scalable (e.g., up-scale or downscale a VM)
  - easy to redeploy (i.e., turn machine into template and "copy" 10 times...)
- Consider:
  - marginal costs (compute, storage, data transfer)
  - best to use Linux (to avoid Microsoft's licensing fees for Windows); choice: Ubuntu Desktop to start

---

## Choice 2: Automation

__Scheduling__

- Use the **scheduler** on your operating system:
  - Windows → Task Scheduler
  - macOS/Linux → `cron`
- Reliable for:
  - daily/weekly scrapes  
  - monitoring jobs  
  - incremental updates  

Scheduling scrapers at the level of your operating system (i.e., not in Python or R) is recommended in research workflows.[^retail]

[^retail]: Guyt et al. (2024), *Journal of Retailing*.

---

## Choice 2: Automation (continued)

__Scheduling require scripts to be "restartable"__

- Scrapers should **resume** after minor failures, and **crash** at larger failures
  - prevents hanging scrapers and half-finished datasets  
- For catching minor failures:
  - try–except blocks
  - waiting a few seconds before retrying
  - checkpointing progress
- Larger failures (e.g., relating to a package update, to a version conflict with Chromedriver) cannot always be anticipated, and it's fine for a scraper to also simply "crash"

---

## Choice 3: Storage & Retention

__Flat Files (CSV / JSON)__

- Good for most research projects  
- Easy to validate, inspect, share  
- Human-readable  
- Lightweight and transparent  

---

## Choice 3: Storage & Retention (continued)

__Raw HTML Snapshots__

- Store **full HTML** of each page  
- Essential for reproducibility:
  - websites change layout  
  - selectors must be updated  
  - raw HTML enables re-extraction  
- Often recommended in academic scraping[^fields], but can increase legal risks

[^fields]: Boegershausen, Datta, Borah & Stephen (2022), *Fields of Gold*.

---

## Choice 3: Storage & Retention (continued)

__Databases__

- SQL is the standard of data storage/handling and a must for marketing analytics students to master
- SQL has different implementations
  - server-based (e.g., PostgreSQL / MySQL)
  - file-based (e.g., SQLite)
- Useful for:
  - incremental scraping
  - avoiding duplicate pages
  - storing metadata (such as logs)
- Much safer than large CSV collections
  - use SQLite for small-scale collections; use server-based SQL implementations for medium- to large projects

---

## Storage & Retention (continued)

__Cloud Object Storage__

- Amazon S3 / Azure Blob / Google Cloud Storage  
- Think of it as a “large folder in the cloud”  
- Ideal for:
  - HTML dumps  
  - storing images, larger files
- Supports backup, versioning & retention policies natively

---

## Choice 4: Monitoring & Health Checks

__Metadata-Based Monitoring / Logs__

Log for every run:

- URLs visited + HTTP status codes (e.g., 200, 404, 500)
- timestamps
- number of items scraped, file sizes
- errors / retries

Logs support:

- debugging (e.g., website/endpoint changes, rate limit issues, connection failure)
- scientific transparency
- compliance audits  

---

## Choice 4: Monitoring & Health Checks (continued)

__Notifications__

- Automatic alerts when something breaks:  
  - Email  
  - Slack  
  - Push notifications (e.g., via Pushover, ntfy.sh) -- API-based
- Helps maintain unattended scrapers  
- Ensures “silent failures” do not accumulate  

---

## Choice 5: Maintenance, Security & Compliance

__Versioning Environments__

- Use reproducible environments:
  - R: `renv`
  - Python: `virtualenv` / `requirements.txt`  
- Prevents:
  - “works on my machine” issues  
  - dependency drift  
- Supports long-term maintainable scraping pipelines  

---

## Choice 5: Maintenance, Security & Compliance (continued)

__Docker__

- Package your scraper, dependencies, and environment in a single container  
- Ensures reproducibility across machines and servers  
- Eliminates “Python version mismatch” and system-level dependency issues  
- Best for:
  - Long-running scraping projects  
  - Pipelines that multiple people need to run
  - Deploying scrapers on remote machines or many VMs
  
---

## Choice 5: Maintenance, Security & Compliance (continued)

__Secrets Management__

- Never hard-code credentials  
- Store API keys in:
  - `.env` files  
  - environment variables  
- Avoid committing secrets to GitHub  

---

## Governance, Security & Compliance (continued)

__GDPR-Safe Infrastructure__

For any scraping that collects personal data:

- use encrypted disks  
- restrict folder permissions  
- delete identifiers when possible  
- document processing & retention  
- avoid personal laptops if insecure  

## Governance, Security & Compliance (continued)

__Ethical Scraping Practices__

- Minimize scraping frequency  
- Minimize amount of collected data  
- Maintain audit logs  
- Adapt scraping to public interest (research context)  
- Use scheduling responsibly  
- Avoid unnecessary load  


---

# 3. Summary

## Summary

- Infrastructure = key to reliability, reproducibility, and compliance  
- Choose compute environment wisely  
- Automate with simple, robust tools  
- Store raw HTML for future-proof extraction  
- Monitor scraper health  
- Log everything  
- Protect data & credentials  
- Build ethically and responsibly  
