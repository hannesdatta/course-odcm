{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101\n",
    "\n",
    "*By the end of this tutorial, you’ll scrape data from multiple web pages and export it to JSON and CSV for analysis. Set aside a few hours and take breaks to stay sharp.*\n",
    "\n",
    "*New to web scraping? Start with the [\"Webdata for Dummies\" tutorial](https://odcm.hannesdatta.com/docs/modules/week2/webdata-for-dummies/).*\n",
    "\n",
    "*Enjoy!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Our main goal is to compile a panel data set of music consumption data for (simulated) users of music-to-scrape.org, a platform developed for practicing web scraping skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Generating seeds**: Extract multiple elements with `.find_all()`, avoiding array misalignment  \n",
    "* **Navigating websites**: Visit pages via URLs and use loops for bulk data collection  \n",
    "* **Optimizing extraction**: Add timers, modularize code, and save data with metadata in CSV/JSON  \n",
    "* **Scraping advanced sites**: Headless requests vs. browser emulation (`requests` vs. `selenium`)  \n",
    "--- \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating seeds (\"sampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__  \n",
    "\n",
    "So far, we’ve parsed data from individual *artist pages* (e.g., featured artists' names), but we haven't explored individual user behavior yet. User-level data is often central to web scraping, like sampling tweets from Twitter/X or tracking movie-watching patterns on trakt.tv.  \n",
    "\n",
    "To build a *panel dataset* (multiple users observed over time), we must first decide __which users to track__. This requires generating a *sample of users* (or books, movies, series, games—depending on the platform).  \n",
    "\n",
    "In web scraping, a \"seed\" is the starting point for data collection—without it, there’s no data. For example, before crawling all users at [music-to-scrape.org](https://music-to-scrape.org), we need a *list of users*. Obtaining every username is nearly impossible, so we can:  \n",
    "\n",
    "1. Visit the [music-to-scrape.org](https://music-to-scrape.org) homepage to find recently active users.  \n",
    "2. Go to each user's profile and scrape their data (as done in the *Webdata for Dummies* tutorial).  \n",
    "\n",
    "The homepage provides navigation to user profiles by clicking usernames or avatars (see red boxes below).  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-users.png\" align=\"left\" width=\"80%\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Collecting Links as Seeds  \n",
    "\n",
    "Open the [website](https://music-to-scrape.org) and inspect the HTML using Chrome or Firefox (right-click → *Inspect*). Hover over elements and select a user avatar.  \n",
    "\n",
    "Notice that each user has a clickable `<a>` tag with an `href` pointing to their profile page.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-inspect-link.png\" align=\"left\" width=\"60%\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how could we tell a computer to capture the links to the various user pages?\n",
    "\n",
    "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>How to extract multiple elements at once?</b>\n",
    "    <br>\n",
    "    \n",
    "- By working through other tutorials, you may already be familiar with the <code>.find()</code> function of BeautifulSoup. The <code>.find()</code> function returns the <b>first element</b> that matches your particular \"search query\". <br>\n",
    "- If you want to extract <b>all elements</b> that match a particular search pattern (say, a class name), you can use BeautifulSoup's <code>.find_all()</code> function.<br>\n",
    "- Note that the \"result\" of the <code>.find_all()</code> option is a list of results __that you need to iterate through.__\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1__  \n",
    "\n",
    "Run the code below to extract all `<a>` tags and print their `href` values. Don’t worry about understanding the code yet—we’ll break it down step by step soon!  \n",
    "\n",
    "Look closely at the extracted links. Not all are relevant for user profiles.  \n",
    "\n",
    "**Task:** Make a list of links *not* pointing to user pages. Which links are these, and why do they appear?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://music-to-scrape.org'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "for link in soup.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "The links we want to ignore are...\n",
    "\n",
    "* The links to the about or privacy pages\n",
    "* Any link pointing to the most popular songs or artists\n",
    "* Any social media links, etc.\n",
    "\n",
    "These links are present on the page, because they are used by users to navigate on the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collecting Targeted Information  \n",
    "\n",
    "__Importance__  \n",
    "\n",
    "When scraping data, extracting everything by a general tag often returns irrelevant information. To get more targeted results, we need to be more specific in how we select elements—in this case, links are just one example. __The goal is to extract only the data we care about and ignore the rest.__  \n",
    "\n",
    "To illustrate this, let’s inspect the \"recently active users\" section again. __Open your browser’s inspect tool and hover over that section.__  \n",
    "\n",
    "You’ll see that the user links are inside a `<section>` with the attribute `name=\"recent_users\"`. This structure helps us focus on the relevant content, while unrelated elements like \"about\" or \"privacy\" links are excluded.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-section.png\" align=\"left\" width=\"60%\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now focus our scraper to target only the `<a>` tags *inside* the `<section>` with the attribute `name=\"recent_users\"`. This way, we filter out irrelevant links and get exactly what we need.  \n",
    "\n",
    "__Let’s try it out!__  \n",
    "\n",
    "We’ll still use `.find_all()` to capture matching elements on the page. However, instead of directly extracting `<a>` tags, we’ll first select the specific section containing the relevant links and then collect the `<a>` tags within it.  \n",
    "\n",
    "Run the code below to see how it works. First, we grab elements from the `recent_users` section, then extract all `<a>` tags from that section.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make request\n",
    "url = 'https://music-to-scrape.org'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "users = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        users.append(link.attrs['href'])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we retrieve up to six user names. You can now also use the `users` object to look at the data for the first, second, third, ... user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users[0] # returns the link to the user page of the 1st user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to subsequently try to extract the link for the first book..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the user list still contains a lot of \"other\" things, unrelated to the user name. Remember, we extracted the __links__ to the profile pages, not just the user names.\n",
    "\n",
    "If we want to remove anything but the usernames, we can modify our extraction function slightly, for example using Python's `split` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        users.append(link.attrs['href'].split('=')[1])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need explanation on this code? Just copy-paste it to ChatGPT and ask for an explanation, e.g., using this prompt:\n",
    "\n",
    "> I struggle to understand this piece of Python code in the context of web scraping. \n",
    "> Can you please explain it, paying attention to the complicated last line (user.append())?\n",
    "\n",
    "Pretty cool, right? So let's proceed with some exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2  \n",
    "\n",
    "1. Modify the loop (`for link in relevant_section...`) to extract *absolute URLs* instead of relative ones. Combine the website's base URL (`https://music-to-scrape.org/`) with the extracted string (e.g., `user?username=GalaxyShadow34`). The final URL should look like: `https://music-to-scrape.org/user?username=GalaxyShadow34`.  \n",
    "\n",
    "2. Wrap your code from step 1 in a function called `get_users()`. This function should return an array of user profile links. We’ll use it later to repeatedly collect user names (seeds) from this page.  \n",
    "\n",
    "3. Run your `get_users()` function inside a `while` loop that executes every 2 seconds for 15 seconds. Write all collected URLs to a new-line-separated JSON file named `seeds.json`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1 \n",
    "urls = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        extracted_link = link.attrs['href']\n",
    "        urls.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_users():\n",
    "    url = 'https://music-to-scrape.org/'\n",
    "  \n",
    "    res = requests.get(url)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "    links = []\n",
    "    for link in relevant_section.find_all(\"a\"):\n",
    "        if 'href' in link.attrs: \n",
    "            extracted_link = link.attrs['href']\n",
    "            links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "    return(links) # to return all links\n",
    "\n",
    "get_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Define the duration in seconds (1 minute = 60 seconds)\n",
    "duration = 15\n",
    "\n",
    "# Calculate the end time\n",
    "end_time = time.time() + duration\n",
    "\n",
    "f = open('seeds.json','a')\n",
    "\n",
    "# Run the loop until the current time reaches the end time\n",
    "while time.time() < end_time:\n",
    "    for user in get_users():\n",
    "        f.write(json.dumps(user)+'\\n')\n",
    "    time.sleep(2)  # Sleep for a few seconds between each execution\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Working with JSON data in Python</b>\n",
    "    <br>\n",
    "    In Python, we often need to work with JSON data, which is a common format for exchanging information. \n",
    "    \n",
    "- To make a string (such as one read from a file) queryable as JSON, we use the <code>json.loads()</code> function.\n",
    "  The <code>json.loads()</code> function takes a JSON-formatted string and converts it into a Python data structure, such as a dictionary or a list, so you can easily access its contents.\n",
    "- If you want to save a Python data structure as a JSON file, you can use the <code>json.dumps()</code> function.\n",
    "        The <code>json.dumps()</code> function takes a Python object, like a dictionary or a list, and converts it into a JSON-formatted string that you can save to a text file for later use.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Preventing array misalignment\n",
    "\n",
    "So far, we have only extracted *one* piece of information (the URL) from the list of recently active users. But, what if we want to also extract the names of recently consumed songs? For example, you can view this song by hovering over the user profile pictures on the landing page.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-hover.png\" align=\"left\" width=30%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closely inspecting the source also shows you this information!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-song-tag.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A simple solution may be to just use the `.find_all()` command from BeautifulSoup, extracting all tags called `span`.\n",
    "\n",
    "__Example__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://music-to-scrape.org/'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "# getting links\n",
    "links = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        extracted_link = link.attrs['href']\n",
    "        links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "\n",
    "# getting songs\n",
    "songs = []\n",
    "for song in relevant_section.find_all(\"span\"):\n",
    "    songs.append(song.get_text())\n",
    "\n",
    "\n",
    "# links for each user\n",
    "print(links)\n",
    "\n",
    "# recent songs for each user\n",
    "print(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this approach seems easily implemented, it is __highly error-prone and needs to be avoided.__ \n",
    "\n",
    "So... what happened?\n",
    "\n",
    "The length for these two objects - `links` and `songs` - differ! Didn't spot it? Then see for yourself!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(links))\n",
    "print(len(songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the links are properly rendered for each user, we can only retrieve song information for a subset of songs. Ultimately, we won't be able to tell WHICH song is part of WHICH user. This is what we call a misalignment of the arrays that hold the necessary data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>What's an array misalignment?</b>\n",
    "    <br>\n",
    "    \n",
    "<ul>\n",
    "<li>\n",
    "When extracting information from the web, we sometimes are prone to \"ripping apart\" the website's original structure by putting data points into individual arrays (e.g., lists such as one list for user names and another for their recently consumed songs). </li>\n",
    "<li>In so doing, we violate the data's original structure: we should store information on users, and <b>each user</b> has a user name/link and song.</li>\n",
    "    <li>The <b>correct way of organizing the data</b> is to create a list of users (e.g., in a dictionary) and then store each attribute (e.g., the song, etc.) <b>within</b> these objects. <b>Only if we store data this way</b> can we be sure to store everything correctly. </li>\n",
    "<br>\n",
    "<li>When we do not adhere to this practice, we run the risk of \"array misalignment\". For example, if only ONE data point were missing for a user, then the (independent) user names array (say, with 6 items) wouldn't be \"1:1 aligned\" with the song array (say, with only 2-5 items).</li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So, how to do it correctly?__\n",
    "\n",
    "Similar to how we first \"zoomed in\" on the recently active user section earlier, we will *first* zoom in on each __user__, and then, *within each user*, extract the required information.\n",
    "\n",
    "Subsequently, we will store the information in a list of dictionaries, where each element of the dictionary corresponds to a user. This data structure will allow us to also omit some of the song names. After all, whether or not a song is listed for users is now exactly tied to a particular usre. \n",
    "\n",
    "__See the example below.__ Pay attention to how we capture the \"unavailability\" of a song name with a `try` and `except` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = 'https://music-to-scrape.org/'\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response\n",
    "res = requests.get(url, headers=user_agent)\n",
    "\n",
    "# Set the encoding of the response to the apparent encoding\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# Find the HTML section with the attribute 'name' equal to 'recent_users'\n",
    "relevant_section = soup.find('section', attrs={'name': 'recent_users'})\n",
    "\n",
    "# Identify individual users within the relevant section\n",
    "users = relevant_section.find_all(class_='mobile-user-margin')\n",
    "\n",
    "# Initialize a list to store user data\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user in the list of users\n",
    "for user in users:\n",
    "    # Check if the user has an 'href' attribute within an anchor tag\n",
    "    if 'href' in user.find('a').attrs:\n",
    "        # Extract the link from the 'href' attribute\n",
    "        extracted_link = user.find('a').attrs['href']\n",
    "    \n",
    "    # Check if the user has a 'span' element\n",
    "    if user.find('span') is not None:\n",
    "        # Get the text content of the 'span' element, which represents song names\n",
    "        song_name = user.find('span').get_text()\n",
    "    else:\n",
    "        # If there is no 'span' element, set the song_name to 'NA'\n",
    "        song_name = 'NA'\n",
    "    \n",
    "    # Create a dictionary object with the extracted data\n",
    "    obj = {'url': extracted_link, 'song_name': song_name}\n",
    "    \n",
    "    # Append the dictionary to the user_data list\n",
    "    user_data.append(obj)\n",
    "\n",
    "# user_data now contains a list of dictionaries, each representing user information with a URL and song name\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Handling Errors with <code>try</code> and <code>except</code> in Python</b>\n",
    "    <br>\n",
    "    \n",
    "- In Python, we have a useful way to deal with potential errors or exceptions in our code. We use a construct called a <code>try</code> and <code>except</code> clause.\n",
    "  - The <code>try</code> block is where you place the code that might potentially cause an error. For example, if you're trying to find an element on a website, you can put this code inside the <code>try</code> block.\n",
    "  - If the code inside the <code>try</code> block encounters an error, instead of crashing your program, Python will jump to the <code>except</code> block. This is incredibly useful for handling situations where, for instance, the element you're trying to find on a website isn't available.\n",
    "  - Inside the <code>except</code> block, you can define what action to take when an error occurs. In our example, you could set the missing data point to \"NA\" so that you know it wasn't available.\n",
    "- However, it's crucial to use the <code>try</code> and <code>except</code> construct sparingly. You don't want to skip the entire process for a user just because one data point isn't available. Instead, use it selectively to handle specific errors and ensure your program continues running smoothly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Navigating and Extracting User Profile Data  \n",
    "\n",
    "__Importance__  \n",
    "\n",
    "So far, we’ve learned how to extract seeds (users) from a single page—the homepage.  \n",
    "\n",
    "What’s next?  \n",
    "\n",
    "[`music-to-scrape.org`](https://music-to-scrape.org) holds user consumption data across multiple pages (one per week). Our goal is to navigate each user’s profile and save the names of all songs, artists, and timestamps (date/time) by visiting these pages one by one.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open [the website](https://music-to-scrape.org/user?username=StarCoder49&week=36), and click on the \"previous\" button at the top of the page. Do you understand how you will be able to \"loop\" through the site?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-user-page.png\" align=\"left\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extract Consumption Data From User Profile Pages\n",
    "\n",
    "We’ve identified the seeds (usernames) and target pages (weeks 37–0) but haven’t extracted any consumption data yet (e.g., songs a user listened to).  \n",
    "\n",
    "Use what you’ve learned (e.g., from *Web Scraping for Dummies*) to iterate through the table and collect this data.  \n",
    "\n",
    "__Try it out__  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s helpful to prototype before assembling a full working script.  \n",
    "\n",
    "Let’s start by downloading the first page of a user and storing it in a variable called `soup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=6'\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try a few commands to access information on the site. Of course, the browser inspect tool is important to have opened on the side. You probably notice that the table is quite easy to capture - it has it's own tag, called `table`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-table.png\" align=\"left\" width=90%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? This one worked quite well! Inspecting the table a bit more, you can get at the individual rows using the `tr` tag. Again, use your browser's inspect tool to spot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.find('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the first row. Using `.find_all()`, instead, will give you a list of all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = table.find_all('tr')\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check whether the number of rows is equal to what we would expect from looking at the website. Using the `len` function for this yields..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks about right? Yes! So, let's now try to extract, for one row, the name of the song and artist, corresponding to the first and second column of the table.\n",
    "\n",
    "Let's first select one row for prototyping. We take row 2 (which is the first row after the table header)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row = rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row.find_all('td')[0].get_text() # for song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row.find_all('td')[1].get_text() # for artist name, corresponding to the second \"column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now put everything together in one script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=6'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    #print(row)\n",
    "    data = row.find_all('td')\n",
    "    \n",
    "    if len(data)>0:\n",
    "        song_name=data[0].get_text()\n",
    "        artist_name=data[1].get_text()\n",
    "        \n",
    "        print(f'Song \"{song_name}\" by \"{artist_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.1__\n",
    "\n",
    "1. Rather than printing the data to the screen, store it in a list of dictionaries, containing the following data points:\n",
    "    - song\n",
    "    - artist\n",
    "    - date\n",
    "    - username\n",
    "    - and time of data extraction.\n",
    "2. Wrap your code in a function, that returns the JSON dictionary from 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q1:\n",
    "import time\n",
    "\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=36'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "json_data=[]\n",
    "\n",
    "for row in rows:\n",
    "    data = row.find_all('td')\n",
    "\n",
    "    if len(data)>0:\n",
    "        song_name=data[0].get_text()\n",
    "        artist_name=data[1].get_text()\n",
    "        date=data[2].get_text()\n",
    "        timestamp=data[3].get_text()\n",
    "        json_data.append({'song_name': song_name,\n",
    "                          'artist_name': artist_name,\n",
    "                          'date': date,\n",
    "                          'time': timestamp,\n",
    "                          'timestamp_of_extraction': int(time.time()),\n",
    "                          'username': url.split('=')[1]})\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "\n",
    "def get_consumption_history(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    json_data=[]\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "    \n",
    "        if len(data)>0:\n",
    "            song_name=data[0].get_text()\n",
    "            artist_name=data[1].get_text()\n",
    "            date=data[2].get_text()\n",
    "            timestamp=data[3].get_text()\n",
    "            json_data.append({'song_name': song_name,\n",
    "                              'artist_name': artist_name,\n",
    "                              'date': date,\n",
    "                              'time': timestamp,\n",
    "                              'timestamp_of_extraction': int(time.time()),\n",
    "                              'username': url.split('=')[1]})\n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try running the function\n",
    "get_consumption_history('https://music-to-scrape.org/user?username=StarCoder49&week=6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check whether it also works for different weeks\n",
    "get_consumption_history('https://music-to-scrape.org/user?username=StarCoder49&week=4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loop through all weeks for each user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "Alright - what have we achieve so far?\n",
    "\n",
    "- In section 1, we've built a function to retrieve user names of currently active users. We call this the stage of our project in which we collect \"seeds\".\n",
    "- In section 2.1, we've managed to extract a user's consumption history from a table displayed on the user's profile page.\n",
    "\n",
    "What's missing, though, is __ALL of a user's consumption data__, i.e., from __ALL possible weeks__.\n",
    "\n",
    "For this, we're making use of the \"previous page\" button.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mits-previous-button.png\" align=\"left\" width=30%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open the user's profile page at https://music-to-scrape.org/user?username=StarCoder49. __Click on the previous button__ a few times, and observe how the URL in your browser bar is changing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=37`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=36`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=35`\n",
    "- ...\n",
    "\n",
    "Can you guess the next one...?\n",
    "\n",
    "A general solution is to look up whether there is a `previous` button on the page (see HTML code below). We can then either \"grab\" the URL and visit it, or - instead - \"click\" on the button.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-previous-page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's write a snippet that \"captures\" the link of the previous page button! We always proceed in small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the website's source code and convert to BeautifulSoup object\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Trying to locate the previous button, using a combination of class names and attribute-value pairs.\n",
    "soup.find(class_='page-link', attrs={'type':'previous_page'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Trying to extract the `href` attribute\n",
    "soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Storing \"previous page\" link\n",
    "previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "previous_page_link # print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we can observe how we're getting closer to the information we need.\n",
    "\n",
    "Now, we only need to combine the base URL (`https://music-to-scrape.org/`) with the page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "f'https://music-to-scrape.org/{previous_page_link}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.2__\n",
    "\n",
    "Please first load the snippet below, which has wrapped the \"previous page\" capturing in a function. Observe the use of `try` and `except`, which accounts for the last page NOT having a next page button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_page(soup):\n",
    "    try:\n",
    "        previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "        return(f'https://music-to-scrape.org/{previous_page_link}')\n",
    "    except:\n",
    "        return('no previous page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out this function on the source code of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get('https://music-to-scrape.org/user?username=StarCoder49').text)\n",
    "previous_page(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, it worked! Now, proceed with the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Make a web requests to 'https://music-to-scrape.org/user?username=StarCoder49&week=36', and pass on the (souped) object to the `previous_page()` function and observe the output. Then, use 'https://music-to-scrape.org/user?username=StarCoder49&week=0'. Is that what you expected? \n",
    "\n",
    "2. Write a while loop that continuously visits all pages for the user `StarCoder49`, by extracting previous page URLs from each page and continuing the data collection until there is no previous page to fetch. Start with week 10 to minimize server load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "soup = BeautifulSoup(requests.get('https://music-to-scrape.org/user?username=StarCoder49&week=6').text)\n",
    "previous_page(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get('https://music-to-scrape.org/user?username=StarCoder49&week=0').text)\n",
    "previous_page(soup)\n",
    "# returns \"no previous page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "urls = []\n",
    "\n",
    "# define first URL to start from\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=6'\n",
    "\n",
    "while True:\n",
    "    print(f'Opening {url} and checking for next page...')\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "    previous_url = previous_page(soup)\n",
    "    if 'no previous page' in previous_url: break\n",
    "    url = previous_url\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "So... seems like we're almost there!\n",
    "\n",
    "The only thing that's missing is to actually also extract the song consumption data from each of the user profile pages.\n",
    "\n",
    "We turn towards this issue next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving Extraction Design  \n",
    "\n",
    "### 3.1 Timers  \n",
    "\n",
    "__Importance__  \n",
    "\n",
    "Notice the use of `time.sleep` earlier? Sending too many requests at once can overload a server and get your IP blocked. Pausing between requests is essential to avoid this.  \n",
    "\n",
    "__Try it out__  \n",
    "\n",
    "In Python, use the `time` module to pause execution. For example, after `time.sleep(2)`, the print statement runs only after a 2-second delay:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "import time\n",
    "pause = 2\n",
    "time.sleep(pause)\n",
    "print(f\"I'll be printed to the console after {pause} seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1__\n",
    "\n",
    "Modify the code above to sleep for 2 minutes. Go grab a coffee in-between. Did it take you longer than 2 minutes?\n",
    "\n",
    "(if you want to abort the running code, just select the cell and push the \"stop\" button!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2*60)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modularization  \n",
    "\n",
    "**Importance**  \n",
    "\n",
    "In scraping, many tasks must be repeated—like extracting all book links each time we open a new user page on *music-to-scrape.org*.  \n",
    "\n",
    "To make this easier, we’ll modularize our code into functions. This improves readability, reusability, and allows us to call the same code whenever needed. Need a refresher? Please revisit the [Python Bootcamp](https://odcm.hannesdatta.com/docs/modules/week1/pythonbootcamp/).  \n",
    "\n",
    "**Try it out**  \n",
    "\n",
    "Let’s complete our scraper by combining everything we’ve learned.  \n",
    "\n",
    "Re-run the `get_users` function from Exercise 1.2 (3), or the cell below (which is a copy from above). Then, continue with the exercises.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_users():\n",
    "    url = 'https://music-to-scrape.org/'\n",
    "  \n",
    "    res = requests.get(url)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "    links = []\n",
    "    for link in relevant_section.find_all(\"a\"):\n",
    "        if 'href' in link.attrs: \n",
    "            extracted_link = link.attrs['href']\n",
    "            links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "    return(links) # to return all links\n",
    "\n",
    "get_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2__\n",
    "\n",
    "Execute the function `get_users()` for a few minutes to collect a list of usernames. Store the user names in a JSON file (new-line separated), along with the timestamp of data retrieval `int(time.time())`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "duration = 15 # for testing, just 15 seconds\n",
    "\n",
    "# Calculate the end time\n",
    "end_time = time.time() + duration\n",
    "\n",
    "f = open('seeds.json','w') # start a new file with seeds, so, use `w` (write new file) instead of `a` (append to existing file)\n",
    "\n",
    "# Run the loop until the current time reaches the end time\n",
    "while time.time() < end_time:\n",
    "    print(f'Scraping user names...')\n",
    "    for user in get_users():\n",
    "        new_user = {'url': user,\n",
    "                    'timestamp': int(time.time())}\n",
    "        f.write(json.dumps(new_user)+'\\n')\n",
    "    time.sleep(2)  # Sleep for a few seconds between each execution\n",
    "f.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify whether you can open the data\n",
    "\n",
    "import json\n",
    "f = open('seeds.json','r',encoding = 'utf-8')\n",
    "data = f.readlines()\n",
    "for item in data:\n",
    "    print(json.loads(item))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.3__\n",
    "\n",
    "Now, let's write some code that loads `seeds.json`, and visit each user's __first profile page__ to extract consumption data. Remember to build in a little timer (e.g., waiting for 2 seconds or so). The prototype/starting code below stops automatically after 5 iterations to minimize server load. Try removing the prototyping condition using the comment character `#` when you think you're done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the code below\n",
    "\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import json\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    print(obj['url'])\n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(2)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tips</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Use the function <code>get_consumption_history(url)</code> from exercise 2.3 above!\n",
    "        </li>\n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import json\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    url = obj['url']\n",
    "\n",
    "    print(f'Extracting information for {url}...')\n",
    "    \n",
    "    output_file = open('output_data.json','a')\n",
    "\n",
    "    songs = get_consumption_history(url)\n",
    "\n",
    "    for song in songs:\n",
    "        output_file.write(json.dumps(song))\n",
    "        output_file.write('\\n')\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip: Understanding the Difference Between <code>'a'</code> and <code>'w'</code> When Writing Files in Python</b>\n",
    "    <br>\n",
    "    \n",
    "- When working with files in Python, it's essential to know the difference between <code>'a'</code> and <code>'w'</code>  when opening them.\n",
    "- <code>'a'</code> stands for \"append\" mode. When you open a file with <code>'a'</code> , Python will let you add data to the end of the existing file without erasing its contents. This is useful when you want to add new information to a file without losing what's already there. It's like adding new lines to the end of an ongoing document.\n",
    "- <code>'w'</code>  stands for \"write\" mode. When you open a file with <code>'w'</code> , Python will create a new file or overwrite an existing one. This means that if the file already has data in it, using <code>'w'</code>  will erase all the existing content and start fresh. It's like creating a new document or wiping out the old one.\n",
    "- Remember, when scraping data or working with files, it's generally safer to use <code>'a'</code>. This way, you won't accidentally delete valuable data. Using <code>'w'</code>  should be done with caution, and only when you intentionally want to start with a clean slate or create a new file altogether.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can re-open the extracted data in Python to see whether what we retrieved seems complete.\n",
    "\n",
    "Verify you've the `pandas` package installed by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect data in pandas\n",
    "import pandas as pd\n",
    "pd.read_json('output_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summary\n",
    "\n",
    "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Limitations of BeautifulSoup and the Advantages of Selenium</b>\n",
    "    <br>\n",
    "    \n",
    "- While BeautifulSoup is a powerful tool for parsing and navigating HTML documents, it has some limitations when it comes to interacting with websites:\n",
    "  - BeautifulSoup is a static parser, meaning it can't interact with dynamic web content that loads or changes after the initial page load. This makes it less suitable for websites that heavily rely on, say, JavaScript to update their content. For example, this is relevant for Twitter or Instagram.\n",
    "  - BeautifulSoup can't handle user interactions such as clicking buttons, filling out forms, or navigating through complex web applications.\n",
    "- When you need to scrape data from very modern and interactive websites, consider using a tool like Selenium. Selenium is a web automation framework that allows you to control a web browser programmatically.\n",
    "  - With Selenium, you can automate interactions with websites, simulate user actions, and retrieve data from pages that rely heavily on JavaScript.\n",
    "  - It's an excellent choice for scraping data from dynamic websites, conducting web testing, and performing tasks that require a more interactive approach.\n",
    "- Keep in mind that while BeautifulSoup is great for many scraping tasks, knowing when to use Selenium can open up new possibilities and make your web scraping efforts more effective.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A Primer on Scraping Advanced, Dynamic Websites with Selenium  \n",
    "\n",
    "So far, you’ve used the `requests` library to retrieve web data. While this works for simpler sites, it often fails on modern, dynamic websites like Twitch, Twitter, or Instagram, where content is loaded dynamically through JavaScript.  \n",
    "\n",
    "A powerful solution is to use the `selenium` library, which allows you to control a web browser programmatically. With `selenium`, you can simulate user actions such as clicking buttons, scrolling, or filling out forms—making it possible to access dynamic content that `requests` can't handle.  \n",
    "\n",
    "In this section, we’ll focus on how to *open* and *navigate* websites using `selenium`. Once you’re on the site and the content has fully loaded, you can continue using `BeautifulSoup` to parse and extract the data you need. \n",
    "\n",
    "This combination gives you the best of both worlds: `selenium` for interaction and `BeautifulSoup` for efficient data extraction.  \n",
    "\n",
    "__Let’s get started__  \n",
    "\n",
    "We’ll begin with setting up `selenium` and writing a simple script to open a website and retrieve some data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Making a connection to a website using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Installing Selenium and Chromedriver</b> \n",
    "\n",
    "To install Selenium and Chromedriver locally, please follow the <a href=\"https://tilburgsciencehub.com/configure/python-for-scraping/?utm_campaign=referral-short\">Tutorial on Tilburg Science Hub</a>.\n",
    "    \n",
    "You can also use the code snippet below to automate the installation. Running this snippet takes a little longer each time, but the benefit is that it almost always works!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install webdriver_manager --upgrade\n",
    "!pip install selenium --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using selenium 4 - ensure you have Chrome installed (and wait a bit for Chrome to show up!)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "url = \"https://music-to-scrape.org/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smooth, your computer opened a new Chrome window, and opened `music-to-scrape.org`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Using Google Colab</b> \n",
    "\n",
    "If you're using Google Colab, you don't see your browser open up manually.\n",
    "    \n",
    "Whenever you switch pages, just manually open that page in your browser. Although this feels like a little less interactive, you will still be able to work through this tutorial!\n",
    "\n",
    "</div>\n",
    "\n",
    "From now onwards, you can use `driver.get('https://google.com')` to point to different websites (i.e., you don't need to install it over and over again, unless you open up a new instance of Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using BeautifulSoup with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now also try to extract information. Note that we're converting the source code of the site to a `BeautifulSoup` object (because you may have learnt how to use `BeautifulSoup` earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the time package to wait a few seconds until the page is loaded\n",
    "import time\n",
    "url = \"https://music-to-scrape.org/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using the \"source code\" obtained with the `requests` library, we can now convert the source code of the Selenium website to a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and start experimenting with querying the site (such as retrieving the text of all cards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cards = soup.find_all(class_='card-body')\n",
    "\n",
    "# print \n",
    "counter = 0\n",
    "for card in cards:\n",
    "    counter = counter + 1\n",
    "    print('Card ' + str(counter) + ': ' + card.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Clicking and Scrolling with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "For more dynamic websites, we may have to click on certain elements (rather than extracting some URL).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Extracting elements using Selenium, not BeautifulSoup</b> \n",
    "\n",
    "Selenium is really great for navigating dynamic website. There are two ways in which you can use it for querying sites:\n",
    "    \n",
    "<ul>\n",
    "    <li>put the \"selenium\" source code (<code>driver.page_source</code>) to BeautifulSoup, and then use BeautifulSoup commands, or </li>\n",
    "    <li>directly use selenium (and it's own query language) to extract elements.</li>\n",
    "</ul>\n",
    "    \n",
    "In the next few examples, we are using selenium's \"internal\" query language (which you identify easily because it is a subfunction of the `driver` object, and because it has a different name (`find_element`, instead of `find` or `find_all`).\n",
    "    \n",
    "Want to know more about selenium's built-in query language? Check out the \"Advanced Web Scraping Tutorial\", or dig up some extra material from the web. Knowing both BeautifulSoup and Selenium makes you most productive!\n",
    "  \n",
    "</div>\n",
    "\n",
    "__Try it out__\n",
    "\n",
    "If you haven't done so, rerun the installation code for `selenium` from above. Then, proceed by running the following cell and observe what happens in your browser:\n",
    "\n",
    "1. **Click a button** to accept the cookie banner.\n",
    "2. **Scroll down** the page to reveal hidden elements.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://music-to-scrape.org/\"\n",
    "driver.get(url)\n",
    "time.sleep(3) # wait for 3 seconds\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "try:\n",
    "    cookie_button = driver.find_element(By.ID, \"accept-cookies\")\n",
    "    cookie_button.click()\n",
    "except:\n",
    "    print('No cookie button found (anymore)!')\n",
    "    \n",
    "    \n",
    "# Scroll down the page\n",
    "scroll_pause_time = 2\n",
    "for _ in range(3):  # Scroll down 3 times\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(scroll_pause_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clicking and scrolling are essential steps when working with dynamic websites. With `Selenium`, you can use `driver.find_element()` to interact with various elements, such as buttons and scrollable sections. These interactions are often necessary to fully load content that would otherwise remain hidden. Unlike `requests`, `Selenium` gives you full control over the webpage, but it comes with trade-offs—it’s slower and can sometimes be buggy, especially on headless systems.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Exercise 4.1__\n",
    "\n",
    "Please write code snippets to extract the following pieces of information. Do you choose `requests` or `selenium`?\n",
    "\n",
    "1. The titles of all `<h2>` tags from `https://odcm.hannesdatta.com/docs/course/`\n",
    "2. The titles of all available TV series from `https://www.bol.com/nl/nl/l/series/3133/30291/` (about 24)\n",
    "\n",
    "```\n",
    "soup.find_all('a', class_='product-title')\n",
    "```\n",
    "\n",
    "\n",
    "We also need the time package to wait a few seconds until the page is loaded.\n",
    "\n",
    "```\n",
    "import time\n",
    "url = \"https://twitch.tv/\" # some example URL\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to question 1:\n",
    "header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "request = requests.get('https://odcm.hannesdatta.com/docs/course/', headers = header)\n",
    "request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "soup = BeautifulSoup(request.text)\n",
    "for title in soup.find_all('h2'): print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to question 2:\n",
    "driver.get('https://www.bol.com/nl/nl/l/series/3133/30291/')\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "urls = []\n",
    "for url in soup.find_all('a', class_='product-title'):\n",
    "    urls.append(url.attrs['href'])\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Wrapping Up__  \n",
    "Wow – you’ve just learned another way to open and interact with websites using `Selenium`. This powerful tool allows you to work with highly dynamic sites and helps you avoid blocks that sometimes happen with `requests`. However, keep in mind that `Selenium` is slower and less efficient for scaling up large data collections.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you want to go deeper, here are some excellent resources for additional study:  \n",
    "- **Selenium Documentation**: [https://www.selenium.dev/documentation/](https://www.selenium.dev/documentation/)  \n",
    "- **Selenium with Python**: [https://selenium-python.readthedocs.io/](https://selenium-python.readthedocs.io/)  \n",
    "- **Stack Overflow** for troubleshooting common issues.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Awesome stuff with Selenium</b> \n",
    "\n",
    "Selenium is your best shot at navigating a dynamic website. It can do amazing things, such as \n",
    "    \n",
    "<ul>\n",
    "    <li>\"clicking\" on buttons</li>\n",
    "    <li>scrolling through a site</li>\n",
    "    <li>hovering over items and capturing information from popups,</li>\n",
    "    <li>starting to play a stream,</li>\n",
    "    <li>typing text and submitting it in the chat, and</li>\n",
    "    <li>so much more...!</li>\n",
    "</ul>\n",
    "    \n",
    "Note though that we won't cover the advanced functionality of Selenium in this tutorial, but the optional \"Web data advanced\" tutorial holds the necessary information.\n",
    "   \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After-class exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Can you extend the code written in 3.2 to extract data from ALL of a user's profile pages?\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Please port your data collection into two Python scripts. One called `collect_seeds.py` that collects seeds for 5 minutes. You can use a task scheduler to launch this task every 15 minutes and keep it running for a few hours.\n",
    "\n",
    "Building on exercise 1 above, write a second script, called `collect_user_data.py`, which you run once (after you've finalized collecting seeds). This script collects all of the required data for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first modify the `get_consumption_history()` function, ensuring it shows us whether there is a `previous page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "def get_consumption_history(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    json_data=[]\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "    \n",
    "        if len(data)>0:\n",
    "            song_name=data[0].get_text()\n",
    "            artist_name=data[1].get_text()\n",
    "            date=data[2].get_text()\n",
    "            timestamp=data[3].get_text()\n",
    "            json_data.append({'song_name': song_name,\n",
    "                              'artist_name': artist_name,\n",
    "                              'date': date,\n",
    "                              'time': timestamp,\n",
    "                              'timestamp_of_extraction': int(time.time()),\n",
    "                              'username': url.split('=')[1]})\n",
    "\n",
    "    url_of_previous_page = previous_page(soup)\n",
    "        \n",
    "    return({'songs': json_data, 'previous_page': url_of_previous_page})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import json\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    url = obj['url']\n",
    "\n",
    "    while 'no previous page' not in url:\n",
    "        print(f'Extracting information for {url}...')\n",
    "    \n",
    "        output_file = open('output_data.json','a')\n",
    "    \n",
    "        songs = get_consumption_history(url)\n",
    "        \n",
    "        for song in songs['songs']:\n",
    "            output_file.write(json.dumps(song))\n",
    "            output_file.write('\\n')\n",
    "        output_file.close()\n",
    "        \n",
    "        url = songs['previous_page']\n",
    "        time.sleep(2)\n",
    "    \n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
