{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101\n",
    "\n",
    "*After finishing this tutorial, you can extract data from multiple pages on the web and export such data to JSON and CSV files to use in an analysis. Plan a few hours to work through this notebook. Taking a few breaks in between keeps you sharp!*\n",
    "\n",
    "*Just starting with web scraping? Then make sure to have followed the [\"webdata for dummies\" tutorial](https://odcm.hannesdatta.com/docs/modules/week2/webdata-for-dummies/) first.*\n",
    "\n",
    "*Enjoy!*\n",
    "\n",
    "--- \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Our main goal is to compile a panel data set of music consumption data for (simulated) users of music-to-scrape.org, a platform developed for practicing web scraping skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Identifying a strategy to generating seeds (“sampling”)\n",
    "    * Extracting multiple elements at once using the `.find_all()` function\n",
    "    * Preventing array misalignment\n",
    "* Navigating on a website \n",
    "    * Using URLs to programmatically visit web pages\n",
    "    * Writing loops to execute data collections in bulk using functions\n",
    "* Improving extraction design\n",
    "    * Implementing timers and modularizing extraction code\n",
    "    * Storing data in CSV or JSON files with relevant meta data\n",
    "* Scraping more advanced, dynamic websites\n",
    "    * Understanding the difference between headless requests and browser emulation \n",
    "    * Learn when to apply one of the two methods (using `requests` and `selenium`)\n",
    "\n",
    "--- \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Support Needed?</b> \n",
    "    For technical issues outside of scheduled classes, please check the <a href=\"https://odcm.hannesdatta.com/docs/course/support\" target=\"_blank\">support section</a> on the course website.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating seeds (\"sampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "So far, we've extracted (=parsed) some information (e.g., names of featured artists) from an artist's individual *artist page*. What we haven't done yet is to take a closer look at the consumption of individual users.\n",
    "\n",
    "In fact, individual users are often a focal point of attention in web scraping. For example, we can sample users' tweets on Twitter/X, or users' movie watching behavior on trakt.tv. \n",
    "\n",
    "Yet, before we can start building what is called a \"panel data set\" (i.e., multiple users, observed over multiple time periods), we need to decide for __which users to obtain information__. Ideally, we would like to capture information for a *sample of users* (or books, movies, series, games - depending on the platform.).\n",
    "\n",
    "In web scraping, we typically refer to a \"seed\" as a starting point for a data collection. Without a seed, there's no data to collect.\n",
    "\n",
    "For example, before we can crawl through all users available at [music-to-scrape.org](https://music-to-scrape.org), we first need to generate a *list of many users of the platform*. (Note that obtaining the user names of ALL users of the site is barely possible).\n",
    "\n",
    "One way to get there would be to:\n",
    "\n",
    "1. first visit the main homepage of [music-to-scrape.org](https://music-to-scrape.org), showing a few recently active users at the time, and\n",
    "2. visit a users' profile page and start scraping their consumption data (or anything else on that page; we have done this in the webdata for dummies tutorial). \n",
    "\n",
    "Note that the homepage allows us to \"navigate\" to the users' profile pages, such as by clicking on the user name or the avatar (see red boxes in the figure below). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-users.png\" align=\"left\" width=80%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Collecting links to use as seeds\n",
    "\n",
    "Let's take a look at how the links for users' profile pages are written in the website's source code.\n",
    "\n",
    "Open the [website](https://music-to-scrape.org), and inspect the underlying HTML code with the Chrome or Firefox Inspector (right click --> inspect element). Hover around with your mouse a bit, and then select one of the user avatars. \n",
    "\n",
    "Do you see in the source code that each user contains a clickable link (`<a>`), containing the link (`href`) to the user's profile page? \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-inspect-link.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, how could we tell a computer to capture the links to the various user pages?\n",
    "\n",
    "One simple way is to select *elements by their tags*. For example, to extract all links (`<a>` tags). \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>How to extract multiple elements at once?</b>\n",
    "    <br>\n",
    "    \n",
    "- By working through other tutorials, you may already be familiar with the <code>.find()</code> function of BeautifulSoup. The <code>.find()</code> function returns the <b>first element</b> that matches your particular \"search query\". <br>\n",
    "- If you want to extract <b>all elements</b> that match a particular search pattern (say, a class name), you can use BeautifulSoup's <code>.find_all()</code> function.<br>\n",
    "- Note that the \"result\" of the <code>.find_all()</code> option is a list of results __that you need to iterate through.__\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1__\n",
    "\n",
    "Please run the code cell below, which extracts all links (the `a` tag!), and prints the URL (`href`) to the screen. Don't worry, you don't need need to understand the code yet, we'll go over it line by line shortly!\n",
    "\n",
    "If you look at these links more closely, you'll notice that we're not interested in many of these links... \n",
    "\n",
    "Make a list of all links we're *not* interested in (i.e., those *not* pointing to a user page). Which ones are those? Can you find out why they are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/privacy_terms\n",
      "/privacy_terms\n",
      "about\n",
      "/\n",
      "/\n",
      "/tutorial_scraping\n",
      "/tutorial_api\n",
      "#\n",
      "https://api.music-to-scrape.org/docs\n",
      "/about\n",
      "song?song-id=SOAXZLM12A6D4F7A95\n",
      "song?song-id=SOVUNDW12A58A7AF38\n",
      "song?song-id=SOGYYUL12A6D4FB2CB\n",
      "song?song-id=SOOYBBU12A6D4F9AF3\n",
      "song?song-id=SOCSZBL12CF530E5B0\n",
      "song?song-id=SOKHQEF12AB0183CF6\n",
      "song?song-id=SOPWVGR12A8AE46957\n",
      "song?song-id=SOLGUGY12AB01897BE\n",
      "song?song-id=SOTKTCQ12AB01863FF\n",
      "song?song-id=SOJURPV12A8C141B82\n",
      "song?song-id=SOJZTXJ12AB01845FB\n",
      "song?song-id=SOZKQWY12A6D4FA5E6\n",
      "song?song-id=SOGXHEG12AB018653E\n",
      "song?song-id=SOQVMXR12A81C21483\n",
      "song?song-id=SODRPTE12A58A7BE10\n",
      "song?song-id=SOBVAPJ12AB018739D\n",
      "song?song-id=SOECLAD12AAF3B120A\n",
      "song?song-id=SORJVDO12AF72A1970\n",
      "song?song-id=SOLIQRN12A8C1391A6\n",
      "song?song-id=SOAYRZU12A8C133EBD\n",
      "song?song-id=SOQVVDQ12AB018300C\n",
      "song?song-id=SOQQVIP12A8C13E7E6\n",
      "song?song-id=SOCDLSK12AB018168E\n",
      "song?song-id=SONLBRH12A6D4FBAEE\n",
      "song?song-id=SOMCTKM12A8C138B86\n",
      "artist?artist-id=ARJ66JQ1187B99D2FF\n",
      "artist?artist-id=ARIN12F1187FB3E92C\n",
      "artist?artist-id=ARYFAT91187B99FEF5\n",
      "artist?artist-id=AR5OH2Z1187B9A46A9\n",
      "artist?artist-id=ARQFPFK1187FB533E1\n",
      "artist?artist-id=ARICCN811C8A41750F\n",
      "artist?artist-id=ARSWORN1187B991A7B\n",
      "artist?artist-id=ARR2NH51187B98CE4C\n",
      "user?username=CyberStar49\n",
      "user?username=Wizard79\n",
      "user?username=WizardShadow42\n",
      "user?username=Panda89\n",
      "user?username=SonicNinja25\n",
      "user?username=ShadowPixel58\n",
      "/tutorial_scraping\n",
      "/tutorial_api\n",
      "https://api.music-to-scrape.org/docs\n",
      "/about\n",
      "/privacy_terms\n",
      "https://www.linkedin.com/company/tilburgsciencehub\n",
      "https://github.com/tilburgsciencehub/music-to-scrape\n",
      "https://twitter.com/tilburgscience\n"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make a get request to the books overview page (see Webdata for Dummies tutorial)\n",
    "user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "url = 'https://music-to-scrape.org'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# return the href attribute in the <a> tag nested within the first product class element\n",
    "for link in soup.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        print(link.attrs[\"href\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__\n",
    "\n",
    "The links we want to ignore are...\n",
    "\n",
    "* The links to the about or privacy pages\n",
    "* Any link pointing to the most popular songs or artists\n",
    "* Any social media links, etc.\n",
    "\n",
    "These links are present on the page, because they are used by users to navigate on the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Collecting *More Specific* Links\n",
    "\n",
    "__Importance__\n",
    "\n",
    "We've just discovered that selecting elements by their tags gives us many irrelevant links. But, how can we narrow down these links, or, in other words, __how can we scrape only the users we're interested in?__.\n",
    "\n",
    "To answer this question, we need to briefly revisit the notion of how an HTML code is structured. __Open your browser's inspect tool again and hover over the \"recently active users\" section on the site.__\n",
    "\n",
    "After inspecting, you'd probably notice that the page is generated according to a rigid structure: all user links are contained in a `<section>` tag, with the attribute `name=\"recent_users\"`. The \"wrong links\" extracted above (i.e., to the about or privacy pages) are *not* part of these elements. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-section.png\" align=\"left\" width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, if we can tell our scraper that we're only interested in the `<a>` tags *within the particular `<section>` with attribute `name` equal to `recent_users`, we end up with our desired selection of links. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Like before, we'll use `.find_all()` to capture all matching elements on the page. The difference, however, is that we do not directly try to extract the __links__ with the tag `a`, but first try to select the section containing the relevant links.\n",
    "\n",
    "Run the code below, in which we first try to capture only elements in the section with `name=recent_users`. Then, we collect all `<a>` tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user?username=CyberStar49',\n",
       " 'user?username=Wizard79',\n",
       " 'user?username=WizardShadow42',\n",
       " 'user?username=Panda89',\n",
       " 'user?username=SonicNinja25',\n",
       " 'user?username=ShadowPixel58']"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# make request\n",
    "url = 'https://music-to-scrape.org'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "users = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        users.append(link.attrs['href'])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we retrieve up to six user names. You can now also use the `users` object to look at the data for the first, second, third, ... user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user?username=CyberStar49'"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[0] # returns the link to the user page of the 1st user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to subsequently try to extract the link for the first book..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the user list still contains a lot of \"other\" things, unrelated to the user name. Remember, we extracted the __links__ to the profile pages, not just the user names.\n",
    "\n",
    "If we want to remove anything but the usernames, we can modify our extraction function slightly, for example using Python's `split` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CyberStar49',\n",
       " 'Wizard79',\n",
       " 'WizardShadow42',\n",
       " 'Panda89',\n",
       " 'SonicNinja25',\n",
       " 'ShadowPixel58']"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        users.append(link.attrs['href'].split('=')[1])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need explanation on this code? Just copy-paste it to ChatGPT and ask for an explanation, e.g., using this prompt:\n",
    "\n",
    "> I struggle to understand this piece of Python code in the context of web scraping. \n",
    "> Can you please explain it, paying attention to the complicated last line (user.append())?\n",
    "\n",
    "Pretty cool, right? So let's proceed with some exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "1. Modify the loop (`for link in relevant_section`...) written above to extract the *absolute URLs* rather than the relative URLs. Specifically, combine the website's URL (`https://music-to-scrape.org/`) and the string you extracted earlier (`user?username=GalaxyShadow34`). The final URL needs to be: `https://music-to-scrape.org/user?username=GalaxyShadow34`.\n",
    "\n",
    "2. Wrap your code from (1) in a function, called `get_users()`, returning the links to the user profile pages as an arary. We will use it later to repeatedly collect user names (seeds) from this page. \n",
    "\n",
    "3. Execute your function from 2) in a while loop, that runs every 2 seconds for a duration of 15 seconds. Importantly, write all URLs to a new-line separated JSON file, called `seeds.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=CyberStar49',\n",
       " 'https://music-to-scrape.org/user?username=Wizard79',\n",
       " 'https://music-to-scrape.org/user?username=WizardShadow42',\n",
       " 'https://music-to-scrape.org/user?username=Panda89',\n",
       " 'https://music-to-scrape.org/user?username=SonicNinja25',\n",
       " 'https://music-to-scrape.org/user?username=ShadowPixel58']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1 \n",
    "urls = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        extracted_link = link.attrs['href']\n",
    "        urls.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=CosmicSonic89',\n",
       " 'https://music-to-scrape.org/user?username=CyberStar49',\n",
       " 'https://music-to-scrape.org/user?username=Wizard79',\n",
       " 'https://music-to-scrape.org/user?username=WizardShadow42',\n",
       " 'https://music-to-scrape.org/user?username=Panda89',\n",
       " 'https://music-to-scrape.org/user?username=SonicNinja25']"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_users():\n",
    "    url = 'https://music-to-scrape.org/'\n",
    "  \n",
    "    res = requests.get(url)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    \n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "    links = []\n",
    "    for link in relevant_section.find_all(\"a\"):\n",
    "        if 'href' in link.attrs: \n",
    "            extracted_link = link.attrs['href']\n",
    "            links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "    return(links) # to return all links\n",
    "\n",
    "get_users()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Define the duration in seconds (1 minute = 60 seconds)\n",
    "duration = 15\n",
    "\n",
    "# Calculate the end time\n",
    "end_time = time.time() + duration\n",
    "\n",
    "f = open('seeds.json','a')\n",
    "\n",
    "# Run the loop until the current time reaches the end time\n",
    "while time.time() < end_time:\n",
    "    for user in get_users():\n",
    "        f.write(json.dumps(user)+'\\n')\n",
    "    time.sleep(2)  # Sleep for a few seconds between each execution\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Working with JSON data in Python</b>\n",
    "    <br>\n",
    "    In Python, we often need to work with JSON data, which is a common format for exchanging information. \n",
    "    \n",
    "- To make a string (such as one read from a file) queryable as JSON, we use the <code>json.loads()</code> function.\n",
    "  The <code>json.loads()</code> function takes a JSON-formatted string and converts it into a Python data structure, such as a dictionary or a list, so you can easily access its contents.\n",
    "- If you want to save a Python data structure as a JSON file, you can use the <code>json.dumps()</code> function.\n",
    "        The <code>json.dumps()</code> function takes a Python object, like a dictionary or a list, and converts it into a JSON-formatted string that you can save to a text file for later use.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Preventing array misalignment\n",
    "\n",
    "So far, we have only extracted *one* piece of information (the URL) from the list of recently active users. But, what if we want to also extract the names of recently consumed songs? For example, you can view this song by hovering over the user profile pictures on the landing page.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-hover.png\" align=\"left\" width=30%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closely inspecting the source also shows you this information!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-song-tag.png\" align=\"left\" width=60%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A simple solution may be to just use the `.find_all()` command from BeautifulSoup, extracting all tags called `span`.\n",
    "\n",
    "__Example__:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://music-to-scrape.org/user?username=TechGeek32', 'https://music-to-scrape.org/user?username=CosmicSonic89', 'https://music-to-scrape.org/user?username=CyberStar49', 'https://music-to-scrape.org/user?username=Wizard79', 'https://music-to-scrape.org/user?username=WizardShadow42', 'https://music-to-scrape.org/user?username=Panda89']\n",
      "['White Heart - Hold On', 'Vanessa Daou - Black & White', 'N.E.R.D. - Intro / Time For Some Action', 'Elizabeth Cotten - Freight Train']\n"
     ]
    }
   ],
   "source": [
    "# Run this code now\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://music-to-scrape.org/'\n",
    "\n",
    "res = requests.get(url, headers = user_agent)\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "relevant_section = soup.find('section',attrs={'name':'recent_users'})\n",
    "\n",
    "# getting links\n",
    "links = []\n",
    "for link in relevant_section.find_all(\"a\"):\n",
    "    if 'href' in link.attrs: \n",
    "        extracted_link = link.attrs['href']\n",
    "        links.append(f'https://music-to-scrape.org/{extracted_link}')\n",
    "\n",
    "# getting songs\n",
    "songs = []\n",
    "for song in relevant_section.find_all(\"span\"):\n",
    "    songs.append(song.get_text())\n",
    "\n",
    "\n",
    "# links for each user\n",
    "print(links)\n",
    "\n",
    "# recent songs for each user\n",
    "print(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this approach seems easily implemented, it is __highly error-prone and needs to be avoided.__ \n",
    "\n",
    "So... what happened?\n",
    "\n",
    "The length for these two objects - `links` and `songs` - differ! Didn't spot it? Then see for yourself!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(links))\n",
    "print(len(songs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the links are properly rendered for each user, we can only retrieve song information for a subset of songs. Ultimately, we won't be able to tell WHICH song is part of WHICH user. This is what we call a misalignment of the arrays that hold the necessary data.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>What's an array misalignment?</b>\n",
    "    <br>\n",
    "    \n",
    "<ul>\n",
    "<li>\n",
    "When extracting information from the web, we sometimes are prone to \"ripping apart\" the website's original structure by putting data points into individual arrays (e.g., lists such as one list for user names and another for their recently consumed songs). </li>\n",
    "<li>In so doing, we violate the data's original structure: we should store information on users, and <b>each user</b> has a user name/link and song.</li>\n",
    "    <li>The <b>correct way of organizing the data</b> is to create a list of users (e.g., in a dictionary) and then store each attribute (e.g., the song, etc.) <b>within</b> these objects. <b>Only if we store data this way</b> can we be sure to store everything correctly. </li>\n",
    "<br>\n",
    "<li>When we do not adhere to this practice, we run the risk of \"array misalignment\". For example, if only ONE data point were missing for a user, then the (independent) user names array (say, with 6 items) wouldn't be \"1:1 aligned\" with the song array (say, with only 2-5 items).</li>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__So, how to do it correctly?__\n",
    "\n",
    "Similar to how we first \"zoomed in\" on the recently active user section earlier, we will *first* zoom in on each __user__, and then, *within each user*, extract the required information.\n",
    "\n",
    "Subsequently, we will store the information in a list of dictionaries, where each element of the dictionary corresponds to a user. This data structure will allow us to also omit some of the song names. After all, whether or not a song is listed for users is now exactly tied to a particular usre. \n",
    "\n",
    "__See the example below.__ Pay attention to how we capture the \"unavailability\" of a song name with a `try` and `except` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'user?username=TechGeek32', 'song_name': 'Dictators - Weekend'},\n",
       " {'url': 'user?username=CosmicSonic89',\n",
       "  'song_name': 'Kitty Kallen - If You Smile At The Sun'},\n",
       " {'url': 'user?username=CyberStar49', 'song_name': 'NA'},\n",
       " {'url': 'user?username=Wizard79', 'song_name': 'NA'},\n",
       " {'url': 'user?username=WizardShadow42',\n",
       "  'song_name': 'N.E.R.D. - Intro / Time For Some Action'},\n",
       " {'url': 'user?username=Panda89',\n",
       "  'song_name': 'Elizabeth Cotten - Freight Train'}]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL you want to scrape\n",
    "url = 'https://music-to-scrape.org/'\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response\n",
    "res = requests.get(url, headers=user_agent)\n",
    "\n",
    "# Set the encoding of the response to the apparent encoding\n",
    "res.encoding = res.apparent_encoding\n",
    "\n",
    "# Parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "# Find the HTML section with the attribute 'name' equal to 'recent_users'\n",
    "relevant_section = soup.find('section', attrs={'name': 'recent_users'})\n",
    "\n",
    "# Identify individual users within the relevant section\n",
    "users = relevant_section.find_all(class_='mobile-user-margin')\n",
    "\n",
    "# Initialize a list to store user data\n",
    "user_data = []\n",
    "\n",
    "# Loop through each user in the list of users\n",
    "for user in users:\n",
    "    # Check if the user has an 'href' attribute within an anchor tag\n",
    "    if 'href' in user.find('a').attrs:\n",
    "        # Extract the link from the 'href' attribute\n",
    "        extracted_link = user.find('a').attrs['href']\n",
    "    \n",
    "    # Check if the user has a 'span' element\n",
    "    if user.find('span') is not None:\n",
    "        # Get the text content of the 'span' element, which represents song names\n",
    "        song_name = user.find('span').get_text()\n",
    "    else:\n",
    "        # If there is no 'span' element, set the song_name to 'NA'\n",
    "        song_name = 'NA'\n",
    "    \n",
    "    # Create a dictionary object with the extracted data\n",
    "    obj = {'url': extracted_link, 'song_name': song_name}\n",
    "    \n",
    "    # Append the dictionary to the user_data list\n",
    "    user_data.append(obj)\n",
    "\n",
    "# user_data now contains a list of dictionaries, each representing user information with a URL and song name\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Handling Errors with <code>try</code> and <code>except</code> in Python</b>\n",
    "    <br>\n",
    "    \n",
    "- In Python, we have a useful way to deal with potential errors or exceptions in our code. We use a construct called a <code>try</code> and <code>except</code> clause.\n",
    "  - The <code>try</code> block is where you place the code that might potentially cause an error. For example, if you're trying to find an element on a website, you can put this code inside the <code>try</code> block.\n",
    "  - If the code inside the <code>try</code> block encounters an error, instead of crashing your program, Python will jump to the <code>except</code> block. This is incredibly useful for handling situations where, for instance, the element you're trying to find on a website isn't available.\n",
    "  - Inside the <code>except</code> block, you can define what action to take when an error occurs. In our example, you could set the missing data point to \"NA\" so that you know it wasn't available.\n",
    "- However, it's crucial to use the <code>try</code> and <code>except</code> construct sparingly. You don't want to skip the entire process for a user just because one data point isn't available. Instead, use it selectively to handle specific errors and ensure your program continues running smoothly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Navigating and Extracting Information from User Profile Pages\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Alright - what have we learnt up this point?\n",
    "\n",
    "We've learnt how to extract seeds (here: users) from __one page -- the homepage of the platform.__\n",
    "\n",
    "So... what's missing?\n",
    "\n",
    "Exactly! [`music-to-scrape.org`](https://music-to-scrape.org) contains consumption data on many users. \n",
    "\n",
    "The objective of this section is to navigate through each user's __consumption history__ and save the name of all songs, artists, and corresponding timestamps (time/date). However, it's important to note that this information is __spread across multiple pages__ (one for every week in the data), and we need to visit them one by one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open [the website](https://music-to-scrape.org/user?username=StarCoder49&week=36), and click on the \"previous\" button at the top of the page. Do you understand how you will be able to \"loop\" through the site?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-user-page.png\" align=\"left\" width=90%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Capture Information from User Profile Pages\n",
    "\n",
    "The goal of this section is to extract the information on a users' consumption history from the website.\n",
    "\n",
    "Up to this moment, we have defined which seeds to use (usernames from the homepage), and identified from which pages we would like to extract information (e.g., for weeks 37 through 0). Yet, we haven't yet extracted any of the consumption data from the website (e.g., which song a particular user has listened to in a given week.\n",
    "\n",
    "For this, we use our previous learnings (e.g., see \"Web scraping for Dummies\" tutorial in this course) to iterate through the table.\n",
    "\n",
    "__Try it out__\n",
    "\n",
    "View the code snippet below, which *prints* the information on what songs were listened to by a user to the screen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to start doing this in a prototype first, before assembling everything in a \"working script\". So, let's start.\n",
    "\n",
    "First, let us download the first page of a user, and store it in a variable called `soup`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=36'\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try a few commands to access information on the site. Of course, the browser inspect tool is important to have opened on the side. You probably notice that the table is quite easy to capture - it has it's own tag, called `table`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-table.png\" align=\"left\" width=90%/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"table table-striped\">\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Song Title</th>\n",
       "<th>Artist</th>\n",
       "<th>Date</th>\n",
       "<th>Time</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Too Much Saturn</td>\n",
       "<td>Francis Dunnery</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:49:31</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Sample Track 11</td>\n",
       "<td>Simon Harris</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:46:24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Stand (Album Version)</td>\n",
       "<td>Kiss</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:41:33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>High Tide</td>\n",
       "<td>Richard Souther</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:37:44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Sense</td>\n",
       "<td>Beherit</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:27:19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Outro</td>\n",
       "<td>Attack Attack</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:25:55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>I don't Want to Be Alone Tonight</td>\n",
       "<td>Jauida</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:21:25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>E. Warren</td>\n",
       "<td>DJ Omega</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:17:04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Banana Man</td>\n",
       "<td>C.J. Chenier</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:13:55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Happiness Stan - Original</td>\n",
       "<td>Small Facers</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>09:54:54</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Phorever People</td>\n",
       "<td>Shamen</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>09:50:01</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Signs Of Insanity</td>\n",
       "<td>Headhunter</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>09:45:24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Doctrines</td>\n",
       "<td>Arthur Fiedler</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:48:14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>It Hurts Me Too</td>\n",
       "<td>Jeremy Spencer</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:43:24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>A B***** Is A B***** (Edited)</td>\n",
       "<td>N.W.A.</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:40:36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Real Love</td>\n",
       "<td>The Smashing Pumpkins</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:36:26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Say What!?!</td>\n",
       "<td>Chris Standring</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:31:57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Revelations</td>\n",
       "<td>Marco Beltrami</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:28:28</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Jos Defiance</td>\n",
       "<td>Don Francisco</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:24:01</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Quickstep</td>\n",
       "<td>FSTZ</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:19:44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Shape Of My Heart</td>\n",
       "<td>Backstreet Boys</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:16:50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Class Act</td>\n",
       "<td>James Hunter</td>\n",
       "<td>2023-09-07</td>\n",
       "<td>18:13:50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Secrets</td>\n",
       "<td>Sunscreem</td>\n",
       "<td>2023-09-06</td>\n",
       "<td>05:43:38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>She's My Woman</td>\n",
       "<td>Crusaders</td>\n",
       "<td>2023-09-06</td>\n",
       "<td>05:41:41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Anything For My Baby</td>\n",
       "<td>Kiss</td>\n",
       "<td>2023-09-06</td>\n",
       "<td>05:39:06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Apogee (ft. TechTonic)</td>\n",
       "<td>Paul Taylor</td>\n",
       "<td>2023-09-06</td>\n",
       "<td>05:31:33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Le poulet</td>\n",
       "<td>Pierre Perret</td>\n",
       "<td>2023-09-06</td>\n",
       "<td>05:30:02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Manzenera</td>\n",
       "<td>Adam Ant</td>\n",
       "<td>2023-09-06</td>\n",
       "<td>05:26:39</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>I Want You Back</td>\n",
       "<td>The Plimsouls</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:44:35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Zero M2</td>\n",
       "<td>Benga</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:40:41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Elizabeth Story_ et al._ Honeybabe_ Your Papa Cares For You</td>\n",
       "<td>Elizabeth Cotten</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:32:45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Can't Break a Dead Girl's Heart</td>\n",
       "<td>Zombina &amp; The Skeletones</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:30:13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Killing Me Softly With His Song</td>\n",
       "<td>CoCo Lee</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:25:02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>This Kind Of Love (Full Band)</td>\n",
       "<td>Sister Hazel</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:21:12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Unsaveable (Album Version)</td>\n",
       "<td>Faith Hill</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:17:20</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Three Little Fishes</td>\n",
       "<td>Frankie Howard</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:14:01</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Trouble</td>\n",
       "<td>Bruce Robison</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:10:57</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Ke T' Oniro Egine Efialtis</td>\n",
       "<td>Despina Vandi</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:06:53</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Abilene</td>\n",
       "<td>The Highwaymen</td>\n",
       "<td>2023-09-05</td>\n",
       "<td>04:02:16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Healed (LP Version)</td>\n",
       "<td>Shirley Caesar</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:30:04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>I Want To Go Home</td>\n",
       "<td>Remmy Ongala</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:21:55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Baby My Love</td>\n",
       "<td>The In Crowd / Jah Stitch</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:13:58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>The Same Man (For Matthew)</td>\n",
       "<td>Rachel Loy</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:10:38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Better Days (Album Version)</td>\n",
       "<td>Faith Hill</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:07:10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Lametavolante</td>\n",
       "<td>Facto Delafe y las flores azules</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:05:02</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Jenny Take a Ride</td>\n",
       "<td>Mitch Ryder</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>22:01:35</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Pushing Me Away (Album Version)</td>\n",
       "<td>Linkin Park</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>21:58:25</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>M'En Vas A La Fontaine</td>\n",
       "<td>Jacques Douai</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>21:56:37</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Catcher In The Rye</td>\n",
       "<td>Guns N' Roses</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>21:50:44</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>The Bubble</td>\n",
       "<td>Tha Liks featuring King T</td>\n",
       "<td>2023-09-04</td>\n",
       "<td>21:46:33</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = soup.find('table')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? This one worked quite well! Inspecting the table a bit more, you can get at the individual rows using the `tr` tag. Again, use your browser's inspect tool to spot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr>\n",
       "<th>Song Title</th>\n",
       "<th>Artist</th>\n",
       "<th>Date</th>\n",
       "<th>Time</th>\n",
       "</tr>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.find('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the first row. Using `.find_all()`, instead, will give you a list of all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr>\n",
       " <th>Song Title</th>\n",
       " <th>Artist</th>\n",
       " <th>Date</th>\n",
       " <th>Time</th>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Too Much Saturn</td>\n",
       " <td>Francis Dunnery</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:49:31</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Sample Track 11</td>\n",
       " <td>Simon Harris</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:46:24</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Stand (Album Version)</td>\n",
       " <td>Kiss</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:41:33</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>High Tide</td>\n",
       " <td>Richard Souther</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:37:44</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Sense</td>\n",
       " <td>Beherit</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:27:19</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Outro</td>\n",
       " <td>Attack Attack</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:25:55</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>I don't Want to Be Alone Tonight</td>\n",
       " <td>Jauida</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:21:25</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>E. Warren</td>\n",
       " <td>DJ Omega</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:17:04</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Banana Man</td>\n",
       " <td>C.J. Chenier</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>10:13:55</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Happiness Stan - Original</td>\n",
       " <td>Small Facers</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>09:54:54</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Phorever People</td>\n",
       " <td>Shamen</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>09:50:01</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Signs Of Insanity</td>\n",
       " <td>Headhunter</td>\n",
       " <td>2023-09-10</td>\n",
       " <td>09:45:24</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Doctrines</td>\n",
       " <td>Arthur Fiedler</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:48:14</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>It Hurts Me Too</td>\n",
       " <td>Jeremy Spencer</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:43:24</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>A B***** Is A B***** (Edited)</td>\n",
       " <td>N.W.A.</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:40:36</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Real Love</td>\n",
       " <td>The Smashing Pumpkins</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:36:26</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Say What!?!</td>\n",
       " <td>Chris Standring</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:31:57</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Revelations</td>\n",
       " <td>Marco Beltrami</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:28:28</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Jos Defiance</td>\n",
       " <td>Don Francisco</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:24:01</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Quickstep</td>\n",
       " <td>FSTZ</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:19:44</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Shape Of My Heart</td>\n",
       " <td>Backstreet Boys</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:16:50</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Class Act</td>\n",
       " <td>James Hunter</td>\n",
       " <td>2023-09-07</td>\n",
       " <td>18:13:50</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Secrets</td>\n",
       " <td>Sunscreem</td>\n",
       " <td>2023-09-06</td>\n",
       " <td>05:43:38</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>She's My Woman</td>\n",
       " <td>Crusaders</td>\n",
       " <td>2023-09-06</td>\n",
       " <td>05:41:41</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Anything For My Baby</td>\n",
       " <td>Kiss</td>\n",
       " <td>2023-09-06</td>\n",
       " <td>05:39:06</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Apogee (ft. TechTonic)</td>\n",
       " <td>Paul Taylor</td>\n",
       " <td>2023-09-06</td>\n",
       " <td>05:31:33</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Le poulet</td>\n",
       " <td>Pierre Perret</td>\n",
       " <td>2023-09-06</td>\n",
       " <td>05:30:02</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Manzenera</td>\n",
       " <td>Adam Ant</td>\n",
       " <td>2023-09-06</td>\n",
       " <td>05:26:39</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>I Want You Back</td>\n",
       " <td>The Plimsouls</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:44:35</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Zero M2</td>\n",
       " <td>Benga</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:40:41</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Elizabeth Story_ et al._ Honeybabe_ Your Papa Cares For You</td>\n",
       " <td>Elizabeth Cotten</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:32:45</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Can't Break a Dead Girl's Heart</td>\n",
       " <td>Zombina &amp; The Skeletones</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:30:13</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Killing Me Softly With His Song</td>\n",
       " <td>CoCo Lee</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:25:02</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>This Kind Of Love (Full Band)</td>\n",
       " <td>Sister Hazel</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:21:12</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Unsaveable (Album Version)</td>\n",
       " <td>Faith Hill</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:17:20</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Three Little Fishes</td>\n",
       " <td>Frankie Howard</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:14:01</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Trouble</td>\n",
       " <td>Bruce Robison</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:10:57</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Ke T' Oniro Egine Efialtis</td>\n",
       " <td>Despina Vandi</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:06:53</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Abilene</td>\n",
       " <td>The Highwaymen</td>\n",
       " <td>2023-09-05</td>\n",
       " <td>04:02:16</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Healed (LP Version)</td>\n",
       " <td>Shirley Caesar</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:30:04</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>I Want To Go Home</td>\n",
       " <td>Remmy Ongala</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:21:55</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Baby My Love</td>\n",
       " <td>The In Crowd / Jah Stitch</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:13:58</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>The Same Man (For Matthew)</td>\n",
       " <td>Rachel Loy</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:10:38</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Better Days (Album Version)</td>\n",
       " <td>Faith Hill</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:07:10</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Lametavolante</td>\n",
       " <td>Facto Delafe y las flores azules</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:05:02</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Jenny Take a Ride</td>\n",
       " <td>Mitch Ryder</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>22:01:35</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Pushing Me Away (Album Version)</td>\n",
       " <td>Linkin Park</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>21:58:25</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>M'En Vas A La Fontaine</td>\n",
       " <td>Jacques Douai</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>21:56:37</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>Catcher In The Rye</td>\n",
       " <td>Guns N' Roses</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>21:50:44</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>The Bubble</td>\n",
       " <td>Tha Liks featuring King T</td>\n",
       " <td>2023-09-04</td>\n",
       " <td>21:46:33</td>\n",
       " </tr>]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = table.find_all('tr')\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check whether the number of rows is equal to what we would expect from looking at the website. Using the `len` function for this yields..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks about right? Yes! So, let's now try to extract, for one row, the name of the song and artist, corresponding to the first and second column of the table.\n",
    "\n",
    "Let's first select one row for prototyping. We take row 2 (which is the first row after the table header)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_row = rows[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr>\n",
       "<td>Too Much Saturn</td>\n",
       "<td>Francis Dunnery</td>\n",
       "<td>2023-09-10</td>\n",
       "<td>10:49:31</td>\n",
       "</tr>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Too Much Saturn'"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_row.find_all('td')[0].get_text() # for song name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Francis Dunnery'"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_row.find_all('td')[1].get_text() # for artist name, corresponding to the second \"column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now put everything together in one script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Song \"Too Much Saturn\" by \"Francis Dunnery\"\n",
      "Song \"Sample Track 11\" by \"Simon Harris\"\n",
      "Song \"Stand (Album Version)\" by \"Kiss\"\n",
      "Song \"High Tide\" by \"Richard Souther\"\n",
      "Song \"Sense\" by \"Beherit\"\n",
      "Song \"Outro\" by \"Attack Attack\"\n",
      "Song \"I don't Want to Be Alone Tonight\" by \"Jauida\"\n",
      "Song \"E. Warren\" by \"DJ Omega\"\n",
      "Song \"Banana Man\" by \"C.J. Chenier\"\n",
      "Song \"Happiness Stan - Original\" by \"Small Facers\"\n",
      "Song \"Phorever People\" by \"Shamen\"\n",
      "Song \"Signs Of Insanity\" by \"Headhunter\"\n",
      "Song \"Doctrines\" by \"Arthur Fiedler\"\n",
      "Song \"It Hurts Me Too\" by \"Jeremy Spencer\"\n",
      "Song \"A B***** Is A B***** (Edited)\" by \"N.W.A.\"\n",
      "Song \"Real Love\" by \"The Smashing Pumpkins\"\n",
      "Song \"Say What!?!\" by \"Chris Standring\"\n",
      "Song \"Revelations\" by \"Marco Beltrami\"\n",
      "Song \"Jos Defiance\" by \"Don Francisco\"\n",
      "Song \"Quickstep\" by \"FSTZ\"\n",
      "Song \"Shape Of My Heart\" by \"Backstreet Boys\"\n",
      "Song \"Class Act\" by \"James Hunter\"\n",
      "Song \"Secrets\" by \"Sunscreem\"\n",
      "Song \"She's My Woman\" by \"Crusaders\"\n",
      "Song \"Anything For My Baby\" by \"Kiss\"\n",
      "Song \"Apogee (ft. TechTonic)\" by \"Paul Taylor\"\n",
      "Song \"Le poulet\" by \"Pierre Perret\"\n",
      "Song \"Manzenera\" by \"Adam Ant\"\n",
      "Song \"I Want You Back\" by \"The Plimsouls\"\n",
      "Song \"Zero M2\" by \"Benga\"\n",
      "Song \"Elizabeth Story_ et al._ Honeybabe_ Your Papa Cares For You\" by \"Elizabeth Cotten\"\n",
      "Song \"Can't Break a Dead Girl's Heart\" by \"Zombina & The Skeletones\"\n",
      "Song \"Killing Me Softly With His Song\" by \"CoCo Lee\"\n",
      "Song \"This Kind Of Love (Full Band)\" by \"Sister Hazel\"\n",
      "Song \"Unsaveable (Album Version)\" by \"Faith Hill\"\n",
      "Song \"Three Little Fishes\" by \"Frankie Howard\"\n",
      "Song \"Trouble\" by \"Bruce Robison\"\n",
      "Song \"Ke T' Oniro Egine Efialtis\" by \"Despina Vandi\"\n",
      "Song \"Abilene\" by \"The Highwaymen\"\n",
      "Song \"Healed (LP Version)\" by \"Shirley Caesar\"\n",
      "Song \"I Want To Go Home\" by \"Remmy Ongala\"\n",
      "Song \"Baby My Love\" by \"The In Crowd / Jah Stitch\"\n",
      "Song \"The Same Man (For Matthew)\" by \"Rachel Loy\"\n",
      "Song \"Better Days (Album Version)\" by \"Faith Hill\"\n",
      "Song \"Lametavolante\" by \"Facto Delafe y las flores azules\"\n",
      "Song \"Jenny Take a Ride\" by \"Mitch Ryder\"\n",
      "Song \"Pushing Me Away (Album Version)\" by \"Linkin Park\"\n",
      "Song \"M'En Vas A La Fontaine\" by \"Jacques Douai\"\n",
      "Song \"Catcher In The Rye\" by \"Guns N' Roses\"\n",
      "Song \"The Bubble\" by \"Tha Liks featuring King T\"\n"
     ]
    }
   ],
   "source": [
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=36'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    #print(row)\n",
    "    data = row.find_all('td')\n",
    "    \n",
    "    if len(data)>0:\n",
    "        song_name=data[0].get_text()\n",
    "        artist_name=data[1].get_text()\n",
    "        \n",
    "        print(f'Song \"{song_name}\" by \"{artist_name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.1__\n",
    "\n",
    "1. Rather than printing the data to the screen, store it in a list of dictionaries, containing the following data points:\n",
    "    - song\n",
    "    - artist\n",
    "    - date\n",
    "    - username\n",
    "    - and time of data extraction.\n",
    "2. Wrap your code in a function, that returns the JSON dictionary from 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'song_name': 'Too Much Saturn',\n",
       "  'artist_name': 'Francis Dunnery',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:49:31',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Sample Track 11',\n",
       "  'artist_name': 'Simon Harris',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:46:24',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Stand (Album Version)',\n",
       "  'artist_name': 'Kiss',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:41:33',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'High Tide',\n",
       "  'artist_name': 'Richard Souther',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:37:44',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Sense',\n",
       "  'artist_name': 'Beherit',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:27:19',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Outro',\n",
       "  'artist_name': 'Attack Attack',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:25:55',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"I don't Want to Be Alone Tonight\",\n",
       "  'artist_name': 'Jauida',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:21:25',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'E. Warren',\n",
       "  'artist_name': 'DJ Omega',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:17:04',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Banana Man',\n",
       "  'artist_name': 'C.J. Chenier',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:13:55',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Happiness Stan - Original',\n",
       "  'artist_name': 'Small Facers',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '09:54:54',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Phorever People',\n",
       "  'artist_name': 'Shamen',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '09:50:01',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Signs Of Insanity',\n",
       "  'artist_name': 'Headhunter',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '09:45:24',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Doctrines',\n",
       "  'artist_name': 'Arthur Fiedler',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:48:14',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'It Hurts Me Too',\n",
       "  'artist_name': 'Jeremy Spencer',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:43:24',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'A B***** Is A B***** (Edited)',\n",
       "  'artist_name': 'N.W.A.',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:40:36',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Real Love',\n",
       "  'artist_name': 'The Smashing Pumpkins',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:36:26',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Say What!?!',\n",
       "  'artist_name': 'Chris Standring',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:31:57',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Revelations',\n",
       "  'artist_name': 'Marco Beltrami',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:28:28',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Jos Defiance',\n",
       "  'artist_name': 'Don Francisco',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:24:01',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Quickstep',\n",
       "  'artist_name': 'FSTZ',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:19:44',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Shape Of My Heart',\n",
       "  'artist_name': 'Backstreet Boys',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:16:50',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Class Act',\n",
       "  'artist_name': 'James Hunter',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:13:50',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Secrets',\n",
       "  'artist_name': 'Sunscreem',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:43:38',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"She's My Woman\",\n",
       "  'artist_name': 'Crusaders',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:41:41',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Anything For My Baby',\n",
       "  'artist_name': 'Kiss',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:39:06',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Apogee (ft. TechTonic)',\n",
       "  'artist_name': 'Paul Taylor',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:31:33',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Le poulet',\n",
       "  'artist_name': 'Pierre Perret',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:30:02',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Manzenera',\n",
       "  'artist_name': 'Adam Ant',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:26:39',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'I Want You Back',\n",
       "  'artist_name': 'The Plimsouls',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:44:35',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Zero M2',\n",
       "  'artist_name': 'Benga',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:40:41',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Elizabeth Story_ et al._ Honeybabe_ Your Papa Cares For You',\n",
       "  'artist_name': 'Elizabeth Cotten',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:32:45',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Can't Break a Dead Girl's Heart\",\n",
       "  'artist_name': 'Zombina & The Skeletones',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:30:13',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Killing Me Softly With His Song',\n",
       "  'artist_name': 'CoCo Lee',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:25:02',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'This Kind Of Love (Full Band)',\n",
       "  'artist_name': 'Sister Hazel',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:21:12',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Unsaveable (Album Version)',\n",
       "  'artist_name': 'Faith Hill',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:17:20',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Three Little Fishes',\n",
       "  'artist_name': 'Frankie Howard',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:14:01',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Trouble',\n",
       "  'artist_name': 'Bruce Robison',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:10:57',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Ke T' Oniro Egine Efialtis\",\n",
       "  'artist_name': 'Despina Vandi',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:06:53',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Abilene',\n",
       "  'artist_name': 'The Highwaymen',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:02:16',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Healed (LP Version)',\n",
       "  'artist_name': 'Shirley Caesar',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:30:04',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'I Want To Go Home',\n",
       "  'artist_name': 'Remmy Ongala',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:21:55',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Baby My Love',\n",
       "  'artist_name': 'The In Crowd / Jah Stitch',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:13:58',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Same Man (For Matthew)',\n",
       "  'artist_name': 'Rachel Loy',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:10:38',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Better Days (Album Version)',\n",
       "  'artist_name': 'Faith Hill',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:07:10',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Lametavolante',\n",
       "  'artist_name': 'Facto Delafe y las flores azules',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:05:02',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Jenny Take a Ride',\n",
       "  'artist_name': 'Mitch Ryder',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:01:35',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Pushing Me Away (Album Version)',\n",
       "  'artist_name': 'Linkin Park',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:58:25',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"M'En Vas A La Fontaine\",\n",
       "  'artist_name': 'Jacques Douai',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:56:37',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Catcher In The Rye',\n",
       "  'artist_name': \"Guns N' Roses\",\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:50:44',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Bubble',\n",
       "  'artist_name': 'Tha Liks featuring King T',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:46:33',\n",
       "  'timestamp_of_extraction': 1694551620,\n",
       "  'username': 'StarCoder49&week'}]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1:\n",
    "import time\n",
    "\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=36'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)\n",
    "\n",
    "table = soup.find('table')\n",
    "\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "json_data=[]\n",
    "\n",
    "for row in rows:\n",
    "    data = row.find_all('td')\n",
    "\n",
    "    if len(data)>0:\n",
    "        song_name=data[0].get_text()\n",
    "        artist_name=data[1].get_text()\n",
    "        date=data[2].get_text()\n",
    "        timestamp=data[3].get_text()\n",
    "        json_data.append({'song_name': song_name,\n",
    "                          'artist_name': artist_name,\n",
    "                          'date': date,\n",
    "                          'time': timestamp,\n",
    "                          'timestamp_of_extraction': int(time.time()),\n",
    "                          'username': url.split('=')[1]})\n",
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "\n",
    "def get_consumption_history(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    json_data=[]\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "    \n",
    "        if len(data)>0:\n",
    "            song_name=data[0].get_text()\n",
    "            artist_name=data[1].get_text()\n",
    "            date=data[2].get_text()\n",
    "            timestamp=data[3].get_text()\n",
    "            json_data.append({'song_name': song_name,\n",
    "                              'artist_name': artist_name,\n",
    "                              'date': date,\n",
    "                              'time': timestamp,\n",
    "                              'timestamp_of_extraction': int(time.time()),\n",
    "                              'username': url.split('=')[1]})\n",
    "    return(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'song_name': 'Too Much Saturn',\n",
       "  'artist_name': 'Francis Dunnery',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:49:31',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Sample Track 11',\n",
       "  'artist_name': 'Simon Harris',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:46:24',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Stand (Album Version)',\n",
       "  'artist_name': 'Kiss',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:41:33',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'High Tide',\n",
       "  'artist_name': 'Richard Souther',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:37:44',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Sense',\n",
       "  'artist_name': 'Beherit',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:27:19',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Outro',\n",
       "  'artist_name': 'Attack Attack',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:25:55',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"I don't Want to Be Alone Tonight\",\n",
       "  'artist_name': 'Jauida',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:21:25',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'E. Warren',\n",
       "  'artist_name': 'DJ Omega',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:17:04',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Banana Man',\n",
       "  'artist_name': 'C.J. Chenier',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '10:13:55',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Happiness Stan - Original',\n",
       "  'artist_name': 'Small Facers',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '09:54:54',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Phorever People',\n",
       "  'artist_name': 'Shamen',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '09:50:01',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Signs Of Insanity',\n",
       "  'artist_name': 'Headhunter',\n",
       "  'date': '2023-09-10',\n",
       "  'time': '09:45:24',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Doctrines',\n",
       "  'artist_name': 'Arthur Fiedler',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:48:14',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'It Hurts Me Too',\n",
       "  'artist_name': 'Jeremy Spencer',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:43:24',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'A B***** Is A B***** (Edited)',\n",
       "  'artist_name': 'N.W.A.',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:40:36',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Real Love',\n",
       "  'artist_name': 'The Smashing Pumpkins',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:36:26',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Say What!?!',\n",
       "  'artist_name': 'Chris Standring',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:31:57',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Revelations',\n",
       "  'artist_name': 'Marco Beltrami',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:28:28',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Jos Defiance',\n",
       "  'artist_name': 'Don Francisco',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:24:01',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Quickstep',\n",
       "  'artist_name': 'FSTZ',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:19:44',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Shape Of My Heart',\n",
       "  'artist_name': 'Backstreet Boys',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:16:50',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Class Act',\n",
       "  'artist_name': 'James Hunter',\n",
       "  'date': '2023-09-07',\n",
       "  'time': '18:13:50',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Secrets',\n",
       "  'artist_name': 'Sunscreem',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:43:38',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"She's My Woman\",\n",
       "  'artist_name': 'Crusaders',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:41:41',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Anything For My Baby',\n",
       "  'artist_name': 'Kiss',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:39:06',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Apogee (ft. TechTonic)',\n",
       "  'artist_name': 'Paul Taylor',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:31:33',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Le poulet',\n",
       "  'artist_name': 'Pierre Perret',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:30:02',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Manzenera',\n",
       "  'artist_name': 'Adam Ant',\n",
       "  'date': '2023-09-06',\n",
       "  'time': '05:26:39',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'I Want You Back',\n",
       "  'artist_name': 'The Plimsouls',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:44:35',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Zero M2',\n",
       "  'artist_name': 'Benga',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:40:41',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Elizabeth Story_ et al._ Honeybabe_ Your Papa Cares For You',\n",
       "  'artist_name': 'Elizabeth Cotten',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:32:45',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Can't Break a Dead Girl's Heart\",\n",
       "  'artist_name': 'Zombina & The Skeletones',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:30:13',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Killing Me Softly With His Song',\n",
       "  'artist_name': 'CoCo Lee',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:25:02',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'This Kind Of Love (Full Band)',\n",
       "  'artist_name': 'Sister Hazel',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:21:12',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Unsaveable (Album Version)',\n",
       "  'artist_name': 'Faith Hill',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:17:20',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Three Little Fishes',\n",
       "  'artist_name': 'Frankie Howard',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:14:01',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Trouble',\n",
       "  'artist_name': 'Bruce Robison',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:10:57',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Ke T' Oniro Egine Efialtis\",\n",
       "  'artist_name': 'Despina Vandi',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:06:53',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Abilene',\n",
       "  'artist_name': 'The Highwaymen',\n",
       "  'date': '2023-09-05',\n",
       "  'time': '04:02:16',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Healed (LP Version)',\n",
       "  'artist_name': 'Shirley Caesar',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:30:04',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'I Want To Go Home',\n",
       "  'artist_name': 'Remmy Ongala',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:21:55',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Baby My Love',\n",
       "  'artist_name': 'The In Crowd / Jah Stitch',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:13:58',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Same Man (For Matthew)',\n",
       "  'artist_name': 'Rachel Loy',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:10:38',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Better Days (Album Version)',\n",
       "  'artist_name': 'Faith Hill',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:07:10',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Lametavolante',\n",
       "  'artist_name': 'Facto Delafe y las flores azules',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:05:02',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Jenny Take a Ride',\n",
       "  'artist_name': 'Mitch Ryder',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '22:01:35',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Pushing Me Away (Album Version)',\n",
       "  'artist_name': 'Linkin Park',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:58:25',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"M'En Vas A La Fontaine\",\n",
       "  'artist_name': 'Jacques Douai',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:56:37',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Catcher In The Rye',\n",
       "  'artist_name': \"Guns N' Roses\",\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:50:44',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Bubble',\n",
       "  'artist_name': 'Tha Liks featuring King T',\n",
       "  'date': '2023-09-04',\n",
       "  'time': '21:46:33',\n",
       "  'timestamp_of_extraction': 1694551626,\n",
       "  'username': 'StarCoder49&week'}]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try running the function\n",
    "get_consumption_history('https://music-to-scrape.org/user?username=StarCoder49&week=36')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'song_name': '7 Miles',\n",
       "  'artist_name': 'Brixx',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '03:06:17',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Good Texan',\n",
       "  'artist_name': 'The Vaughan Brothers',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '03:01:54',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Danny',\n",
       "  'artist_name': 'Billie Jo Spears',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:59:50',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Sally Can't Dance\",\n",
       "  'artist_name': 'Lou Reed',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:56:18',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Tien An Man Dream Again',\n",
       "  'artist_name': 'fIREHOSE',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:55:00',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Cycle Time',\n",
       "  'artist_name': 'Liars',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:52:44',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Stop Breaking Down (1994 Digital Remaster)',\n",
       "  'artist_name': 'The Rolling Stones',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:48:09',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Country Preacher',\n",
       "  'artist_name': 'Jimmy Johnson',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:43:22',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Uncle B.S. - 1963',\n",
       "  'artist_name': 'Tim Wilson',\n",
       "  'date': '2023-03-26',\n",
       "  'time': '02:41:42',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Spooks In Space',\n",
       "  'artist_name': 'Perrey And Kingsley',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:51:21',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Hello',\n",
       "  'artist_name': 'Jan Wayne',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:44:59',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Lord Will Go Before You',\n",
       "  'artist_name': 'Janet Paschal',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:41:32',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Like A Hurricane',\n",
       "  'artist_name': 'Eddy Raven',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:37:56',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Junko partner',\n",
       "  'artist_name': 'Mike Bloomfield',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:33:12',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Ricky Ticky Toc',\n",
       "  'artist_name': 'Eminem',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:30:20',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Like a Bird Pulling Up At a Worm',\n",
       "  'artist_name': 'We Show Up On Radar',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:27:06',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Imperial March',\n",
       "  'artist_name': 'The London Pops Orchestra',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:24:05',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Stolen Pills',\n",
       "  'artist_name': 'Spiral Stairs',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:21:38',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Isn't That Just Like God (Low Key Performance Track Without Background Vocals)\",\n",
       "  'artist_name': 'Babbie Mason',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:17:42',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'O.R.P.',\n",
       "  'artist_name': 'Vulgar Pigeons',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:15:36',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Always',\n",
       "  'artist_name': 'RyanDan',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:11:41',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'I Did it for You',\n",
       "  'artist_name': 'David Cook',\n",
       "  'date': '2023-03-25',\n",
       "  'time': '15:07:52',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Tristeza (Brazilian Blues)',\n",
       "  'artist_name': 'Luiz Bonfa / Oscar Castro Neves / Lalo Schifrin',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:36:35',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Night And Day',\n",
       "  'artist_name': 'Oscar Peterson Trio',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:34:03',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Thank You For Calling',\n",
       "  'artist_name': 'Billy Walker',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:31:49',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Seventeen',\n",
       "  'artist_name': 'Sex Pistols',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:29:39',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Intro',\n",
       "  'artist_name': 'Bizzy Bone Presents',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:28:48',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Thank You',\n",
       "  'artist_name': 'Saga',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:24:45',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Lovin' Cup ( Remastered '97 Version )\",\n",
       "  'artist_name': 'The Paul Butterfield Blues Band',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:22:03',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Nature Boy',\n",
       "  'artist_name': 'Quadro Nuevo',\n",
       "  'date': '2023-03-24',\n",
       "  'time': '17:19:50',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'All I Ever',\n",
       "  'artist_name': 'Ms. Dynamite',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '10:23:46',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Hey Louise',\n",
       "  'artist_name': 'Neil Diamond',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '10:20:44',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'New Round And Round',\n",
       "  'artist_name': 'Casey Bill Weldon',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '10:17:41',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Appapas Del Calabar',\n",
       "  'artist_name': 'Los Van Van',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '10:13:13',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Heart Breakin' Blues\",\n",
       "  'artist_name': \"Cannon's Jug Stompers\",\n",
       "  'date': '2023-03-23',\n",
       "  'time': '10:10:06',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': '18 Carat Garbage',\n",
       "  'artist_name': 'Billie Ray Martin Feat. Ann Peebles',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '10:02:19',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Don't Go (Produced by Frequency)\",\n",
       "  'artist_name': 'Wordsworth feat. Adanita Ross',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:58:22',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Resurrection Man',\n",
       "  'artist_name': 'Heaven 17',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:54:20',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': '4 In The Morning',\n",
       "  'artist_name': 'Gwen Stefani',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:49:25',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Cuatro Veinte',\n",
       "  'artist_name': 'Gonzalo Rubalcaba',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:39:39',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Der Kleine Idiot',\n",
       "  'artist_name': 'Marco Rima',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:33:36',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"I'm Going Through\",\n",
       "  'artist_name': 'Edwin Hawkins',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:27:38',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Big Legged Woman',\n",
       "  'artist_name': 'Robert Lockwood_ Jr.',\n",
       "  'date': '2023-03-23',\n",
       "  'time': '09:23:00',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'A Little Of You',\n",
       "  'artist_name': 'Eurythmics',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:27:13',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Shrine (Reissue) (Album Version)',\n",
       "  'artist_name': 'King Diamond',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:22:50',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Somewhere Else',\n",
       "  'artist_name': 'Jim Bryson',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:18:57',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Welcome 2 Detroit',\n",
       "  'artist_name': 'Trick Trick / Eminem',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:14:34',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Lotion (LP Version)',\n",
       "  'artist_name': 'Deftones',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:10:36',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Succotash',\n",
       "  'artist_name': 'Claw Hammer',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:05:44',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'How Do I Maintain Pt. II',\n",
       "  'artist_name': 'Shout Out Out Out Out',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '09:01:50',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Next Time',\n",
       "  'artist_name': 'Nadine Renee',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '08:57:41',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Cursed Angel Of Doom (Previously Unreleased&Re-Recorded Track)',\n",
       "  'artist_name': 'Behemoth',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '08:54:33',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'When Will I Be Loved',\n",
       "  'artist_name': 'Sandy Posey',\n",
       "  'date': '2023-03-22',\n",
       "  'time': '08:52:28',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Salvation',\n",
       "  'artist_name': 'Chroma Key',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '17:21:43',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Un Pensamiento',\n",
       "  'artist_name': 'Seguridad Social',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '17:17:05',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Night on William Street',\n",
       "  'artist_name': 'DJ Godfather',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '17:13:42',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': \"Nuttin' 2 Somethin'\",\n",
       "  'artist_name': 'T-Bone',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '17:09:58',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Whole World Looking At Me (Explicit LP Version)',\n",
       "  'artist_name': 'Busta Rhymes',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '17:06:32',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Federico',\n",
       "  'artist_name': 'Mongo Santamaria Orchestra',\n",
       "  'date': '2023-03-21',\n",
       "  'time': '17:03:09',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'The Mission',\n",
       "  'artist_name': 'Stephen Marley / Damian Marley',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '06:10:46',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Dark Inside',\n",
       "  'artist_name': 'Al Duvall',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '06:07:59',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Ride Wit Me',\n",
       "  'artist_name': 'T-Bone',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '06:04:05',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Better Life',\n",
       "  'artist_name': '3 Doors Down',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '06:00:42',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Bonus Beats (2002 Digital Remaster)',\n",
       "  'artist_name': 'N.W.A.',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '05:57:38',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Love Is Gonna Bring Us Back Alive',\n",
       "  'artist_name': 'Rickie Lee Jones',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '05:52:47',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Phrase',\n",
       "  'artist_name': 'Papa Wemba',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '05:47:38',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Clavelitos',\n",
       "  'artist_name': 'Charanga de la 4',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '05:42:08',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Be Like That',\n",
       "  'artist_name': '3 Doors Down',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '05:37:30',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'},\n",
       " {'song_name': 'Rosemary Recalls',\n",
       "  'artist_name': 'Bruce Rowland',\n",
       "  'date': '2023-03-20',\n",
       "  'time': '05:36:06',\n",
       "  'timestamp_of_extraction': 1694551629,\n",
       "  'username': 'StarCoder49&week'}]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it also works for different weeks\n",
    "get_consumption_history('https://music-to-scrape.org/user?username=StarCoder49&week=12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loop through all weeks for each user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance__\n",
    "\n",
    "Alright - what have we achieve so far?\n",
    "\n",
    "- In section 1, we've built a function to retrieve user names of currently active users. We call this the stage of our project in which we collect \"seeds\".\n",
    "- In section 2.1, we've managed to extract a user's consumption history from a table displayed on the user's profile page.\n",
    "\n",
    "What's missing, though, is __ALL of a user's consumption data__, i.e., from __ALL possible weeks__.\n",
    "\n",
    "For this, we're making use of the \"previous page\" button.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mits-previous-button.png\" align=\"left\" width=30%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let's try it out__\n",
    "\n",
    "Open the user's profile page at https://music-to-scrape.org/user?username=StarCoder49. __Click on the previous button__ a few times, and observe how the URL in your browser bar is changing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=37`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=36`\n",
    "- `https://music-to-scrape.org/user?username=StarCoder49&week=35`\n",
    "- ...\n",
    "\n",
    "Can you guess the next one...?\n",
    "\n",
    "A general solution is to look up whether there is a `previous` button on the page (see HTML code below). We can then either \"grab\" the URL and visit it, or - instead - \"click\" on the button.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/mts-previous-page.png\" align=\"left\" width=60% style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's write a snippet that \"captures\" the link of the previous page button! We always proceed in small steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the website's source code and convert to BeautifulSoup object\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49'\n",
    "\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "res = requests.get(url, headers = header)\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"page-link\" href=\"user?username=StarCoder49&amp;week=36\" type=\"previous_page\">Previous\n",
       "                                        Week</a>"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Trying to locate the previous button, using a combination of class names and attribute-value pairs.\n",
    "soup.find(class_='page-link', attrs={'type':'previous_page'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Trying to extract the `href` attribute\n",
    "soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Storing \"previous page\" link\n",
    "previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "previous_page_link # print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we can observe how we're getting closer to the information we need.\n",
    "\n",
    "Now, we only need to combine the base URL (`https://music-to-scrape.org/`) with the page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://music-to-scrape.org/user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "f'https://music-to-scrape.org/{previous_page_link}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.2__\n",
    "\n",
    "Please first load the snippet below, which has wrapped the \"previous page\" capturing in a function. Observe the use of `try` and `except`, which accounts for the last page NOT having a next page button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_page(soup):\n",
    "    try:\n",
    "        previous_page_link = soup.find(class_='page-link', attrs={'type':'previous_page'}).attrs['href']\n",
    "        return(f'https://music-to-scrape.org/{previous_page_link}')\n",
    "    except:\n",
    "        return('no previous page')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out this function on the source code of the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://music-to-scrape.org/user?username=StarCoder49&week=36'"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get('https://music-to-scrape.org/user?username=StarCoder49').text)\n",
    "previous_page(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, it worked! Now, proceed with the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Make a web requests to 'https://music-to-scrape.org/user?username=StarCoder49&week=36', and pass on the (souped) object to the `previous_page()` function and observe the output. Then, use 'https://music-to-scrape.org/user?username=StarCoder49&week=0'. Is that what you expected? \n",
    "\n",
    "2. Write a while loop that continuously visits all pages for the user `StarCoder49`, by extracting previous page URLs from each page and continuing the data collection until there is no previous page to fetch. Start with week 10 to minimize server load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://music-to-scrape.org/user?username=StarCoder49&week=35'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "soup = BeautifulSoup(requests.get('https://music-to-scrape.org/user?username=StarCoder49&week=36').text)\n",
    "previous_page(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no previous page'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get('https://music-to-scrape.org/user?username=StarCoder49&week=0').text)\n",
    "previous_page(soup)\n",
    "# returns \"no previous page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=10 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=9 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=8 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=7 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=6 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=5 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=4 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=3 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=2 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=1 and checking for next page...\n",
      "Opening https://music-to-scrape.org/user?username=StarCoder49&week=0 and checking for next page...\n"
     ]
    }
   ],
   "source": [
    "# Question 2\n",
    "urls = []\n",
    "\n",
    "# define first URL to start from\n",
    "url = 'https://music-to-scrape.org/user?username=StarCoder49&week=10'\n",
    "\n",
    "while True:\n",
    "    print(f'Opening {url} and checking for next page...')\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "    previous_url = previous_page(soup)\n",
    "    if 'no previous page' in previous_url: break\n",
    "    url = previous_url\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "So... seems like we're almost there!\n",
    "\n",
    "The only thing that's missing is to actually also extract the song consumption data from each of the user profile pages.\n",
    "\n",
    "We turn towards this issue next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Improving Extraction Design\n",
    "\n",
    "### 3.1 Timers\n",
    "\n",
    "__Importance__\n",
    "\n",
    "Before we started running some of the cells above, you may have observed the usage of the `time.sleep` function. Sending many requests at the same time can overload a server. Therefore, pausing between requests rather than sending them all simultaneously is highly recommended. This prevents your IP address (i.e., the numerical label assigned to each device connected to the internet) from getting blocked, not allowing you to visit (and scrape) the website. \n",
    "\n",
    "__Let's try it out__\n",
    "\n",
    "In Python, you can import the `time` module, which pauses the execution of future commands for a given amount of time. For example, the print statement after `time.sleep(2)` will only be executed after 2 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be printed to the console after 2 seconds!\n"
     ]
    }
   ],
   "source": [
    "# run this cell again to see the timer in action yourself!\n",
    "import time\n",
    "pause = 2\n",
    "time.sleep(pause)\n",
    "print(f\"I'll be printed to the console after {pause} seconds!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.1__\n",
    "\n",
    "Modify the code above to sleep for 2 minutes. Go grab a coffee in-between. Did it take you longer than 2 minutes?\n",
    "\n",
    "(if you want to abort the running code, just select the cell and push the \"stop\" button!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2*60)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Modularization\n",
    "\n",
    "**Importance**  \n",
    "\n",
    "In scraping, many things have to be executed *multiple times*. For example, whenever we open a new user page on music-to-scrape.org, we would like to extract all the available book links.\n",
    "\n",
    "To help us execute things repeatedly, we will \"modularize\" our code into functions. We can then call these functions whenever we need them. Another benefit of using functions is that we can improve the readability and reusability of our code. If you need a quick refresher on functions, please revisit section 4 of the [Python Bootcamp](https://odcm.hannesdatta.com/docs/modules/week1/pythonbootcamp/).\n",
    "\n",
    "**Let's try it out**\n",
    "\n",
    "Let's finish our scraper by compiling everything we have learned thus far.\n",
    "\n",
    "Re-execute the function `get_users` from exercise 1.2 (3) above. Remember how it worked? Then proceed with your exercises.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://music-to-scrape.org/user?username=WizardShadow42',\n",
       " 'https://music-to-scrape.org/user?username=SonicNinja25',\n",
       " 'https://music-to-scrape.org/user?username=TechGeek32',\n",
       " 'https://music-to-scrape.org/user?username=CosmicSonic89',\n",
       " 'https://music-to-scrape.org/user?username=CyberStar49',\n",
       " 'https://music-to-scrape.org/user?username=Wizard79']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.2__\n",
    "\n",
    "Execute the function `get_users()` for a few minutes to collect a list of usernames. Store the user names in a JSON file (new-line separated), along with the timestamp of data retrieval `int(time.time())`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping user names...\n",
      "Scraping user names...\n",
      "Scraping user names...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "duration = 15 # for testing, just 15 seconds\n",
    "\n",
    "# Calculate the end time\n",
    "end_time = time.time() + duration\n",
    "\n",
    "f = open('seeds.json','w') # start a new file with seeds, so, use `w` (write new file) instead of `a` (append to existing file)\n",
    "\n",
    "# Run the loop until the current time reaches the end time\n",
    "while time.time() < end_time:\n",
    "    print(f'Scraping user names...')\n",
    "    for user in get_users():\n",
    "        new_user = {'url': user,\n",
    "                    'timestamp': int(time.time())}\n",
    "        f.write(json.dumps(new_user)+'\\n')\n",
    "    time.sleep(2)  # Sleep for a few seconds between each execution\n",
    "f.close()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://music-to-scrape.org/user?username=WizardShadow42', 'timestamp': 1694551693}\n",
      "{'url': 'https://music-to-scrape.org/user?username=SonicNinja25', 'timestamp': 1694551693}\n",
      "{'url': 'https://music-to-scrape.org/user?username=TechGeek32', 'timestamp': 1694551693}\n",
      "{'url': 'https://music-to-scrape.org/user?username=CosmicSonic89', 'timestamp': 1694551693}\n",
      "{'url': 'https://music-to-scrape.org/user?username=CyberStar49', 'timestamp': 1694551693}\n",
      "{'url': 'https://music-to-scrape.org/user?username=Wizard79', 'timestamp': 1694551693}\n",
      "{'url': 'https://music-to-scrape.org/user?username=WizardShadow42', 'timestamp': 1694551699}\n",
      "{'url': 'https://music-to-scrape.org/user?username=SonicNinja25', 'timestamp': 1694551699}\n",
      "{'url': 'https://music-to-scrape.org/user?username=TechGeek32', 'timestamp': 1694551699}\n",
      "{'url': 'https://music-to-scrape.org/user?username=CosmicSonic89', 'timestamp': 1694551699}\n",
      "{'url': 'https://music-to-scrape.org/user?username=CyberStar49', 'timestamp': 1694551699}\n",
      "{'url': 'https://music-to-scrape.org/user?username=Wizard79', 'timestamp': 1694551699}\n",
      "{'url': 'https://music-to-scrape.org/user?username=WizardShadow42', 'timestamp': 1694551705}\n",
      "{'url': 'https://music-to-scrape.org/user?username=SonicNinja25', 'timestamp': 1694551705}\n",
      "{'url': 'https://music-to-scrape.org/user?username=TechGeek32', 'timestamp': 1694551705}\n",
      "{'url': 'https://music-to-scrape.org/user?username=CosmicSonic89', 'timestamp': 1694551705}\n",
      "{'url': 'https://music-to-scrape.org/user?username=CyberStar49', 'timestamp': 1694551705}\n",
      "{'url': 'https://music-to-scrape.org/user?username=Wizard79', 'timestamp': 1694551705}\n"
     ]
    }
   ],
   "source": [
    "# verify whether you can open the data\n",
    "\n",
    "import json\n",
    "f = open('seeds.json','r',encoding = 'utf-8')\n",
    "data = f.readlines()\n",
    "for item in data:\n",
    "    print(json.loads(item))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 3.3__\n",
    "\n",
    "Now, let's write some code that loads `seeds.json`, and visit each user's __first profile page__ to extract consumption data. Remember to build in a little timer (e.g., waiting for 2 seconds or so). The prototype/starting code below stops automatically after 5 iterations to minimize server load. Try removing the prototyping condition using the comment character `#` when you think you're done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://music-to-scrape.org/user?username=WizardShadow42\n",
      "https://music-to-scrape.org/user?username=SonicNinja25\n",
      "https://music-to-scrape.org/user?username=TechGeek32\n",
      "https://music-to-scrape.org/user?username=CosmicSonic89\n",
      "https://music-to-scrape.org/user?username=CyberStar49\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import json\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    print(obj['url'])\n",
    "    \n",
    "    # eventually sleep for a second\n",
    "    time.sleep(2)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tips</b>\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>\n",
    "            Use the function <code>get_consumption_history(url)</code> from exercise 2.3 above!\n",
    "        </li>\n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information for https://music-to-scrape.org/user?username=WizardShadow42...\n",
      "Extracting information for https://music-to-scrape.org/user?username=SonicNinja25...\n",
      "Extracting information for https://music-to-scrape.org/user?username=TechGeek32...\n",
      "Extracting information for https://music-to-scrape.org/user?username=CosmicSonic89...\n",
      "Extracting information for https://music-to-scrape.org/user?username=CyberStar49...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import json\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    url = obj['url']\n",
    "\n",
    "    print(f'Extracting information for {url}...')\n",
    "    \n",
    "    output_file = open('output_data.json','a')\n",
    "\n",
    "    songs = get_consumption_history(url)\n",
    "\n",
    "    for song in songs:\n",
    "        output_file.write(json.dumps(song))\n",
    "        output_file.write('\\n')\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip: Understanding the Difference Between <code>'a'</code> and <code>'w'</code> When Writing Files in Python</b>\n",
    "    <br>\n",
    "    \n",
    "- When working with files in Python, it's essential to know the difference between <code>'a'</code> and <code>'w'</code>  when opening them.\n",
    "- <code>'a'</code> stands for \"append\" mode. When you open a file with <code>'a'</code> , Python will let you add data to the end of the existing file without erasing its contents. This is useful when you want to add new information to a file without losing what's already there. It's like adding new lines to the end of an ongoing document.\n",
    "- <code>'w'</code>  stands for \"write\" mode. When you open a file with <code>'w'</code> , Python will create a new file or overwrite an existing one. This means that if the file already has data in it, using <code>'w'</code>  will erase all the existing content and start fresh. It's like creating a new document or wiping out the old one.\n",
    "- Remember, when scraping data or working with files, it's generally safer to use <code>'a'</code>. This way, you won't accidentally delete valuable data. Using <code>'w'</code>  should be done with caution, and only when you intentionally want to start with a clean slate or create a new file altogether.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can re-open the extracted data in Python to see whether what we retrieved seems complete.\n",
    "\n",
    "Verify you've the `pandas` package installed by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_name</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timestamp_of_extraction</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ninja Tattoo</td>\n",
       "      <td>Shanadoo</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:04:11</td>\n",
       "      <td>2023-09-12 20:05:19</td>\n",
       "      <td>Geek15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Sweetest Sounds</td>\n",
       "      <td>Helen O'Connell</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:01:10</td>\n",
       "      <td>2023-09-12 20:05:19</td>\n",
       "      <td>Geek15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Throwdown At The Hoedown (LP Version)</td>\n",
       "      <td>Bela Fleck And The Flecktones</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>19:56:00</td>\n",
       "      <td>2023-09-12 20:05:19</td>\n",
       "      <td>Geek15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dame Tu Carino</td>\n",
       "      <td>Ray Barretto</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>19:52:53</td>\n",
       "      <td>2023-09-12 20:05:19</td>\n",
       "      <td>Geek15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Finding My Way</td>\n",
       "      <td>Stanley Clarke &amp; George Duke</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>19:47:14</td>\n",
       "      <td>2023-09-12 20:05:19</td>\n",
       "      <td>Geek15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>Morpha Too [Alternate Mix]</td>\n",
       "      <td>Big Star</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:19:00</td>\n",
       "      <td>2023-09-12 20:48:47</td>\n",
       "      <td>CyberStar49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>Turn This Thing Around</td>\n",
       "      <td>El Presidente</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:15:26</td>\n",
       "      <td>2023-09-12 20:48:47</td>\n",
       "      <td>CyberStar49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>Ass Attack (Four Tet Remix)</td>\n",
       "      <td>Hot Chip</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:12:25</td>\n",
       "      <td>2023-09-12 20:48:47</td>\n",
       "      <td>CyberStar49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>You Can't Deep Freeze a Red Hot Mama</td>\n",
       "      <td>Sophie Tucker</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:09:51</td>\n",
       "      <td>2023-09-12 20:48:47</td>\n",
       "      <td>CyberStar49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4677</th>\n",
       "      <td>Denk</td>\n",
       "      <td>Samy Deluxe</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>20:05:54</td>\n",
       "      <td>2023-09-12 20:48:47</td>\n",
       "      <td>CyberStar49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4678 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  song_name                    artist_name  \\\n",
       "0                              Ninja Tattoo                       Shanadoo   \n",
       "1                       The Sweetest Sounds                Helen O'Connell   \n",
       "2     Throwdown At The Hoedown (LP Version)  Bela Fleck And The Flecktones   \n",
       "3                            Dame Tu Carino                   Ray Barretto   \n",
       "4                            Finding My Way   Stanley Clarke & George Duke   \n",
       "...                                     ...                            ...   \n",
       "4673             Morpha Too [Alternate Mix]                       Big Star   \n",
       "4674                 Turn This Thing Around                  El Presidente   \n",
       "4675            Ass Attack (Four Tet Remix)                       Hot Chip   \n",
       "4676   You Can't Deep Freeze a Red Hot Mama                  Sophie Tucker   \n",
       "4677                                   Denk                    Samy Deluxe   \n",
       "\n",
       "           date      time timestamp_of_extraction     username  \n",
       "0    2023-09-12  20:04:11     2023-09-12 20:05:19       Geek15  \n",
       "1    2023-09-12  20:01:10     2023-09-12 20:05:19       Geek15  \n",
       "2    2023-09-12  19:56:00     2023-09-12 20:05:19       Geek15  \n",
       "3    2023-09-12  19:52:53     2023-09-12 20:05:19       Geek15  \n",
       "4    2023-09-12  19:47:14     2023-09-12 20:05:19       Geek15  \n",
       "...         ...       ...                     ...          ...  \n",
       "4673 2023-09-12  20:19:00     2023-09-12 20:48:47  CyberStar49  \n",
       "4674 2023-09-12  20:15:26     2023-09-12 20:48:47  CyberStar49  \n",
       "4675 2023-09-12  20:12:25     2023-09-12 20:48:47  CyberStar49  \n",
       "4676 2023-09-12  20:09:51     2023-09-12 20:48:47  CyberStar49  \n",
       "4677 2023-09-12  20:05:54     2023-09-12 20:48:47  CyberStar49  \n",
       "\n",
       "[4678 rows x 6 columns]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect data in pandas\n",
    "import pandas as pd\n",
    "pd.read_json('output_data.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summary\n",
    "\n",
    "At the beginning of this tutorial, we set out the promise of writing multi-page scrapers from start to finish. Although the examples we have studied are relatively simple, the same principles (seed definition, data extraction plan, page-level data collection) apply to any other website you'd like to scrape. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Limitations of BeautifulSoup and the Advantages of Selenium</b>\n",
    "    <br>\n",
    "    \n",
    "- While BeautifulSoup is a powerful tool for parsing and navigating HTML documents, it has some limitations when it comes to interacting with websites:\n",
    "  - BeautifulSoup is a static parser, meaning it can't interact with dynamic web content that loads or changes after the initial page load. This makes it less suitable for websites that heavily rely on, say, JavaScript to update their content. For example, this is relevant for Twitter or Instagram.\n",
    "  - BeautifulSoup can't handle user interactions such as clicking buttons, filling out forms, or navigating through complex web applications.\n",
    "- When you need to scrape data from very modern and interactive websites, consider using a tool like Selenium. Selenium is a web automation framework that allows you to control a web browser programmatically.\n",
    "  - With Selenium, you can automate interactions with websites, simulate user actions, and retrieve data from pages that rely heavily on JavaScript.\n",
    "  - It's an excellent choice for scraping data from dynamic websites, conducting web testing, and performing tasks that require a more interactive approach.\n",
    "- Keep in mind that while BeautifulSoup is great for many scraping tasks, knowing when to use Selenium can open up new possibilities and make your web scraping efforts more effective.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After-class exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Can you extend the code written in 3.2 to extract data from ALL of a user's profile pages?\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Please port your data collection into two Python scripts. One called `collect_seeds.py` that collects seeds for 5 minutes. You can use a task scheduler to launch this task every 15 minutes and keep it running for a few hours.\n",
    "\n",
    "Building on exercise 1 above, write a second script, called `collect_user_data.py`, which you run once (after you've finalized collecting seeds). This script collects all of the required data for all users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first modify the `get_consumption_history()` function, ensuring it shows us whether there is a `previous page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1\n",
    "\n",
    "def get_consumption_history(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}\n",
    "    res = requests.get(url, headers = header)\n",
    "    res.encoding = res.apparent_encoding\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    \n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    json_data=[]\n",
    "    for row in rows:\n",
    "        data = row.find_all('td')\n",
    "    \n",
    "        if len(data)>0:\n",
    "            song_name=data[0].get_text()\n",
    "            artist_name=data[1].get_text()\n",
    "            date=data[2].get_text()\n",
    "            timestamp=data[3].get_text()\n",
    "            json_data.append({'song_name': song_name,\n",
    "                              'artist_name': artist_name,\n",
    "                              'date': date,\n",
    "                              'time': timestamp,\n",
    "                              'timestamp_of_extraction': int(time.time()),\n",
    "                              'username': url.split('=')[1]})\n",
    "\n",
    "    url_of_previous_page = previous_page(soup)\n",
    "        \n",
    "    return({'songs': json_data, 'previous_page': url_of_previous_page})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting information for https://music-to-scrape.org/user?username=Geek15...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=36...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=35...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=34...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=33...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=32...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=31...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=30...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=29...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=28...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=27...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=26...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=25...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=24...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=23...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=22...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=21...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=20...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=19...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=18...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=17...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=16...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=15...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=14...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=13...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=12...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=11...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=10...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=9...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=8...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=7...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=6...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=5...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=4...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=3...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=2...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=1...\n",
      "Extracting information for https://music-to-scrape.org/user?username=Geek15&week=0...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=36...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=35...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=34...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=33...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=32...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=31...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=30...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=29...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=28...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=27...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=26...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=25...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=24...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=23...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=22...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=21...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=20...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=19...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=18...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=17...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=16...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=15...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=14...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=13...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=12...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=11...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=10...\n",
      "Extracting information for https://music-to-scrape.org/user?username=ShadowPixel58&week=9...\n"
     ]
    }
   ],
   "source": [
    "# start from the code below\n",
    "import time # we need the time package for implementing a bit of waiting time\n",
    "import json\n",
    "\n",
    "content = open('seeds.json', 'r').readlines() # let's read in the seed data\n",
    "\n",
    "counter = 0 # initialize counter to 0\n",
    "\n",
    "# loop through all lines of the JSON file\n",
    "for line in content:\n",
    "    # increment counter and check whether prototyping condition is met\n",
    "    counter = counter + 1\n",
    "    if counter>5: break # deactivate this if you want to loop through the entire file\n",
    "        \n",
    "    # convert loaded data to JSON object/dictionary for querying\n",
    "    obj = json.loads(line)\n",
    "    \n",
    "    # show URL for which product information needs to be captured\n",
    "    url = obj['url']\n",
    "\n",
    "    while 'no previous page' not in url:\n",
    "        print(f'Extracting information for {url}...')\n",
    "    \n",
    "        output_file = open('output_data.json','a')\n",
    "    \n",
    "        songs = get_consumption_history(url)\n",
    "        \n",
    "        for song in songs['songs']:\n",
    "            output_file.write(json.dumps(song))\n",
    "            output_file.write('\\n')\n",
    "        output_file.close()\n",
    "        \n",
    "        url = songs['previous_page']\n",
    "        time.sleep(2)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. A primer on scraping more advanced, dynamic websites using selenium\n",
    "\n",
    "In previous tutorials, you have used the `requests` library to retrieve web data. Yet, this rarely works on more modern websites (such as Twitch, Twitter, or Instagram).\n",
    "\n",
    "A solution is to use the `selenium` library. We'll use it to *navigate on the site*. One we *are* on the site, we can continue using `BeautifulSoup` to extract elements from the source code.\n",
    "\n",
    "We'll continue with [Twitch](https://twitch.tv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Making a connection to a website using Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Installing Selenium and Chromedriver</b> \n",
    "\n",
    "To install Selenium and Chromedriver locally, please follow the <a href=\"https://tilburgsciencehub.com/configure/python-for-scraping/?utm_campaign=referral-short\">Tutorial on Tilburg Science Hub</a>.\n",
    "    \n",
    "You can also use the code snippet below to automate the installation. Running this snippet takes a little longer each time, but the benefit is that it almost always works!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cffi-1.15.1-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: webdriver_manager in /opt/homebrew/lib/python3.11/site-packages (4.0.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.11/site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from webdriver_manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->webdriver_manager) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->webdriver_manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->webdriver_manager) (2023.7.22)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cffi-1.15.1-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cffi-1.15.1-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: Selenium in /opt/homebrew/lib/python3.11/site-packages (4.12.0)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /opt/homebrew/lib/python3.11/site-packages (from Selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/homebrew/lib/python3.11/site-packages (from Selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/homebrew/lib/python3.11/site-packages (from Selenium) (0.10.4)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/homebrew/lib/python3.11/site-packages (from Selenium) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->Selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->Selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->Selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->Selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->Selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/lib/python3.11/site-packages (from trio-websocket~=0.9->Selenium) (1.1.3)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from trio-websocket~=0.9->Selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/homebrew/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->Selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->Selenium) (0.14.0)\n",
      "\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/cffi-1.15.1-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/PyYAML-6.0.1.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/six-1.16.0-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/certifi-2023.7.22-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /opt/homebrew/lib/python3.11/site-packages/pycparser-2.21-py3.11.egg-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager\n",
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using selenium 4 - ensure you have Chrome installed!\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went smooth, your computer opened a new Chrome window, and opened `twitch.tv`. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Using Google Colab</b> \n",
    "\n",
    "If you're using Google Colab, you don't see your browser open up manually.\n",
    "    \n",
    "Whenever you switch pages, just manually open that page in your browser. Although this feels like a little less interactive, you will still be able to work through this tutorial!\n",
    "\n",
    "</div>\n",
    "\n",
    "From now onwards, you can use `driver.get('https://google.com')` to point to different websites (i.e., you don't need to install it over and over again, unless you open up a new instance of Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Using BeautifulSoup with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can now also try to extract information. Note that we're converting the source code of the site to a `BeautifulSoup` object (because you may have learnt how to use `BeautifulSoup` earlier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need the time package to wait a few seconds until the page is loaded\n",
    "import time\n",
    "url = \"https://twitch.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using the \"source code\" obtained with the `requests` library, we can now convert the source code of the Selenium website to a BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and start experimenting with querying the site, such as retrieving the titles of the currently active streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1: 🐧EXCLUSIVE DROP🐧OP PENGUIN DOOR 🐧WEEK LONG !CHARITY STREAM🐧BlizzardID\n",
      "Stream 2: !tg g0bbba unique ak-47 only here))  !fonbet !youtube !boosty !tgg0bbba\n",
      "Stream 3: 🟣DROPS ENABLED🟣SebbyK GARAGE DOOR 🟣!Twitter !Video !ServerSebbyK\n",
      "Stream 4: [STARFIELD CODE RAFFLE] BUT IS IT IMMERSIVE?! <3 !MODS <3 100%/VERY HARD LONG STREAM !socialspaigejxo\n",
      "Stream 5: [🔥Giveaway !SCORPIUS🔥] !mods │ Day 9 Vanguard Quest Line! │ !TTS !Subtemberlokenplays\n",
      "Stream 6: Space Ronin! Moving Up From the Streets of Neon! Socials: @earlmeisterEarlmeister\n"
     ]
    }
   ],
   "source": [
    "streams = soup.find_all('a', attrs = {'data-test-selector':\"TitleAndChannel\"})\n",
    "\n",
    "# print a list of stream names\n",
    "counter = 0\n",
    "for stream in streams:\n",
    "    counter = counter + 1\n",
    "    print('Stream ' + str(counter) + ': ' + stream.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - this is cool. You've just learnt a second way to open websites using `selenium`. The benefit of `selenium` is that you can work with highly dynamic websites (which also helps you to not getting blocked). The drawback is that `selenium` is slower than just using the `requests` library, and it may sometimes be buggy on computers without a screen (which matters when you scale up your data collection.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Awesome stuff with Selenium</b> \n",
    "\n",
    "Selenium is your best shot at navigating a dynamic website. It can do amazing things, such as \n",
    "    \n",
    "<ul>\n",
    "    <li>\"clicking\" on buttons</li>\n",
    "    <li>scrolling through a site</li>\n",
    "    <li>hovering over items and capturing information from popups,</li>\n",
    "    <li>starting to play a stream,</li>\n",
    "    <li>typing text and submitting it in the chat, and</li>\n",
    "    <li>so much more...!</li>\n",
    "</ul>\n",
    "    \n",
    "Note though that we won't cover the advanced functionality of Selenium in this tutorial, but the optional \"Web data advanced\" tutorial holds the necessary information.\n",
    "   \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "__Exercise 4.1__\n",
    "\n",
    "Please write code snippets to extract the following pieces of information. Do you choose `requests` or `selenium`?\n",
    "\n",
    "1. The titles of all `<h2>` tags from `https://odcm.hannesdatta.com/docs/course/`\n",
    "2. The titles of all available TV series from `https://www.bol.com/nl/nl/l/series/3133/30291/` (about 24)\n",
    "\n",
    "```\n",
    "soup.find_all('a', class_='product-title')\n",
    "```\n",
    "\n",
    "\n",
    "We also need the time package to wait a few seconds until the page is loaded.\n",
    "\n",
    "```\n",
    "import time\n",
    "url = \"https://twitch.tv/\" # some example URL\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructor\n",
      "Course description\n",
      "Prerequisites\n",
      "Teaching format\n",
      "Assessment\n",
      "Code of Conduct\n",
      "Structure of the course\n",
      "More links\n"
     ]
    }
   ],
   "source": [
    "# Solution to question 1:\n",
    "header = {'User-agent': 'Mozilla/5.0'} # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "request = requests.get('https://odcm.hannesdatta.com/docs/course/', headers = header)\n",
    "request.encoding = request.apparent_encoding # set encoding to UTF-8\n",
    "soup = BeautifulSoup(request.text)\n",
    "for title in soup.find_all('h2'): print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to question 2:\n",
    "driver.get('https://www.bol.com/nl/nl/l/series/3133/30291/')\n",
    "time.sleep(3)\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/nl/nl/p/midsomer-murders-seizoen-1/9200000126652303/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.16.ProductTitle',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-7-deel-2/9200000130879705/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.17.ProductTitle',\n",
       " '/nl/nl/p/flikken-maastricht-seizoen-17/9300000152726224/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.18.ProductTitle',\n",
       " '/nl/nl/p/jack-ryan-seizoen-3/9300000160137131/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.19.ProductTitle',\n",
       " '/nl/nl/p/chicago-fire-seizoen-11-dvd-import-zonder-nl-ot/9300000160463468/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.20.ProductTitle',\n",
       " '/nl/nl/p/the-sandhamn-murders-seizoen-6/9300000155912985/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.21.ProductTitle',\n",
       " '/nl/nl/p/dagboek-van-een-herdershond-seizoen-1/1002004006832455/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.22.ProductTitle',\n",
       " '/nl/nl/p/last-of-us/9300000142780418/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.23.ProductTitle',\n",
       " '/nl/nl/p/last-of-us/9300000142780419/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.24.ProductTitle',\n",
       " '/nl/nl/p/succession-seizoen-4/9300000154592754/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.25.ProductTitle',\n",
       " '/nl/nl/p/dagboek-van-een-herdershond-seizoen-2/1002004010467064/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.26.ProductTitle',\n",
       " '/nl/nl/p/north-south-complete-collection/1002004005993287/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.27.ProductTitle',\n",
       " '/nl/nl/p/knight-rider-complete-collection/9200000079038783/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.28.ProductTitle',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-18-deel-1/9200000132010326/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.29.ProductTitle',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-19-deel-2/9200000119833762/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.30.ProductTitle',\n",
       " '/nl/nl/p/catherine-the-great/9200000125003063/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.31.ProductTitle',\n",
       " '/nl/nl/p/miami-vice-complete-collection/9200000079038779/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.32.ProductTitle',\n",
       " '/nl/nl/p/game-of-thrones-seizoen-1-8/9300000045366024/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.33.ProductTitle',\n",
       " '/nl/nl/p/beck-10a/9300000153249387/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.34.ProductTitle',\n",
       " '/nl/nl/p/jack-ryan-seizoen-3/9300000160137135/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.35.ProductTitle',\n",
       " '/nl/nl/p/last-of-us/9300000142780417/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.36.ProductTitle',\n",
       " '/nl/nl/p/scott-bailey-seizoen-1-t-m-5-box/9200000078046759/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.37.ProductTitle',\n",
       " '/nl/nl/p/the-sommerdahl-murders-seizoen-4/9300000153249389/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.38.ProductTitle',\n",
       " '/nl/nl/p/a-team-complete-collection/9200000079038831/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.39.ProductTitle',\n",
       " '/nl/nl/p/the-flash-seizoen-9-dvd-import-zonder-nl-ot/9300000160463766/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.40.ProductTitle',\n",
       " '/nl/nl/p/the-walking-dead-seizoen-11/9300000146238625/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.41.ProductTitle',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-18-deel-2/9200000132010306/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.42.ProductTitle',\n",
       " '/nl/nl/p/midsomer-murders-seizoen-16/9200000132010310/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.43.ProductTitle',\n",
       " '/nl/nl/p/sketch-artist-seizoen-2/9300000159537423/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.44.ProductTitle',\n",
       " '/nl/nl/p/chicago-fire-seizoen-10/9300000123634169/?bltgh=nOMDDGdt0suKSrRC0h5Fgg.3_15.45.ProductTitle']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for url in soup.find_all('a', class_='product-title'):\n",
    "    urls.append(url.attrs['href'])\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Using interactive elements (e.g., by clicking buttons)\n",
    "\n",
    "__Importance__\n",
    "\n",
    "For more dynamic websites, we may have to click on certain elements (rather than extracting some URL).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Extracting elements using Selenium, not BeautifulSoup</b> \n",
    "\n",
    "Selenium is really great for navigating dynamic website. There are two ways in which you can use it for querying sites:\n",
    "    \n",
    "<ul>\n",
    "    <li>put the \"selenium\" source code (<code>driver.page_source</code>) to BeautifulSoup, and then use BeautifulSoup commands, or </li>\n",
    "    <li>directly use selenium (and it's own query language) to extract elements.</li>\n",
    "</ul>\n",
    "    \n",
    "In the next few examples, we are using selenium's \"internal\" query language (which you identify easily because it is a subfunction of the `driver` object, and because it has a different name (`find_element`, instead of `find` or `find_all`).\n",
    "    \n",
    "Want to know more about selenium's built-in query language? Check out the \"Advanced Web Scraping Tutorial\", or dig up some extra material from the web. Knowing both BeautifulSoup and Selenium makes you most productive!\n",
    "  \n",
    "</div>\n",
    "\n",
    "__Try it out__\n",
    "\n",
    "If you haven't done so, rerun the installation code for `selenium` from above. Then, proceed by running the following cell and observe what happens in your browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://music-to-scrape.org/user?username=StarCoder49')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, your browser will have loaded the website in Chrome. Now, run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"889393e1e316b7c098b0fb7e4e63c82f\", element=\"7870A3AF1CDB4D7790D28E2374BA835D_element_855\")>"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Let's try location the element\n",
    "from selenium.webdriver.common.by import By\n",
    "button = driver.find_element(By.CSS_SELECTOR, \".page-link\")\n",
    "button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Clicking the link!\n",
    "button = driver.find_element(By.CSS_SELECTOR, \".page-link\")\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom! In step 2, we clicked on the link. Just try rerunning this cell with step 2 over and over again. Does iterating through the pages work?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup: Executing Python Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks versus editors such as Visual Studio Code, PyCharm, or Spyder\n",
    "\n",
    "Jupyter Notebooks are ideal for combining programming and markdown (e.g., text, plots, equations), making it the default choice for sharing and presenting reproducible data analyses. Since we can execute code blocks one by one, it's suitable for developing and debugging code on the fly. \n",
    "\n",
    "That said, Jupyter Notebooks also have some severe limitations when using them in production environments. That's where an \"Integrated Development Environment\" (IDE) comes in, such as Visual Studio Code or PyCharm. Let's revisit the most important differences.\n",
    "\n",
    "First, the order in which you run cells within a notebook may affect the results. While prototyping, you may lose sight of the top-down hierarchy, which can cause problems once you restart the kernel (e.g., a library is imported after it is being used). Second, there is no easy way to browse through directories and files within a Jupyter Notebook. Third, notebooks cannot handle large codebases nor big data remarkably well. \n",
    "\n",
    "That's why we recommend starting in Jupyter Notebooks, moving code into functions along the way, and once all seems to be running well, save your Jupyter Notebook as a `.py` file and continue working with it in Visual Studio Code.\n",
    "\n",
    "Below, we introduce you to the IDE (here, Spyder, but VS Code looks very similar), and show you how to run Python files from the command line. \n",
    "\n",
    "### Introduction to Spyder\n",
    "The first time you need to click on the green \"Install\" button in Anaconda Navigator, after which you start Spyder by clicking on the blue \"Launch\" button (alternatively, type `spyder` in the terminal). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/anaconda_navigator.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interface consists of three panels: \n",
    "1. **Code editor** = where you write Python code (i.e., the content of code cells in a notebook)\n",
    "2. **Variable / files** = depending on which tab you choose either an overview of all declared variables (e.g. look up their type or change their values) or a file explorer (e.g., to open other Python files)\n",
    "3. **Console** = the output of running the Python script from the code editor (what normally appears below each cell in a notebook)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/spyder.png\" width=90% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it out!**     \n",
    "Copy the solution from exercise 3.3 to a new file, called `webscraping_101.py`. To run the script you can\n",
    "\n",
    "- click on the green play button to run all code, or\n",
    "- highlight the parts of the script you want to execute and then click the run selection button.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/toolbar.png\" width=40% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the script is running, you may need to interrupt the execution because it is simply taking too long or you spotted a bug somewhere. Click on the red rectangular in the console to stop the execution. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/interrupt.gif\" width=80% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Python Files \n",
    "\n",
    "__For Mac and Linux users__\n",
    "\n",
    "1. Open the terminal and navigate to the folder in which the `.py` file has been saved (use `cd` to change directories and `ls` to list all files).\n",
    "2. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/hannesdatta/course-odcm/master/content/docs/modules/week3/webscraping101/images/running_python.gif\" width=60% align=\"left\" style=\"border: 1px solid black\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For Windows users__\n",
    "\n",
    "1. Open Windows explorer and navigate to the folder in which the `.py` file has been saved. Type `cmd` to open the command prompt. Alternatively, open the command prompt from the start menu (and use `cd` to change directories and `dir` to list files).\n",
    "2. Activate Anaconda by typing `conda activate`.\n",
    "3. Run the Python script by typing `python <FILENAME.py>` (e.g., `python webscraping_101.py`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
